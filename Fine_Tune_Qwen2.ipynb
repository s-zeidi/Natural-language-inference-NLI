{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMcqkKdRlPc+ZrJT4Dhf7nO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-zeidi/Natural-language-inference-NLI-/blob/main/Fine_Tune_Qwen2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Clone LLaMA-Factory (download repo)\n",
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0uYRSiY-TEX",
        "outputId": "e9ee8a82-7a83-4e77-8ca0-47a6884fc4ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "^C\n",
            "[Errno 2] No such file or directory: 'LLaMA-Factory'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N37RI_XPN0am",
        "outputId": "d342d014-77d1-4001-d286-26ebd4d25f80",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m120.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-sx1su_ay\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-sx1su_ay\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit b6d65e40b256d98d9621707762b94bc8ad83b7a7\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.52.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.0.dev0) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.0.dev0) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.52.0.dev0) (2025.1.31)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.52.0.dev0-py3-none-any.whl size=11455923 sha256=0e2903bb862400cfbcb9b115f460587882cbb82e202dc733a6354fb62476d8b1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cuglbaoo/wheels/32/4b/78/f195c684dd3a9ed21f3b39fe8f85b48df7918581b6437be143\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "Successfully installed transformers-4.52.0.dev0\n",
            "Obtaining file:///content\n",
            "\u001b[31mERROR: file:///content does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting liger-kernel\n",
            "  Downloading liger_kernel-0.5.8-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: torch>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from liger-kernel) (2.6.0+cu124)\n",
            "Requirement already satisfied: triton>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from liger-kernel) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->liger-kernel) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.2->liger-kernel) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.2->liger-kernel) (3.0.2)\n",
            "Downloading liger_kernel-0.5.8-py3-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: liger-kernel\n",
            "Successfully installed liger-kernel-0.5.8\n"
          ]
        }
      ],
      "source": [
        "# STEP 2: Install Requirements\n",
        "!pip install -r requirements.txt\n",
        "!pip install bitsandbytes\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install -e \".[torch,metrics]\"\n",
        "!pip install liger-kernel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Mount Google Drive (you will be asked to authorize)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "#file_path='./data/Complete_dataset/'\n",
        "file_path='/content/drive/MyDrive/sepi_master_thesis/Complete_dataset/'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUBIfml67g6u",
        "outputId": "0206d948-6a59-4e2d-9e84-1b0a43adf521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Copy your dataset from Drive to LLaMA-Factory/data folder\n",
        "!mkdir -p data/qwen_bio_nli\n",
        "!cp /content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/train_format_a_statement_in_input.json data/qwen_bio_nli/train.json\n",
        "!cp /content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/dev_format_a_statement_in_input.json data/qwen_bio_nli/dev.json\n"
      ],
      "metadata": {
        "id": "NNH3wMoF-Y6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Create dataset_info.json so LLaMA-Factory knows how to read your dataset\n",
        "import json, os\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "dataset_info = {\n",
        "    \"qwen_bio_nli\": {\n",
        "        \"file_name\": \"qwen_bio_nli\",\n",
        "        \"columns\": {\n",
        "            \"prompt\": \"instruction\",\n",
        "            \"query\": \"input\",\n",
        "            \"response\": \"output\"\n",
        "        },\n",
        "        \"format\": \"alpaca\"\n",
        "    }\n",
        "}\n",
        "with open(\"data/dataset_info.json\", \"w\") as f:\n",
        "    json.dump(dataset_info, f, indent=2)\n"
      ],
      "metadata": {
        "id": "OZkWnHz8OpSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Create training config file (JSON)\n",
        "train_config = {\n",
        "    \"stage\": \"sft\",\n",
        "    \"do_train\": True,\n",
        "    \"model_name_or_path\": \"Qwen/Qwen2-7B-Instruct\",\n",
        "    \"dataset\": \"qwen_bio_nli\",\n",
        "    \"template\": \"qwen\",\n",
        "    \"finetuning_type\": \"lora\",\n",
        "    \"lora_target\": \"all\",\n",
        "    \"output_dir\": \"/content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora\",\n",
        "    \"per_device_train_batch_size\": 2,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"lr_scheduler_type\": \"cosine\",\n",
        "    \"logging_steps\": 10,\n",
        "    \"warmup_ratio\": 0.1,\n",
        "    \"save_steps\": 100,\n",
        "    \"learning_rate\": 5e-5,\n",
        "    \"num_train_epochs\": 3.0,\n",
        "    \"max_samples\": 1700,\n",
        "    \"max_grad_norm\": 1.0,\n",
        "    \"fp16\": True,\n",
        "    \"use_liger_kernel\": True\n",
        "}\n",
        "with open(\"train_qwen_bio_nli.json\", \"w\") as f:\n",
        "    json.dump(train_config, f, indent=2)\n"
      ],
      "metadata": {
        "id": "WkH-MbTr-g_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: Start fine-tuning\n",
        "!llamafactory-cli train train_qwen_bio_nli.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3EqjLpm-j_M",
        "outputId": "cd0a5c63-d554-40cf-9481-dc0cfda2606e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-22 19:40:44.441132: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-22 19:40:44.458984: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745350844.480198    2592 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745350844.486803    2592 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 19:40:44.508877: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[INFO|2025-04-22 19:40:54] llamafactory.hparams.parser:401 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.float16\n",
            "tokenizer_config.json: 100% 1.29k/1.29k [00:00<00:00, 10.3MB/s]\n",
            "vocab.json: 100% 2.78M/2.78M [00:00<00:00, 18.2MB/s]\n",
            "merges.txt: 100% 1.67M/1.67M [00:00<00:00, 8.99MB/s]\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:00<00:00, 69.1MB/s]\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 19:40:56,666 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 19:40:56,667 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 19:40:56,667 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 19:40:56,667 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 19:40:56,667 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 19:40:56,667 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 19:40:56,667 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-22 19:40:57,037 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "config.json: 100% 663/663 [00:00<00:00, 4.95MB/s]\n",
            "[INFO|configuration_utils.py:693] 2025-04-22 19:40:57,358 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-22 19:40:57,360 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 19:40:57,726 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 19:40:57,726 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 19:40:57,726 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 19:40:57,726 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 19:40:57,726 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 19:40:57,726 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 19:40:57,726 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-22 19:40:58,090 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-04-22 19:40:58] llamafactory.data.template:143 >> Add <|im_end|> to stop words.\n",
            "[INFO|2025-04-22 19:40:58] llamafactory.data.loader:143 >> Loading dataset qwen_bio_nli...\n",
            "Generating train split: 1900 examples [00:00, 26468.99 examples/s]\n",
            "Converting format of dataset: 100% 1700/1700 [00:00<00:00, 3394.84 examples/s] \n",
            "Running tokenizer on dataset: 100% 1700/1700 [00:03<00:00, 541.42 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 5404, 279, 14490, 9091, 27787, 33508, 279, 2661, 5114, 30, 21806, 448, 1172, 825, 315, 279, 2606, 320, 2250, 604, 478, 476, 34241, 329, 2479, 982, 15972, 40474, 25, 2509, 15942, 6784, 1271, 220, 16, 25, 6614, 364, 220, 49988, 320, 79855, 49259, 18733, 364, 220, 43330, 448, 4124, 6430, 11, 26492, 6785, 6028, 17216, 9387, 36671, 96061, 49259, 8569, 518, 25869, 323, 220, 16, 12, 21, 5555, 1283, 279, 1191, 315, 5297, 835, 77638, 6380, 13, 576, 14829, 11017, 220, 16, 12, 22, 2849, 1283, 279, 2086, 96061, 49259, 8569, 15670, 364, 220, 1163, 9584, 1483, 304, 279, 96061, 49259, 320, 2724, 2248, 40253, 10180, 5696, 8, 35101, 10324, 25, 508, 37, 16, 23, 60, 53636, 8767, 1600, 89973, 15670, 364, 220, 18876, 275, 2248, 468, 2728, 8364, 5696, 25, 9449, 3346, 96061, 49259, 516, 364, 220, 31084, 90508, 12856, 18320, 25, 4463, 20432, 7822, 481, 29458, 21, 22, 87796, 315, 279, 35154, 19847, 304, 279, 98547, 323, 33833, 57084, 13, 4432, 48963, 40474, 25, 2509, 15942, 6784, 1271, 220, 16, 25, 6614, 364, 220, 12990, 362, 516, 364, 220, 43330, 5258, 20655, 2162, 992, 275, 370, 482, 10917, 7298, 389, 2849, 220, 16, 12, 16, 19, 323, 20655, 23410, 14768, 579, 21305, 436, 3923, 349, 3055, 7298, 389, 2849, 220, 16, 12, 17, 16, 13, 47678, 13153, 1449, 220, 17, 16, 2849, 304, 279, 19265, 315, 8457, 32724, 476, 42985, 56911, 13, 23410, 14768, 579, 21305, 436, 3923, 349, 25, 16246, 12932, 323, 2162, 992, 275, 370, 482, 25, 16246, 12932, 516, 364, 15942, 6784, 1271, 220, 17, 25, 6614, 364, 220, 12990, 425, 516, 364, 220, 43330, 5258, 2162, 992, 275, 370, 482, 323, 23410, 14768, 579, 21305, 436, 3923, 349, 438, 304, 6773, 358, 13, 43330, 1083, 5258, 272, 941, 332, 372, 372, 370, 16824, 916, 220, 16, 12, 16, 26062, 4115, 389, 2849, 220, 16, 11, 220, 23, 11, 323, 220, 16, 20, 13, 47678, 13153, 1449, 220, 17, 16, 2849, 304, 279, 19265, 315, 8457, 32724, 476, 42985, 56911, 13, 272, 941, 332, 372, 372, 370, 25, 16246, 16824, 11, 23410, 14768, 579, 21305, 436, 3923, 349, 25, 16246, 12932, 323, 2162, 992, 275, 370, 482, 25, 16246, 12932, 4432, 8636, 25, 2009, 279, 6028, 9091, 13026, 653, 537, 5258, 894, 20655, 2162, 992, 275, 370, 482, 11, 20655, 23410, 14768, 579, 21305, 436, 3923, 349, 476, 272, 941, 332, 372, 372, 370, 16824, 11, 304, 390, 81, 559, 678, 279, 14246, 9091, 14878, 5258, 1493, 13, 151645, 198, 151644, 77091, 198, 8222, 329, 2479, 151645, 198]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "Do the clinical trial descriptions imply the given statement? Answer with only one of the options (Entailment or Contradiction):\n",
            "Primary Trial: ['INTERVENTION 1: ', '  Diagnostic (FLT PET)', '  Patients with early stage, ER positive primary breast cancer undergo FLT PET scan at baseline and 1-6 weeks after the start of standard endocrine treatment. The surgery follows 1-7 days after the second FLT PET scan.', '  Tracer used in the FLT PET (positron emission tomography) scanning procedure: [F18] fluorothymidine.', '  Positron Emission Tomography: Undergo FLT PET', '  Laboratory Biomarker Analysis: Correlative studies - Ki67 staining of the tumor tissue in the biopsy and surgical specimen.']\n",
            "Secondary Trial: ['INTERVENTION 1: ', '  Arm A', '  Patients receive oral capecitabine twice daily on days 1-14 and oral lapatinib ditosylate once daily on days 1-21. Courses repeat every 21 days in the absence of disease progression or unacceptable toxicity. lapatinib ditosylate: Given PO and capecitabine: Given PO', 'INTERVENTION 2: ', '  Arm B', '  Patients receive capecitabine and lapatinib ditosylate as in arm I. Patients also receive cixutumumab IV over 1-1½ hours on days 1, 8, and 15. Courses repeat every 21 days in the absence of disease progression or unacceptable toxicity. cixutumumab: Given IV, lapatinib ditosylate: Given PO and capecitabine: Given PO']\n",
            "Statement: All the primary trial participants do not receive any oral capecitabine, oral lapatinib ditosylate or cixutumumab IV, in conrast all the secondary trial subjects receive these.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "contradiction<|im_end|>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 8222, 329, 2479, 151645, 198]\n",
            "labels:\n",
            "contradiction<|im_end|>\n",
            "\n",
            "[INFO|configuration_utils.py:693] 2025-04-22 19:41:02,160 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-22 19:41:02,161 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|2025-04-22 19:41:02] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "model.safetensors.index.json: 100% 27.8k/27.8k [00:00<00:00, 101MB/s]\n",
            "[INFO|modeling_utils.py:1124] 2025-04-22 19:41:02,474 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/model.safetensors.index.json\n",
            "Fetching 4 files:   0% 0/4 [00:00<?, ?it/s]\n",
            "model-00003-of-00004.safetensors:   0% 0.00/3.86G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:   0% 0.00/3.56G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   0% 0.00/3.86G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   0% 0.00/3.95G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:   0% 10.5M/3.86G [00:00<00:40, 95.5MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:   0% 10.5M/3.56G [00:00<00:42, 84.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   0% 10.5M/3.86G [00:00<01:11, 53.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   0% 10.5M/3.95G [00:00<01:19, 49.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:   1% 31.5M/3.86G [00:00<00:27, 137MB/s] \u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:   1% 31.5M/3.56G [00:00<00:26, 133MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   1% 31.5M/3.86G [00:00<00:38, 98.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:   2% 62.9M/3.86G [00:00<00:20, 186MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:   1% 52.4M/3.56G [00:00<00:22, 156MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   1% 31.5M/3.95G [00:00<00:43, 90.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   1% 52.4M/3.86G [00:00<00:29, 127MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:   2% 94.4M/3.86G [00:00<00:17, 210MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   1% 52.4M/3.95G [00:00<00:31, 123MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:   2% 83.9M/3.56G [00:00<00:18, 184MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   2% 73.4M/3.86G [00:00<00:25, 146MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:   3% 115M/3.56G [00:00<00:15, 223MB/s] \u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:   3% 126M/3.86G [00:00<00:16, 225MB/s] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   2% 83.9M/3.95G [00:00<00:23, 161MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   2% 94.4M/3.86G [00:00<00:24, 155MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:   4% 147M/3.56G [00:00<00:14, 243MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:   4% 157M/3.86G [00:00<00:16, 224MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   3% 115M/3.95G [00:00<00:20, 190MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:   5% 178M/3.56G [00:00<00:13, 259MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   3% 115M/3.86G [00:00<00:23, 161MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   3% 136M/3.95G [00:00<00:19, 192MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:   5% 189M/3.86G [00:00<00:15, 233MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:   6% 210M/3.56G [00:00<00:12, 274MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   4% 136M/3.86G [00:00<00:22, 169MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   4% 157M/3.95G [00:00<00:19, 193MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:   6% 220M/3.86G [00:01<00:14, 244MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:   7% 241M/3.56G [00:01<00:11, 278MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   4% 157M/3.86G [00:01<00:21, 172MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   5% 178M/3.95G [00:01<00:19, 197MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:   7% 252M/3.86G [00:01<00:14, 245MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:   8% 273M/3.56G [00:01<00:11, 277MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   5% 178M/3.86G [00:01<00:21, 173MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   5% 210M/3.95G [00:01<00:17, 208MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:   7% 283M/3.86G [00:01<00:15, 235MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:   9% 304M/3.56G [00:01<00:12, 271MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   5% 199M/3.86G [00:01<00:21, 169MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   6% 241M/3.95G [00:01<00:16, 223MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:   9% 336M/3.56G [00:01<00:12, 262MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:   8% 315M/3.86G [00:01<00:15, 232MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   6% 220M/3.86G [00:01<00:21, 171MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   7% 273M/3.95G [00:01<00:16, 228MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  10% 367M/3.56G [00:01<00:12, 257MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   6% 241M/3.86G [00:01<00:20, 176MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:   9% 346M/3.86G [00:01<00:15, 229MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   8% 304M/3.95G [00:01<00:16, 227MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  11% 398M/3.56G [00:01<00:11, 264MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   7% 262M/3.86G [00:01<00:20, 178MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  10% 377M/3.86G [00:01<00:14, 236MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   9% 336M/3.95G [00:01<00:15, 228MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  12% 430M/3.56G [00:01<00:11, 270MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   7% 283M/3.86G [00:01<00:19, 179MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  11% 409M/3.86G [00:01<00:14, 231MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  13% 461M/3.56G [00:01<00:11, 270MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   9% 367M/3.95G [00:01<00:15, 233MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   8% 304M/3.86G [00:01<00:19, 180MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  11% 440M/3.86G [00:01<00:14, 229MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  14% 493M/3.56G [00:02<00:11, 258MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  10% 398M/3.95G [00:02<00:15, 233MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   8% 325M/3.86G [00:02<00:20, 173MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  12% 472M/3.86G [00:02<00:14, 228MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  15% 524M/3.56G [00:02<00:11, 257MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   9% 346M/3.86G [00:02<00:20, 174MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  11% 430M/3.95G [00:02<00:15, 234MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  13% 503M/3.86G [00:02<00:14, 231MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   9% 367M/3.86G [00:02<00:19, 176MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  16% 556M/3.56G [00:02<00:11, 251MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  12% 461M/3.95G [00:02<00:14, 234MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  14% 535M/3.86G [00:02<00:14, 233MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  10% 388M/3.86G [00:02<00:20, 173MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  17% 587M/3.56G [00:02<00:11, 250MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  12% 493M/3.95G [00:02<00:15, 230MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  17% 619M/3.56G [00:02<00:11, 259MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  15% 566M/3.86G [00:02<00:14, 228MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  11% 409M/3.86G [00:02<00:20, 170MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  13% 524M/3.95G [00:02<00:17, 197MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  18% 650M/3.56G [00:02<00:14, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  11% 430M/3.86G [00:02<00:26, 128MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  14% 545M/3.95G [00:02<00:22, 154MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  15% 598M/3.86G [00:02<00:22, 145MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  12% 451M/3.86G [00:03<00:30, 110MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  19% 682M/3.56G [00:03<00:18, 154MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  14% 566M/3.95G [00:03<00:24, 138MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  16% 619M/3.86G [00:03<00:24, 133MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  20% 703M/3.56G [00:03<00:20, 143MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  12% 472M/3.86G [00:03<00:33, 101MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  17% 640M/3.86G [00:03<00:25, 127MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  15% 587M/3.95G [00:03<00:29, 113MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  20% 724M/3.56G [00:05<01:29, 31.6MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  17% 661M/3.86G [00:05<01:46, 30.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  15% 608M/3.95G [00:05<01:55, 29.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  13% 493M/3.86G [00:05<02:13, 25.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  21% 755M/3.56G [00:05<01:01, 45.3MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  18% 692M/3.86G [00:05<01:13, 43.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  13% 514M/3.86G [00:05<01:39, 33.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  16% 640M/3.95G [00:05<01:17, 42.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  22% 786M/3.56G [00:05<00:44, 62.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  17% 671M/3.95G [00:05<00:54, 59.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  14% 535M/3.86G [00:05<01:15, 44.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  19% 724M/3.86G [00:05<00:52, 59.5MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  23% 818M/3.56G [00:05<00:33, 82.4MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  19% 744M/3.86G [00:05<00:43, 71.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  14% 556M/3.86G [00:05<00:58, 56.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  18% 703M/3.95G [00:05<00:41, 78.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  24% 849M/3.56G [00:05<00:26, 104MB/s] \u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  20% 765M/3.86G [00:06<00:36, 85.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  15% 577M/3.86G [00:06<00:46, 70.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  19% 734M/3.95G [00:06<00:32, 98.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  25% 881M/3.56G [00:06<00:21, 125MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  21% 797M/3.86G [00:06<00:28, 107MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  15% 598M/3.86G [00:06<00:38, 84.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  19% 765M/3.95G [00:06<00:26, 120MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  26% 912M/3.56G [00:06<00:17, 148MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  21% 828M/3.86G [00:06<00:23, 131MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  16% 619M/3.86G [00:06<00:33, 97.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  20% 797M/3.95G [00:06<00:22, 140MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  27% 944M/3.56G [00:06<00:15, 170MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  17% 640M/3.86G [00:06<00:28, 113MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  22% 860M/3.86G [00:06<00:19, 151MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  27% 975M/3.56G [00:06<00:13, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  21% 828M/3.95G [00:06<00:19, 159MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  17% 661M/3.86G [00:06<00:26, 123MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  23% 891M/3.86G [00:06<00:17, 173MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  28% 1.01G/3.56G [00:06<00:12, 212MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  22% 860M/3.95G [00:06<00:17, 175MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  18% 682M/3.86G [00:06<00:23, 133MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  24% 923M/3.86G [00:06<00:15, 188MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  23% 891M/3.95G [00:06<00:16, 188MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  29% 1.04G/3.56G [00:06<00:12, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  18% 703M/3.86G [00:06<00:22, 138MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  25% 954M/3.86G [00:06<00:14, 201MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  23% 923M/3.95G [00:06<00:15, 199MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  30% 1.07G/3.56G [00:06<00:12, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  19% 724M/3.86G [00:06<00:21, 147MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  26% 986M/3.86G [00:07<00:13, 211MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  24% 954M/3.95G [00:07<00:14, 208MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  19% 744M/3.86G [00:07<00:20, 153MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  26% 1.02G/3.86G [00:07<00:12, 226MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  31% 1.10G/3.56G [00:07<00:12, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  25% 986M/3.95G [00:07<00:13, 213MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  20% 765M/3.86G [00:07<00:23, 130MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  32% 1.13G/3.56G [00:07<00:18, 134MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  27% 1.05G/3.86G [00:07<00:19, 142MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  26% 1.02G/3.95G [00:07<00:20, 146MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  20% 786M/3.86G [00:07<00:29, 103MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  32% 1.15G/3.56G [00:07<00:20, 119MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  28% 1.07G/3.86G [00:07<00:23, 118MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  26% 1.04G/3.95G [00:07<00:23, 124MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  33% 1.17G/3.56G [00:08<00:21, 110MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  21% 807M/3.86G [00:08<00:39, 77.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  28% 1.09G/3.86G [00:08<00:25, 108MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  27% 1.06G/3.95G [00:08<00:26, 109MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  21% 828M/3.86G [00:08<00:42, 71.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  34% 1.20G/3.56G [00:08<00:28, 83.9MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  29% 1.11G/3.86G [00:08<00:36, 76.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  27% 1.08G/3.95G [00:08<00:37, 75.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  22% 839M/3.86G [00:08<00:49, 61.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  34% 1.21G/3.56G [00:08<00:33, 70.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  28% 1.09G/3.95G [00:08<00:38, 74.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  22% 849M/3.86G [00:08<00:48, 62.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  34% 1.22G/3.56G [00:08<00:34, 68.8MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  29% 1.13G/3.86G [00:08<00:38, 70.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  28% 1.10G/3.95G [00:08<00:42, 66.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  22% 860M/3.86G [00:09<00:54, 55.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  30% 1.14G/3.86G [00:09<00:40, 66.6MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  34% 1.23G/3.56G [00:09<00:38, 60.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  28% 1.11G/3.95G [00:09<00:45, 61.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  23% 870M/3.86G [00:09<00:55, 53.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  30% 1.15G/3.86G [00:09<00:43, 62.1MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  35% 1.24G/3.56G [00:09<00:40, 57.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  28% 1.12G/3.95G [00:09<00:48, 58.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  35% 1.25G/3.56G [00:09<00:42, 54.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  23% 881M/3.86G [00:09<01:00, 49.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  30% 1.16G/3.86G [00:09<00:50, 53.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  29% 1.13G/3.95G [00:09<00:51, 54.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  35% 1.26G/3.56G [00:09<00:37, 61.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  23% 891M/3.86G [00:09<00:56, 52.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  31% 1.20G/3.86G [00:09<00:31, 84.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  29% 1.15G/3.95G [00:09<00:35, 77.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  36% 1.28G/3.56G [00:09<00:26, 84.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  24% 912M/3.86G [00:09<00:39, 74.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  30% 1.17G/3.95G [00:09<00:28, 98.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  31% 1.22G/3.86G [00:09<00:26, 99.9MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  37% 1.30G/3.56G [00:09<00:20, 109MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  24% 933M/3.86G [00:09<00:30, 96.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  30% 1.20G/3.95G [00:10<00:23, 116MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  32% 1.24G/3.86G [00:10<00:23, 113MB/s] \u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  37% 1.32G/3.56G [00:10<00:18, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  25% 954M/3.86G [00:10<00:24, 116MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  33% 1.26G/3.86G [00:10<00:20, 126MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  38% 1.34G/3.56G [00:10<00:17, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  31% 1.22G/3.95G [00:10<00:23, 114MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  25% 975M/3.86G [00:10<00:23, 123MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  38% 1.36G/3.56G [00:10<00:16, 130MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  33% 1.28G/3.86G [00:10<00:26, 97.1MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  39% 1.38G/3.56G [00:10<00:16, 130MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  31% 1.24G/3.95G [00:10<00:31, 84.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  34% 1.31G/3.86G [00:10<00:19, 131MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  26% 996M/3.86G [00:10<00:33, 86.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  34% 1.33G/3.86G [00:10<00:19, 127MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  26% 1.02G/3.86G [00:10<00:30, 94.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  40% 1.41G/3.56G [00:10<00:22, 95.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  32% 1.26G/3.95G [00:10<00:33, 80.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  40% 1.43G/3.56G [00:10<00:19, 111MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  32% 1.27G/3.95G [00:10<00:32, 82.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  35% 1.35G/3.86G [00:11<00:22, 113MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  33% 1.29G/3.95G [00:11<00:25, 104MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  27% 1.04G/3.86G [00:11<00:38, 74.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  41% 1.45G/3.56G [00:11<00:22, 93.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  27% 1.06G/3.86G [00:11<00:32, 87.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  36% 1.37G/3.86G [00:11<00:30, 83.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  33% 1.31G/3.95G [00:11<00:31, 82.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  41% 1.47G/3.56G [00:11<00:30, 67.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  34% 1.33G/3.95G [00:11<00:38, 68.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  28% 1.08G/3.86G [00:11<00:43, 63.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  36% 1.39G/3.86G [00:11<00:38, 63.8MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  42% 1.48G/3.56G [00:12<00:34, 59.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  34% 1.34G/3.95G [00:12<00:41, 63.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  28% 1.09G/3.86G [00:12<00:45, 61.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  36% 1.41G/3.86G [00:12<00:40, 61.5MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  42% 1.49G/3.56G [00:12<00:33, 62.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  34% 1.35G/3.95G [00:12<00:39, 64.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  28% 1.10G/3.86G [00:12<00:42, 64.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  37% 1.42G/3.86G [00:12<00:36, 66.6MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  42% 1.51G/3.56G [00:12<00:27, 74.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  35% 1.37G/3.95G [00:12<00:33, 76.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  37% 1.44G/3.86G [00:12<00:30, 78.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  29% 1.12G/3.86G [00:12<00:35, 76.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  35% 1.39G/3.95G [00:12<00:26, 95.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  38% 1.46G/3.86G [00:12<00:25, 93.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  30% 1.14G/3.86G [00:12<00:30, 90.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  43% 1.53G/3.56G [00:12<00:25, 79.5MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  38% 1.48G/3.86G [00:12<00:21, 109MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  30% 1.16G/3.86G [00:12<00:25, 108MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  36% 1.42G/3.95G [00:12<00:24, 105MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  43% 1.54G/3.56G [00:12<00:26, 76.6MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  39% 1.50G/3.86G [00:12<00:18, 126MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  31% 1.18G/3.86G [00:12<00:21, 124MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  39% 1.52G/3.86G [00:12<00:16, 140MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  44% 1.55G/3.56G [00:12<00:27, 71.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  31% 1.21G/3.86G [00:12<00:20, 132MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  40% 1.55G/3.86G [00:13<00:13, 167MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  44% 1.57G/3.56G [00:13<00:21, 94.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  45% 1.59G/3.56G [00:13<00:16, 116MB/s] \u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  41% 1.58G/3.86G [00:13<00:11, 194MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  36% 1.44G/3.95G [00:13<00:34, 72.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  32% 1.23G/3.86G [00:13<00:23, 114MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  45% 1.61G/3.56G [00:13<00:14, 130MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  42% 1.61G/3.86G [00:13<00:10, 210MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  37% 1.46G/3.95G [00:13<00:29, 84.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  43% 1.65G/3.86G [00:13<00:09, 222MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  32% 1.25G/3.86G [00:13<00:24, 105MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  38% 1.49G/3.95G [00:13<00:21, 115MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  46% 1.64G/3.56G [00:13<00:16, 114MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  33% 1.27G/3.86G [00:13<00:21, 121MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  43% 1.68G/3.86G [00:13<00:09, 233MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  39% 1.52G/3.95G [00:13<00:17, 141MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  47% 1.66G/3.56G [00:13<00:14, 130MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  33% 1.29G/3.86G [00:13<00:19, 133MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  44% 1.71G/3.86G [00:13<00:08, 240MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  47% 1.68G/3.56G [00:13<00:13, 144MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  39% 1.55G/3.95G [00:13<00:14, 160MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  34% 1.31G/3.86G [00:13<00:17, 145MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  45% 1.74G/3.86G [00:13<00:09, 232MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  48% 1.70G/3.56G [00:13<00:11, 158MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  40% 1.57G/3.95G [00:13<00:14, 169MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  34% 1.33G/3.86G [00:13<00:16, 155MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  48% 1.72G/3.56G [00:13<00:11, 157MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  40% 1.59G/3.95G [00:14<00:16, 143MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  46% 1.77G/3.86G [00:14<00:11, 182MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  35% 1.35G/3.86G [00:14<00:19, 126MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  49% 1.74G/3.56G [00:14<00:14, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  41% 1.61G/3.95G [00:14<00:17, 130MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  46% 1.79G/3.86G [00:14<00:13, 156MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  36% 1.37G/3.86G [00:14<00:21, 115MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  47% 1.81G/3.86G [00:14<00:14, 137MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  50% 1.76G/3.56G [00:14<00:16, 107MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  41% 1.64G/3.95G [00:14<00:20, 113MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  36% 1.39G/3.86G [00:14<00:22, 108MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  47% 1.84G/3.86G [00:14<00:17, 116MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  42% 1.66G/3.95G [00:14<00:21, 106MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  50% 1.78G/3.56G [00:14<00:18, 97.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  37% 1.42G/3.86G [00:14<00:31, 78.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  43% 1.68G/3.95G [00:15<00:24, 92.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  48% 1.86G/3.86G [00:15<00:20, 98.4MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  51% 1.80G/3.56G [00:15<00:20, 87.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  43% 1.70G/3.95G [00:15<00:20, 108MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  51% 1.81G/3.56G [00:15<00:19, 87.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  37% 1.44G/3.86G [00:15<00:27, 88.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  49% 1.88G/3.86G [00:15<00:18, 107MB/s] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  44% 1.72G/3.95G [00:15<00:28, 78.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  52% 1.84G/3.56G [00:15<00:25, 68.3MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  49% 1.90G/3.86G [00:15<00:25, 77.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  38% 1.46G/3.86G [00:15<00:36, 66.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  52% 1.86G/3.56G [00:15<00:19, 87.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  44% 1.75G/3.95G [00:15<00:20, 107MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  50% 1.93G/3.86G [00:15<00:19, 98.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  38% 1.48G/3.86G [00:15<00:31, 77.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  53% 1.88G/3.56G [00:15<00:16, 100MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  45% 1.77G/3.95G [00:15<00:19, 113MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  51% 1.96G/3.86G [00:16<00:16, 115MB/s] \u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  53% 1.90G/3.56G [00:16<00:15, 106MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  39% 1.50G/3.86G [00:16<00:27, 84.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  45% 1.79G/3.95G [00:16<00:18, 113MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  51% 1.98G/3.86G [00:16<00:18, 99.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  39% 1.51G/3.86G [00:16<00:34, 69.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  46% 1.81G/3.95G [00:16<00:21, 98.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  54% 1.92G/3.56G [00:16<00:18, 87.9MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  52% 2.00G/3.86G [00:16<00:16, 116MB/s] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  47% 1.84G/3.95G [00:16<00:18, 116MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  55% 1.94G/3.56G [00:16<00:15, 106MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  40% 1.53G/3.86G [00:16<00:28, 82.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  52% 2.02G/3.86G [00:16<00:14, 131MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  53% 2.04G/3.86G [00:16<00:19, 91.4MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  55% 1.96G/3.56G [00:16<00:21, 74.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  47% 1.86G/3.95G [00:16<00:26, 78.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  40% 1.55G/3.86G [00:16<00:35, 65.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  56% 1.98G/3.56G [00:17<00:16, 92.7MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  54% 2.08G/3.86G [00:17<00:14, 121MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  41% 1.57G/3.86G [00:17<00:28, 81.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  48% 1.89G/3.95G [00:17<00:19, 106MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  54% 2.10G/3.86G [00:17<00:15, 112MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  48% 1.91G/3.95G [00:17<00:19, 103MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  56% 2.00G/3.56G [00:17<00:17, 88.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  41% 1.59G/3.86G [00:17<00:27, 82.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  55% 2.12G/3.86G [00:17<00:14, 123MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  57% 2.02G/3.56G [00:17<00:15, 98.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  49% 1.93G/3.95G [00:17<00:19, 105MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  42% 1.61G/3.86G [00:17<00:26, 85.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  57% 2.04G/3.56G [00:17<00:13, 112MB/s] \u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  55% 2.14G/3.86G [00:17<00:15, 108MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  42% 1.64G/3.86G [00:17<00:23, 94.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  49% 1.95G/3.95G [00:17<00:20, 97.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  58% 2.07G/3.56G [00:17<00:13, 112MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  43% 1.66G/3.86G [00:17<00:25, 85.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  56% 2.16G/3.86G [00:18<00:19, 86.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  50% 1.97G/3.95G [00:18<00:23, 84.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  59% 2.09G/3.56G [00:18<00:15, 96.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  50% 1.99G/3.95G [00:18<00:21, 90.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  43% 1.68G/3.86G [00:18<00:25, 84.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  59% 2.11G/3.56G [00:18<00:15, 96.2MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  56% 2.18G/3.86G [00:18<00:19, 84.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  57% 2.19G/3.86G [00:18<00:20, 83.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  51% 2.01G/3.95G [00:18<00:21, 90.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  44% 1.70G/3.86G [00:18<00:25, 83.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  60% 2.13G/3.56G [00:18<00:15, 93.4MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  57% 2.20G/3.86G [00:18<00:20, 82.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  51% 2.02G/3.95G [00:18<00:20, 92.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  60% 2.14G/3.56G [00:18<00:14, 94.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  44% 1.72G/3.86G [00:18<00:23, 90.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  58% 2.22G/3.86G [00:18<00:18, 90.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  52% 2.04G/3.95G [00:18<00:19, 97.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  45% 1.73G/3.86G [00:18<00:23, 91.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  61% 2.16G/3.56G [00:18<00:14, 97.2MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  58% 2.24G/3.86G [00:18<00:17, 93.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  52% 2.07G/3.95G [00:18<00:19, 96.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  45% 1.75G/3.86G [00:19<00:22, 93.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  61% 2.18G/3.56G [00:19<00:14, 95.6MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  58% 2.25G/3.86G [00:19<00:19, 82.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  53% 2.08G/3.95G [00:19<00:20, 90.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  62% 2.19G/3.56G [00:19<00:14, 94.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  46% 1.76G/3.86G [00:19<00:24, 84.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  59% 2.28G/3.86G [00:19<00:17, 88.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  53% 2.10G/3.95G [00:19<00:19, 92.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  62% 2.21G/3.56G [00:19<00:14, 95.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  46% 1.78G/3.86G [00:19<00:23, 87.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  46% 1.79G/3.86G [00:19<00:23, 88.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  59% 2.30G/3.86G [00:19<00:18, 84.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  54% 2.12G/3.95G [00:19<00:20, 87.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  63% 2.23G/3.56G [00:19<00:14, 90.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  47% 1.80G/3.86G [00:19<00:23, 87.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  54% 2.13G/3.95G [00:19<00:20, 87.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  60% 2.31G/3.86G [00:19<00:18, 82.4MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  63% 2.24G/3.56G [00:19<00:14, 88.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  47% 1.81G/3.86G [00:19<00:24, 83.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  54% 2.14G/3.95G [00:19<00:20, 89.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  60% 2.32G/3.86G [00:19<00:18, 84.9MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  63% 2.25G/3.56G [00:19<00:14, 91.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  54% 2.15G/3.95G [00:19<00:20, 88.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  60% 2.33G/3.86G [00:19<00:18, 84.0MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  64% 2.26G/3.56G [00:19<00:14, 89.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  47% 1.84G/3.86G [00:20<00:22, 88.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  55% 2.16G/3.95G [00:20<00:20, 86.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  61% 2.34G/3.86G [00:20<00:18, 82.8MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  64% 2.28G/3.56G [00:20<00:14, 86.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  48% 1.85G/3.86G [00:20<00:23, 85.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  55% 2.17G/3.95G [00:20<00:21, 83.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  61% 2.35G/3.86G [00:20<00:18, 80.9MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  64% 2.29G/3.56G [00:20<00:15, 83.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  48% 1.86G/3.86G [00:20<00:24, 82.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  55% 2.18G/3.95G [00:20<00:22, 79.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  61% 2.36G/3.86G [00:20<00:19, 77.1MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  65% 2.30G/3.56G [00:20<00:15, 78.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  48% 1.87G/3.86G [00:20<00:25, 79.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  56% 2.19G/3.95G [00:20<00:23, 74.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  49% 1.88G/3.86G [00:20<00:24, 80.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  61% 2.37G/3.86G [00:20<00:20, 73.7MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  65% 2.31G/3.56G [00:20<00:16, 73.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  56% 2.20G/3.95G [00:20<00:23, 74.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  62% 2.38G/3.86G [00:20<00:19, 74.8MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  65% 2.32G/3.56G [00:20<00:16, 74.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  56% 2.21G/3.95G [00:20<00:27, 63.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  62% 2.39G/3.86G [00:20<00:23, 62.0MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  65% 2.33G/3.56G [00:20<00:19, 62.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  49% 1.89G/3.86G [00:20<00:39, 50.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  49% 1.90G/3.86G [00:21<00:34, 57.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  56% 2.22G/3.95G [00:21<00:34, 50.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  62% 2.40G/3.86G [00:21<00:32, 44.7MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  66% 2.34G/3.56G [00:21<00:27, 44.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  49% 1.91G/3.86G [00:21<00:44, 43.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  57% 2.23G/3.95G [00:21<00:43, 39.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  62% 2.41G/3.86G [00:21<00:37, 38.3MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  66% 2.35G/3.56G [00:21<00:31, 38.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  50% 1.92G/3.86G [00:21<00:48, 40.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  57% 2.24G/3.95G [00:21<00:39, 43.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  63% 2.42G/3.86G [00:21<00:32, 44.3MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  66% 2.36G/3.56G [00:21<00:26, 44.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  50% 1.93G/3.86G [00:21<00:40, 47.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  57% 2.25G/3.95G [00:21<00:33, 51.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  63% 2.43G/3.86G [00:21<00:27, 52.8MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  67% 2.37G/3.56G [00:21<00:22, 53.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  50% 1.94G/3.86G [00:22<00:34, 56.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  58% 2.28G/3.95G [00:22<00:24, 68.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  63% 2.45G/3.86G [00:22<00:20, 70.4MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  67% 2.39G/3.56G [00:22<00:16, 70.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  51% 1.96G/3.86G [00:22<00:25, 75.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  58% 2.30G/3.95G [00:22<00:19, 85.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  64% 2.47G/3.86G [00:22<00:15, 87.3MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  68% 2.41G/3.56G [00:22<00:13, 87.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  51% 1.98G/3.86G [00:22<00:20, 91.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  59% 2.32G/3.95G [00:22<00:16, 102MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  65% 2.50G/3.86G [00:22<00:13, 104MB/s] \u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  68% 2.43G/3.56G [00:22<00:10, 104MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  52% 2.00G/3.86G [00:22<00:16, 111MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  69% 2.45G/3.56G [00:22<00:08, 124MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  60% 2.35G/3.95G [00:22<00:12, 133MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  65% 2.53G/3.86G [00:22<00:09, 137MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  52% 2.02G/3.86G [00:22<00:14, 125MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  70% 2.47G/3.56G [00:22<00:07, 142MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  66% 2.55G/3.86G [00:22<00:08, 152MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  60% 2.37G/3.95G [00:22<00:10, 145MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  53% 2.04G/3.86G [00:22<00:13, 139MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  70% 2.50G/3.56G [00:22<00:06, 155MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  66% 2.57G/3.86G [00:22<00:07, 162MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  61% 2.39G/3.95G [00:22<00:10, 152MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  53% 2.07G/3.86G [00:22<00:12, 149MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  71% 2.52G/3.56G [00:22<00:06, 153MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  54% 2.09G/3.86G [00:22<00:11, 152MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  61% 2.41G/3.95G [00:22<00:10, 143MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  67% 2.59G/3.86G [00:22<00:09, 141MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  71% 2.54G/3.56G [00:23<00:07, 128MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  55% 2.11G/3.86G [00:23<00:15, 112MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  68% 2.61G/3.86G [00:23<00:11, 107MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  62% 2.43G/3.95G [00:23<00:14, 102MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  72% 2.56G/3.56G [00:23<00:14, 68.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  55% 2.13G/3.86G [00:23<00:25, 67.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  73% 2.58G/3.56G [00:23<00:11, 82.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  56% 2.15G/3.86G [00:23<00:20, 84.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  62% 2.45G/3.95G [00:24<00:25, 57.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  68% 2.63G/3.86G [00:24<00:22, 54.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  56% 2.17G/3.86G [00:24<00:19, 87.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  62% 2.46G/3.95G [00:24<00:24, 59.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  69% 2.66G/3.86G [00:24<00:15, 79.6MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  73% 2.60G/3.56G [00:24<00:12, 74.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  57% 2.19G/3.86G [00:24<00:16, 104MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  63% 2.49G/3.95G [00:24<00:19, 76.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  70% 2.69G/3.86G [00:24<00:11, 105MB/s] \u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  74% 2.62G/3.56G [00:24<00:10, 89.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  64% 2.52G/3.95G [00:24<00:13, 107MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  71% 2.73G/3.86G [00:24<00:08, 129MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  57% 2.21G/3.86G [00:24<00:16, 103MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  71% 2.76G/3.86G [00:24<00:07, 157MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  74% 2.64G/3.56G [00:24<00:10, 86.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  58% 2.23G/3.86G [00:24<00:14, 110MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  65% 2.55G/3.95G [00:24<00:12, 113MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  72% 2.79G/3.86G [00:24<00:05, 180MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  75% 2.66G/3.56G [00:24<00:08, 102MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  58% 2.25G/3.86G [00:24<00:14, 115MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  73% 2.82G/3.86G [00:24<00:05, 202MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  75% 2.68G/3.56G [00:24<00:07, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  59% 2.28G/3.86G [00:24<00:12, 122MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  76% 2.71G/3.56G [00:24<00:06, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  59% 2.30G/3.86G [00:25<00:11, 136MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  65% 2.57G/3.95G [00:25<00:15, 86.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  74% 2.85G/3.86G [00:25<00:07, 128MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  77% 2.73G/3.56G [00:25<00:07, 104MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  60% 2.32G/3.86G [00:25<00:13, 111MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  66% 2.59G/3.95G [00:25<00:16, 83.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  75% 2.88G/3.86G [00:25<00:06, 151MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  77% 2.75G/3.56G [00:25<00:07, 113MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  66% 2.61G/3.95G [00:25<00:13, 100MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  75% 2.92G/3.86G [00:25<00:05, 168MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  61% 2.34G/3.86G [00:25<00:15, 100MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  78% 2.77G/3.56G [00:25<00:08, 96.3MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  76% 2.95G/3.86G [00:25<00:05, 153MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  67% 2.63G/3.95G [00:25<00:16, 78.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  77% 2.98G/3.86G [00:25<00:05, 173MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  78% 2.79G/3.56G [00:25<00:07, 98.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  61% 2.36G/3.86G [00:25<00:18, 80.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  61% 2.37G/3.86G [00:26<00:20, 72.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  67% 2.65G/3.95G [00:26<00:17, 73.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  78% 3.00G/3.86G [00:26<00:06, 129MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  79% 2.81G/3.56G [00:26<00:08, 86.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  68% 2.67G/3.95G [00:28<00:50, 25.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  78% 3.02G/3.86G [00:28<00:26, 32.3MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  80% 2.83G/3.56G [00:28<00:28, 25.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  62% 2.39G/3.86G [00:28<01:05, 22.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  80% 2.85G/3.56G [00:28<00:20, 34.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  69% 2.71G/3.95G [00:28<00:32, 38.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  79% 3.05G/3.86G [00:28<00:17, 45.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  62% 2.41G/3.86G [00:28<00:46, 31.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  81% 2.87G/3.56G [00:28<00:14, 45.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  63% 2.43G/3.86G [00:28<00:33, 42.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  69% 2.74G/3.95G [00:28<00:22, 54.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  80% 3.08G/3.86G [00:28<00:12, 62.2MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  81% 2.89G/3.56G [00:28<00:11, 58.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  70% 2.77G/3.95G [00:28<00:16, 73.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  63% 2.45G/3.86G [00:28<00:25, 54.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  81% 3.11G/3.86G [00:28<00:09, 81.4MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  82% 2.92G/3.56G [00:28<00:08, 73.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  71% 2.79G/3.95G [00:28<00:13, 86.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  64% 2.47G/3.86G [00:28<00:20, 68.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  81% 3.15G/3.86G [00:28<00:07, 102MB/s] \u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  83% 2.94G/3.56G [00:28<00:06, 90.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  65% 2.50G/3.86G [00:29<00:16, 84.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  71% 2.82G/3.95G [00:29<00:10, 109MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  82% 3.18G/3.86G [00:29<00:05, 123MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  83% 2.96G/3.56G [00:29<00:05, 108MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  65% 2.52G/3.86G [00:29<00:13, 101MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  72% 2.85G/3.95G [00:29<00:08, 134MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  84% 2.98G/3.56G [00:29<00:04, 124MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  83% 3.21G/3.86G [00:29<00:04, 142MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  66% 2.54G/3.86G [00:29<00:11, 115MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  73% 2.88G/3.95G [00:29<00:06, 158MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  84% 3.00G/3.56G [00:29<00:04, 135MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  84% 3.24G/3.86G [00:29<00:03, 164MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  66% 2.56G/3.86G [00:29<00:10, 127MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  74% 2.92G/3.95G [00:29<00:05, 179MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  85% 3.02G/3.56G [00:29<00:03, 150MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  85% 3.27G/3.86G [00:29<00:03, 183MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  86% 3.04G/3.56G [00:29<00:03, 159MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  67% 2.58G/3.86G [00:29<00:09, 134MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  75% 2.95G/3.95G [00:29<00:05, 185MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  67% 2.60G/3.86G [00:29<00:08, 145MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  86% 3.06G/3.56G [00:29<00:03, 159MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  85% 3.30G/3.86G [00:29<00:03, 173MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  68% 2.62G/3.86G [00:29<00:08, 154MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  75% 2.98G/3.95G [00:29<00:05, 166MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  87% 3.08G/3.56G [00:29<00:03, 142MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  68% 2.64G/3.86G [00:29<00:08, 138MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  86% 3.33G/3.86G [00:29<00:03, 145MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  76% 3.00G/3.95G [00:30<00:06, 142MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  87% 3.10G/3.56G [00:30<00:03, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  69% 2.66G/3.86G [00:30<00:10, 113MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  87% 3.36G/3.86G [00:30<00:04, 119MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  77% 3.02G/3.95G [00:30<00:08, 112MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  88% 3.12G/3.56G [00:30<00:04, 93.5MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  87% 3.38G/3.86G [00:30<00:05, 97.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  69% 2.68G/3.86G [00:30<00:13, 84.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  77% 3.04G/3.95G [00:30<00:10, 85.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  88% 3.15G/3.56G [00:30<00:05, 76.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  89% 3.16G/3.56G [00:30<00:05, 79.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  70% 2.71G/3.86G [00:30<00:14, 78.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  88% 3.40G/3.86G [00:30<00:05, 84.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  78% 3.06G/3.95G [00:30<00:10, 87.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  88% 3.42G/3.86G [00:31<00:04, 95.0MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  89% 3.18G/3.56G [00:31<00:04, 91.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  71% 2.73G/3.86G [00:31<00:12, 89.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  78% 3.08G/3.95G [00:31<00:08, 97.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  90% 3.20G/3.56G [00:31<00:03, 109MB/s] \u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  89% 3.44G/3.86G [00:31<00:03, 108MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  71% 2.75G/3.86G [00:31<00:10, 104MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  79% 3.10G/3.95G [00:31<00:07, 112MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  72% 2.77G/3.86G [00:31<00:09, 114MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  90% 3.46G/3.86G [00:31<00:03, 115MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  91% 3.22G/3.56G [00:31<00:02, 114MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  79% 3.12G/3.95G [00:31<00:07, 107MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  72% 2.79G/3.86G [00:36<01:27, 12.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  72% 2.80G/3.86G [00:36<01:15, 14.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  90% 3.48G/3.86G [00:36<00:31, 12.1MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  91% 3.24G/3.56G [00:36<00:27, 11.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  73% 2.81G/3.86G [00:36<01:01, 17.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  80% 3.15G/3.95G [00:36<01:06, 12.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  91% 3.51G/3.86G [00:37<00:18, 19.0MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  92% 3.26G/3.56G [00:37<00:18, 15.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  73% 2.83G/3.86G [00:37<00:41, 24.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  91% 3.53G/3.86G [00:37<00:13, 24.9MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  92% 3.28G/3.56G [00:37<00:12, 21.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  80% 3.17G/3.95G [00:37<00:48, 16.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  74% 2.84G/3.86G [00:37<00:35, 29.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  81% 3.19G/3.95G [00:37<00:34, 22.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  93% 3.30G/3.56G [00:37<00:08, 29.6MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  92% 3.55G/3.86G [00:37<00:09, 31.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  74% 2.86G/3.86G [00:37<00:24, 40.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  81% 3.21G/3.95G [00:37<00:24, 29.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  93% 3.32G/3.56G [00:37<00:06, 38.6MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  93% 3.58G/3.86G [00:37<00:07, 39.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  75% 2.88G/3.86G [00:37<00:17, 54.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  82% 3.23G/3.95G [00:37<00:19, 37.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  94% 3.34G/3.56G [00:37<00:04, 46.8MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  93% 3.60G/3.86G [00:37<00:06, 44.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  75% 2.90G/3.86G [00:37<00:16, 57.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  82% 3.25G/3.95G [00:37<00:16, 42.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  75% 2.92G/3.86G [00:37<00:15, 59.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  95% 3.37G/3.56G [00:38<00:03, 49.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  76% 2.93G/3.86G [00:38<00:15, 58.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  83% 3.26G/3.95G [00:38<00:15, 43.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  95% 3.38G/3.56G [00:38<00:03, 51.0MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  94% 3.62G/3.86G [00:38<00:05, 47.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  83% 3.27G/3.95G [00:38<00:15, 44.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  76% 2.94G/3.86G [00:38<00:16, 55.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  95% 3.39G/3.56G [00:38<00:03, 50.0MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  94% 3.63G/3.86G [00:38<00:05, 47.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  83% 3.28G/3.95G [00:38<00:14, 45.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  76% 2.95G/3.86G [00:38<00:17, 53.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  94% 3.64G/3.86G [00:38<00:04, 47.7MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  96% 3.40G/3.56G [00:38<00:03, 49.1MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  94% 3.65G/3.86G [00:38<00:04, 53.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  83% 3.29G/3.95G [00:38<00:13, 48.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  77% 2.96G/3.86G [00:38<00:16, 55.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  96% 3.42G/3.56G [00:38<00:02, 66.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  84% 3.32G/3.95G [00:38<00:07, 82.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  95% 3.68G/3.86G [00:38<00:02, 78.3MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  97% 3.44G/3.56G [00:39<00:01, 77.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  85% 3.34G/3.95G [00:39<00:06, 93.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  96% 3.69G/3.86G [00:39<00:02, 81.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  77% 2.98G/3.86G [00:39<00:15, 57.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  97% 3.46G/3.56G [00:39<00:01, 86.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  78% 3.00G/3.86G [00:39<00:11, 75.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  96% 3.71G/3.86G [00:39<00:01, 81.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  85% 3.37G/3.95G [00:39<00:06, 87.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  98% 3.48G/3.56G [00:39<00:00, 95.1MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  96% 3.72G/3.86G [00:39<00:01, 83.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  78% 3.02G/3.86G [00:39<00:10, 83.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  98% 3.50G/3.56G [00:39<00:00, 99.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  86% 3.39G/3.95G [00:39<00:06, 86.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  97% 3.74G/3.86G [00:39<00:01, 82.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  86% 3.40G/3.95G [00:39<00:06, 85.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  79% 3.04G/3.86G [00:39<00:09, 84.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors:  99% 3.52G/3.56G [00:39<00:00, 106MB/s] \u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  97% 3.76G/3.86G [00:39<00:01, 99.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  86% 3.41G/3.95G [00:39<00:07, 75.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  98% 3.79G/3.86G [00:39<00:00, 117MB/s] \u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors: 100% 3.54G/3.56G [00:39<00:00, 101MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  79% 3.05G/3.86G [00:39<00:11, 71.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  79% 3.06G/3.86G [00:40<00:10, 73.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  87% 3.42G/3.95G [00:40<00:07, 69.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  98% 3.81G/3.86G [00:40<00:00, 117MB/s]\u001b[A\n",
            "\n",
            "model-00004-of-00004.safetensors: 100% 3.56G/3.56G [00:40<00:00, 85.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00004-of-00004.safetensors: 100% 3.56G/3.56G [00:40<00:00, 88.3MB/s]\n",
            "\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  80% 3.08G/3.86G [00:40<00:09, 85.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors:  99% 3.83G/3.86G [00:40<00:00, 109MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  87% 3.45G/3.95G [00:40<00:05, 87.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  80% 3.10G/3.86G [00:40<00:07, 102MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00004.safetensors: 100% 3.86G/3.86G [00:40<00:00, 95.4MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  88% 3.46G/3.95G [00:40<00:05, 88.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  81% 3.12G/3.86G [00:40<00:07, 106MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  88% 3.48G/3.95G [00:40<00:04, 104MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  81% 3.15G/3.86G [00:40<00:05, 123MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  89% 3.51G/3.95G [00:40<00:03, 144MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  82% 3.17G/3.86G [00:40<00:05, 139MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  90% 3.53G/3.95G [00:41<00:05, 78.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  82% 3.19G/3.86G [00:41<00:09, 71.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  90% 3.55G/3.95G [00:41<00:04, 87.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  83% 3.21G/3.86G [00:41<00:07, 87.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  91% 3.59G/3.95G [00:41<00:03, 117MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  84% 3.24G/3.86G [00:41<00:05, 115MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  92% 3.62G/3.95G [00:41<00:02, 141MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  85% 3.27G/3.86G [00:41<00:04, 145MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  92% 3.65G/3.95G [00:41<00:01, 165MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  85% 3.30G/3.86G [00:41<00:03, 170MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  93% 3.68G/3.95G [00:42<00:01, 181MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  86% 3.33G/3.86G [00:42<00:02, 194MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  94% 3.71G/3.95G [00:42<00:01, 195MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  87% 3.37G/3.86G [00:42<00:02, 192MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  95% 3.74G/3.95G [00:42<00:01, 202MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  88% 3.40G/3.86G [00:42<00:02, 213MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  89% 3.43G/3.86G [00:42<00:01, 227MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  96% 3.77G/3.95G [00:42<00:00, 212MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  90% 3.46G/3.86G [00:42<00:01, 240MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  96% 3.81G/3.95G [00:42<00:00, 210MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  90% 3.49G/3.86G [00:42<00:01, 250MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  97% 3.84G/3.95G [00:42<00:00, 218MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  91% 3.52G/3.86G [00:42<00:01, 256MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  98% 3.87G/3.95G [00:42<00:00, 224MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  92% 3.55G/3.86G [00:42<00:01, 263MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  99% 3.90G/3.95G [00:42<00:00, 229MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  93% 3.59G/3.86G [00:43<00:01, 265MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors: 100% 3.93G/3.95G [00:43<00:00, 234MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors: 100% 3.95G/3.95G [00:43<00:00, 91.4MB/s]\n",
            "Fetching 4 files:  25% 1/4 [00:43<02:10, 43.38s/it]\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  94% 3.65G/3.86G [00:43<00:00, 267MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  95% 3.68G/3.86G [00:43<00:00, 269MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  96% 3.71G/3.86G [00:43<00:00, 266MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  97% 3.74G/3.86G [00:43<00:00, 269MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  98% 3.77G/3.86G [00:43<00:00, 272MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  98% 3.81G/3.86G [00:43<00:00, 267MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  99% 3.84G/3.86G [00:43<00:00, 264MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors: 100% 3.86G/3.86G [00:44<00:00, 87.7MB/s]\n",
            "Fetching 4 files: 100% 4/4 [00:44<00:00, 11.07s/it]\n",
            "[INFO|modeling_utils.py:2167] 2025-04-22 19:41:46,795 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1142] 2025-04-22 19:41:46,796 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:328] 2025-04-22 19:41:46,826 >> Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.13s/it]\n",
            "[INFO|modeling_utils.py:4930] 2025-04-22 19:41:51,407 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4938] 2025-04-22 19:41:51,407 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-7B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 243/243 [00:00<00:00, 1.34MB/s]\n",
            "[INFO|configuration_utils.py:1097] 2025-04-22 19:41:51,567 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/generation_config.json\n",
            "[INFO|configuration_utils.py:1142] 2025-04-22 19:41:51,568 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.05,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[INFO|2025-04-22 19:41:51] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-04-22 19:41:51] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-04-22 19:41:51] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-04-22 19:41:51] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-04-22 19:41:51] llamafactory.model.model_utils.misc:143 >> Found linear modules: k_proj,up_proj,o_proj,v_proj,gate_proj,q_proj,down_proj\n",
            "[INFO|2025-04-22 19:41:52] llamafactory.model.loader:143 >> trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643\n",
            "Applied Liger kernels to Qwen2\n",
            "[INFO|trainer.py:748] 2025-04-22 19:41:53,021 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2414] 2025-04-22 19:41:53,456 >> ***** Running training *****\n",
            "[INFO|trainer.py:2415] 2025-04-22 19:41:53,456 >>   Num examples = 1,700\n",
            "[INFO|trainer.py:2416] 2025-04-22 19:41:53,456 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2417] 2025-04-22 19:41:53,456 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2420] 2025-04-22 19:41:53,456 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2421] 2025-04-22 19:41:53,456 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2422] 2025-04-22 19:41:53,456 >>   Total optimization steps = 636\n",
            "[INFO|trainer.py:2423] 2025-04-22 19:41:53,460 >>   Number of trainable parameters = 20,185,088\n",
            "[INFO|integration_utils.py:831] 2025-04-22 19:41:53,475 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "{'loss': 2.3854, 'grad_norm': 3.5688629150390625, 'learning_rate': 7.031250000000001e-06, 'epoch': 0.05}\n",
            "{'loss': 2.1884, 'grad_norm': 3.4119162559509277, 'learning_rate': 1.484375e-05, 'epoch': 0.09}\n",
            "{'loss': 1.546, 'grad_norm': 6.4956793785095215, 'learning_rate': 2.2656250000000002e-05, 'epoch': 0.14}\n",
            "{'loss': 0.3984, 'grad_norm': 1.8745259046554565, 'learning_rate': 3.0468750000000002e-05, 'epoch': 0.19}\n",
            "{'loss': 0.1459, 'grad_norm': 5.255706787109375, 'learning_rate': 3.828125e-05, 'epoch': 0.24}\n",
            "{'loss': 0.2006, 'grad_norm': 1.7721480131149292, 'learning_rate': 4.609375e-05, 'epoch': 0.28}\n",
            "{'loss': 0.1246, 'grad_norm': 2.2046754360198975, 'learning_rate': 4.999057393530141e-05, 'epoch': 0.33}\n",
            "{'loss': 0.149, 'grad_norm': 3.2077975273132324, 'learning_rate': 4.9915208060686504e-05, 'epoch': 0.38}\n",
            "{'loss': 0.2203, 'grad_norm': 1.472448468208313, 'learning_rate': 4.976470359775556e-05, 'epoch': 0.42}\n",
            "{'loss': 0.2174, 'grad_norm': 0.6353673338890076, 'learning_rate': 4.953951443366266e-05, 'epoch': 0.47}\n",
            " 16% 100/636 [04:01<18:29,  2.07s/it][INFO|trainer.py:3984] 2025-04-22 19:51:42,167 >> Saving model checkpoint to /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/checkpoint-100\n",
            "[INFO|configuration_utils.py:693] 2025-04-22 19:51:42,481 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-22 19:51:42,482 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-22 19:51:42,744 >> tokenizer config file saved in /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/checkpoint-100/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-22 19:51:42,759 >> Special tokens file saved in /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/checkpoint-100/special_tokens_map.json\n",
            "{'loss': 0.1225, 'grad_norm': 1.0784389972686768, 'learning_rate': 4.924031968759688e-05, 'epoch': 0.52}\n",
            "{'loss': 0.1127, 'grad_norm': 0.7318646311759949, 'learning_rate': 4.886802166271364e-05, 'epoch': 0.56}\n",
            "{'loss': 0.1423, 'grad_norm': 2.924476385116577, 'learning_rate': 4.842374312499405e-05, 'epoch': 0.61}\n",
            "{'loss': 0.2044, 'grad_norm': 0.846530020236969, 'learning_rate': 4.7908823917238596e-05, 'epoch': 0.66}\n",
            "{'loss': 0.1175, 'grad_norm': 1.7117170095443726, 'learning_rate': 4.7324816918406744e-05, 'epoch': 0.71}\n",
            "{'loss': 0.1836, 'grad_norm': 2.555635452270508, 'learning_rate': 4.667348336048788e-05, 'epoch': 0.75}\n",
            "{'loss': 0.0999, 'grad_norm': 1.9499413967132568, 'learning_rate': 4.595678751702713e-05, 'epoch': 0.8}\n",
            "{'loss': 0.1559, 'grad_norm': 1.492857813835144, 'learning_rate': 4.5176890779324065e-05, 'epoch': 0.85}\n",
            "{'loss': 0.126, 'grad_norm': 2.4821832180023193, 'learning_rate': 4.433614513816925e-05, 'epoch': 0.89}\n",
            "{'loss': 0.1349, 'grad_norm': 1.1070570945739746, 'learning_rate': 4.3437086090776067e-05, 'epoch': 0.94}\n",
            " 31% 200/636 [07:47<18:06,  2.49s/it][INFO|trainer.py:3984] 2025-04-22 19:55:28,635 >> Saving model checkpoint to /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/checkpoint-200\n",
            "[INFO|configuration_utils.py:693] 2025-04-22 19:55:28,788 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-22 19:55:28,789 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-22 19:55:29,018 >> tokenizer config file saved in /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/checkpoint-200/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-22 19:55:29,021 >> Special tokens file saved in /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/checkpoint-200/special_tokens_map.json\n",
            "{'loss': 0.201, 'grad_norm': 0.7198502421379089, 'learning_rate': 4.248242499429915e-05, 'epoch': 0.99}\n",
            "{'loss': 0.1117, 'grad_norm': 0.7560024857521057, 'learning_rate': 4.1475040888999135e-05, 'epoch': 1.03}\n",
            "{'loss': 0.0804, 'grad_norm': 1.1235325336456299, 'learning_rate': 4.0417971815713584e-05, 'epoch': 1.08}\n",
            "{'loss': 0.114, 'grad_norm': 1.1172975301742554, 'learning_rate': 3.931440565381823e-05, 'epoch': 1.13}\n",
            "{'loss': 0.1154, 'grad_norm': 0.8367405533790588, 'learning_rate': 3.816767050730951e-05, 'epoch': 1.17}\n",
            "{'loss': 0.0677, 'grad_norm': 1.031691551208496, 'learning_rate': 3.6981224668001424e-05, 'epoch': 1.22}\n",
            "{'loss': 0.1483, 'grad_norm': 2.9118685722351074, 'learning_rate': 3.575864618610581e-05, 'epoch': 1.27}\n",
            "{'loss': 0.25, 'grad_norm': 2.7426650524139404, 'learning_rate': 3.4503622079648405e-05, 'epoch': 1.32}\n",
            "{'loss': 0.1073, 'grad_norm': 0.5624222755432129, 'learning_rate': 3.321993721526297e-05, 'epoch': 1.36}\n",
            "{'loss': 0.0925, 'grad_norm': 1.1999468803405762, 'learning_rate': 3.1911462893896214e-05, 'epoch': 1.41}\n",
            " 47% 300/636 [11:36<12:47,  2.28s/it][INFO|trainer.py:3984] 2025-04-22 19:59:17,580 >> Saving model checkpoint to /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/checkpoint-300\n",
            "[INFO|configuration_utils.py:693] 2025-04-22 19:59:17,840 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-22 19:59:17,841 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-22 19:59:18,066 >> tokenizer config file saved in /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/checkpoint-300/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-22 19:59:18,069 >> Special tokens file saved in /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/checkpoint-300/special_tokens_map.json\n",
            "{'loss': 0.0844, 'grad_norm': 1.8956326246261597, 'learning_rate': 3.058214517584654e-05, 'epoch': 1.46}\n",
            "{'loss': 0.0999, 'grad_norm': 1.002556324005127, 'learning_rate': 2.923599298034574e-05, 'epoch': 1.5}\n",
            "{'loss': 0.1171, 'grad_norm': 2.2023203372955322, 'learning_rate': 2.787706599557226e-05, 'epoch': 1.55}\n",
            "{'loss': 0.1275, 'grad_norm': 0.6066808104515076, 'learning_rate': 2.6509462435557152e-05, 'epoch': 1.6}\n",
            "{'loss': 0.0968, 'grad_norm': 0.984538733959198, 'learning_rate': 2.5137306680904643e-05, 'epoch': 1.64}\n",
            "{'loss': 0.1017, 'grad_norm': 1.9457571506500244, 'learning_rate': 2.3764736840600323e-05, 'epoch': 1.69}\n",
            "{'loss': 0.0958, 'grad_norm': 0.6893338561058044, 'learning_rate': 2.2395892272417758e-05, 'epoch': 1.74}\n",
            "{'loss': 0.131, 'grad_norm': 1.1056355237960815, 'learning_rate': 2.1034901099558915e-05, 'epoch': 1.79}\n",
            "{'loss': 0.0889, 'grad_norm': 2.715894937515259, 'learning_rate': 1.9685867761175584e-05, 'epoch': 1.83}\n",
            "{'loss': 0.1037, 'grad_norm': 2.289907217025757, 'learning_rate': 1.8352860634316458e-05, 'epoch': 1.88}\n",
            " 63% 400/636 [15:25<09:43,  2.47s/it][INFO|trainer.py:3984] 2025-04-22 20:03:06,882 >> Saving model checkpoint to /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/checkpoint-400\n",
            "[INFO|configuration_utils.py:693] 2025-04-22 20:03:06,989 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-22 20:03:06,989 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-22 20:03:07,212 >> tokenizer config file saved in /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/checkpoint-400/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-22 20:03:07,216 >> Special tokens file saved in /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/checkpoint-400/special_tokens_map.json\n",
            "{'loss': 0.1128, 'grad_norm': 1.1531909704208374, 'learning_rate': 1.7039899764629286e-05, 'epoch': 1.93}\n",
            "{'loss': 0.1065, 'grad_norm': 0.9549922943115234, 'learning_rate': 1.5750944742819562e-05, 'epoch': 1.97}\n",
            "{'loss': 0.068, 'grad_norm': 0.8332272171974182, 'learning_rate': 1.4489882763427501e-05, 'epoch': 2.02}\n",
            "{'loss': 0.0578, 'grad_norm': 1.435677170753479, 'learning_rate': 1.3260516901935346e-05, 'epoch': 2.07}\n",
            "{'loss': 0.0576, 'grad_norm': 1.3509221076965332, 'learning_rate': 1.2066554645558578e-05, 'epoch': 2.11}\n",
            "{'loss': 0.0592, 'grad_norm': 2.3332018852233887, 'learning_rate': 1.0911596712309619e-05, 'epoch': 2.16}\n",
            "{'loss': 0.076, 'grad_norm': 1.2991348505020142, 'learning_rate': 9.79912619205298e-06, 'epoch': 2.21}\n",
            "{'loss': 0.0969, 'grad_norm': 1.2394987344741821, 'learning_rate': 8.732498042300217e-06, 'epoch': 2.25}\n",
            "{'loss': 0.058, 'grad_norm': 0.8772835731506348, 'learning_rate': 7.714928970422816e-06, 'epoch': 2.3}\n",
            "{'loss': 0.0585, 'grad_norm': 1.383399248123169, 'learning_rate': 6.7494877327959335e-06, 'epoch': 2.35}\n",
            " 79% 500/636 [19:04<04:59,  2.20s/it][INFO|trainer.py:3984] 2025-04-22 20:06:44,982 >> Saving model checkpoint to /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/checkpoint-500\n",
            "[INFO|configuration_utils.py:693] 2025-04-22 20:06:45,092 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-22 20:06:45,093 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-22 20:06:45,363 >> tokenizer config file saved in /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-22 20:06:45,367 >> Special tokens file saved in /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.0527, 'grad_norm': 0.7133373022079468, 'learning_rate': 5.8390858801287195e-06, 'epoch': 2.4}\n",
            "{'loss': 0.0596, 'grad_norm': 0.19503629207611084, 'learning_rate': 4.986468976890993e-06, 'epoch': 2.44}\n",
            "{'loss': 0.0425, 'grad_norm': 1.4180091619491577, 'learning_rate': 4.19420832131665e-06, 'epoch': 2.49}\n",
            "{'loss': 0.0593, 'grad_norm': 0.6273914575576782, 'learning_rate': 3.4646931909542933e-06, 'epoch': 2.54}\n",
            "{'loss': 0.0441, 'grad_norm': 1.2389334440231323, 'learning_rate': 2.8001236371507674e-06, 'epoch': 2.58}\n",
            "{'loss': 0.0901, 'grad_norm': 1.3478262424468994, 'learning_rate': 2.2025038501977486e-06, 'epoch': 2.63}\n",
            "{'loss': 0.0519, 'grad_norm': 2.9264137744903564, 'learning_rate': 1.6736361151507062e-06, 'epoch': 2.68}\n",
            "{'loss': 0.0548, 'grad_norm': 1.5345518589019775, 'learning_rate': 1.2151153765479455e-06, 'epoch': 2.72}\n",
            "{'loss': 0.1099, 'grad_norm': 0.8263207077980042, 'learning_rate': 8.283244284214647e-07, 'epoch': 2.77}\n",
            "{'loss': 0.051, 'grad_norm': 0.45981940627098083, 'learning_rate': 5.144297441053353e-07, 'epoch': 2.82}\n",
            " 94% 600/636 [22:52<01:25,  2.39s/it][INFO|trainer.py:3984] 2025-04-22 20:10:33,363 >> Saving model checkpoint to /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/checkpoint-600\n",
            "[INFO|configuration_utils.py:693] 2025-04-22 20:10:33,470 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-22 20:10:33,471 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-22 20:10:33,708 >> tokenizer config file saved in /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/checkpoint-600/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-22 20:10:33,712 >> Special tokens file saved in /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/checkpoint-600/special_tokens_map.json\n",
            "{'loss': 0.0556, 'grad_norm': 1.5385217666625977, 'learning_rate': 2.7437795841801505e-07, 'epoch': 2.87}\n",
            "{'loss': 0.0612, 'grad_norm': 1.088114857673645, 'learning_rate': 1.0889301282752118e-07, 'epoch': 2.91}\n",
            "{'loss': 0.0571, 'grad_norm': 2.0158379077911377, 'learning_rate': 1.8473972208962742e-08, 'epoch': 2.96}\n",
            "100% 636/636 [24:20<00:00,  2.56s/it][INFO|trainer.py:3984] 2025-04-22 20:12:01,172 >> Saving model checkpoint to /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/checkpoint-636\n",
            "[INFO|configuration_utils.py:693] 2025-04-22 20:12:01,344 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-22 20:12:01,345 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-22 20:12:01,571 >> tokenizer config file saved in /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/checkpoint-636/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-22 20:12:01,575 >> Special tokens file saved in /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/checkpoint-636/special_tokens_map.json\n",
            "[INFO|trainer.py:2681] 2025-04-22 20:12:02,291 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1808.831, 'train_samples_per_second': 2.82, 'train_steps_per_second': 0.352, 'train_loss': 0.20272300476735494, 'epoch': 2.99}\n",
            "100% 636/636 [24:21<00:00,  2.30s/it]\n",
            "[INFO|trainer.py:3984] 2025-04-22 20:12:02,375 >> Saving model checkpoint to /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora\n",
            "[INFO|configuration_utils.py:693] 2025-04-22 20:12:02,485 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-22 20:12:02,486 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-22 20:12:02,713 >> tokenizer config file saved in /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-22 20:12:02,716 >> Special tokens file saved in /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =      2.9882\n",
            "  total_flos               = 154970136GF\n",
            "  train_loss               =      0.2027\n",
            "  train_runtime            =  0:30:08.83\n",
            "  train_samples_per_second =        2.82\n",
            "  train_steps_per_second   =       0.352\n",
            "[INFO|modelcard.py:450] 2025-04-22 20:12:02,981 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[1;34mwandb\u001b[0m: \u001b[1mwandb sync /content/LLaMA-Factory/wandb/offline-run-20250422_194740-a38i6m2r\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/offline-run-20250422_194740-a38i6m2r/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 8: Merge LoRA adapters into the base model (recommended if you want a standalone model)\n",
        "merge_config = {\n",
        "    \"model_name_or_path\": \"Qwen/Qwen2-7B-Instruct\",\n",
        "    \"adapter_name_or_path\": \"/content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora\",\n",
        "    \"template\": \"qwen\",\n",
        "    \"finetuning_type\": \"lora\",\n",
        "    \"export_dir\": \"/content/drive/MyDrive/sepi_master_thesis/models/qwen2_7b_lora_merged\",\n",
        "    \"export_size\": 2,\n",
        "    \"export_device\": \"cpu\"\n",
        "}\n",
        "with open(\"merge_qwen2_7b.json\", \"w\") as f:\n",
        "    json.dump(merge_config, f, indent=2)\n",
        "\n",
        "!llamafactory-cli export merge_qwen2_7b.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvp-_ET9_rvI",
        "outputId": "c5e09982-edd6-4482-a9cb-3b57ba762731",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-22 20:35:06.948284: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-22 20:35:06.965418: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745354106.986326   17080 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745354106.992658   17080 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 20:35:07.013672: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 20:35:13,661 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 20:35:13,661 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 20:35:13,661 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 20:35:13,661 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 20:35:13,661 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 20:35:13,661 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 20:35:13,662 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-22 20:35:14,001 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:693] 2025-04-22 20:35:14,298 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-22 20:35:14,300 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 20:35:14,376 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 20:35:14,376 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 20:35:14,376 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 20:35:14,376 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 20:35:14,376 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 20:35:14,376 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-22 20:35:14,376 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-22 20:35:14,724 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-04-22 20:35:14] llamafactory.data.template:143 >> Add <|im_end|> to stop words.\n",
            "[INFO|configuration_utils.py:693] 2025-04-22 20:35:14,806 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-22 20:35:14,806 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|2025-04-22 20:35:14] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
            "[INFO|modeling_utils.py:1124] 2025-04-22 20:35:14,861 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:2167] 2025-04-22 20:35:14,861 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1142] 2025-04-22 20:35:14,863 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:328] 2025-04-22 20:35:14,865 >> Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Loading checkpoint shards: 100% 4/4 [00:00<00:00, 51.81it/s]\n",
            "[INFO|modeling_utils.py:4930] 2025-04-22 20:35:14,982 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4938] 2025-04-22 20:35:14,982 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-7B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1097] 2025-04-22 20:35:15,024 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/generation_config.json\n",
            "[INFO|configuration_utils.py:1142] 2025-04-22 20:35:15,025 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.05,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[INFO|2025-04-22 20:35:15] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-04-22 20:35:39] llamafactory.model.adapter:143 >> Merged 1 adapter(s).\n",
            "[INFO|2025-04-22 20:35:39] llamafactory.model.adapter:143 >> Loaded adapter(s): /content/drive/MyDrive/sepi_master_thesis/finetuned_models/qwen_bio_nli_lora\n",
            "[INFO|2025-04-22 20:35:39] llamafactory.model.loader:143 >> all params: 7,615,616,512\n",
            "[INFO|2025-04-22 20:35:39] llamafactory.train.tuner:143 >> Convert model dtype to: torch.bfloat16.\n",
            "[INFO|configuration_utils.py:419] 2025-04-22 20:35:39,789 >> Configuration saved in /content/drive/MyDrive/sepi_master_thesis/models/qwen2_7b_lora_merged/config.json\n",
            "[INFO|configuration_utils.py:911] 2025-04-22 20:35:39,794 >> Configuration saved in /content/drive/MyDrive/sepi_master_thesis/models/qwen2_7b_lora_merged/generation_config.json\n",
            "[INFO|modeling_utils.py:3580] 2025-04-22 20:36:28,419 >> The model is bigger than the maximum size per checkpoint (2GB) and is going to be split in 9 checkpoint shards. You can find where each parameters has been saved in the index located at /content/drive/MyDrive/sepi_master_thesis/models/qwen2_7b_lora_merged/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-22 20:36:28,430 >> tokenizer config file saved in /content/drive/MyDrive/sepi_master_thesis/models/qwen2_7b_lora_merged/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-22 20:36:28,439 >> Special tokens file saved in /content/drive/MyDrive/sepi_master_thesis/models/qwen2_7b_lora_merged/special_tokens_map.json\n",
            "[INFO|2025-04-22 20:36:28] llamafactory.train.tuner:143 >> Ollama modelfile saved in /content/drive/MyDrive/sepi_master_thesis/models/qwen2_7b_lora_merged/Modelfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run merge\n",
        "!llamafactory-cli export merge_qwen2_7b.json"
      ],
      "metadata": {
        "id": "ZB9W5xcM_vyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_path = '/content/drive/MyDrive/sepi_master_thesis/models/qwen2_7b_lora_merged'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, trust_remote_code=True) # Removed device_map=\"auto\" as it's redundant when loading to CPU.\n",
        "model.eval() # Set the model to evaluation mode to disable dropout and other training-specific features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "FGeo2IrHJXro",
        "outputId": "06d62cca-efe3-4848-cd88-f4c85e2340d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HFValidationError",
          "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/sepi_master_thesis/models/qwen2_7b_lora_merged'. Use `repo_type` argument if needed.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    425\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/sepi_master_thesis/models/qwen2_7b_lora_merged'. Use `repo_type` argument if needed.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-78c305ba7f39>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/sepi_master_thesis/models/qwen2_7b_lora_merged'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Removed device_map=\"auto\" as it's redundant when loading to CPU.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Set the model to evaluation mode to disable dropout and other training-specific features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \"\"\"\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;31m# Now we try to recover if we can find all files correctly in the cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         resolved_files = [\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0m_get_cache_file_to_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         ]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;31m# Now we try to recover if we can find all files correctly in the cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         resolved_files = [\n\u001b[0;32m--> 471\u001b[0;31m             \u001b[0m_get_cache_file_to_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m         ]\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresolved_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36m_get_cache_file_to_return\u001b[0;34m(path_or_repo_id, full_filename, cache_dir, revision)\u001b[0m\n\u001b[1;32m    132\u001b[0m ):\n\u001b[1;32m    133\u001b[0m     \u001b[0;31m# We try to see if we have a cached version (not up to date):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mresolved_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtry_to_load_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresolved_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresolved_file\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_CACHED_NO_EXIST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresolved_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         ):\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"token\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;34mf\" '{repo_id}'. Use `repo_type` argument if needed.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/sepi_master_thesis/models/qwen2_7b_lora_merged'. Use `repo_type` argument if needed."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "AlEyRABn6hwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Load test set\n",
        "#test_path = \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/test_format_a_statement_in_input.json\"\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/test-zero-shot.csv\")\n",
        "\n",
        "predictions = []\n",
        "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Running model predictions\"):\n",
        "    full_prompt = row[\"text\"]\n",
        "    inputs = tokenizer(\n",
        "    full_prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True,\n",
        "    max_length=4096\n",
        "    ).to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        eos_token_id=tokenizer.eos_token_id,  # <- important!\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        max_new_tokens=3)\n",
        "    # Decode and remove original input from the result\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    pred = decoded.replace(full_prompt, \"\").strip()\n",
        "\n",
        "    predictions.append(pred)\n",
        "df[\"predicted_output\"] = predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "gughqUVUx-KC",
        "outputId": "2d4cb01e-fe00-49f6-a652-2d9bf5471c82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running model predictions:   0%|          | 0/10 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 24.12 MiB is free. Process 83010 has 14.71 GiB memory in use. Of the allocated memory 14.46 GiB is allocated by PyTorch, and 132.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-fd47315decf3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     ).to(model.device)\n\u001b[0;32m---> 18\u001b[0;31m     outputs = model.generate(\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0meos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# <- important!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2451\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2452\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2453\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2454\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3417\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3418\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3419\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3420\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m             layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    549\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 24.12 MiB is free. Process 83010 has 14.71 GiB memory in use. Of the allocated memory 14.46 GiB is allocated by PyTorch, and 132.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.read_csv(\"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/test-zero-shot.csv\")\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "UrCdFCsF3TSk",
        "outputId": "f97a42ef-cd5f-457f-c7ba-99fb29648efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text          label  \\\n",
              "0  Primary trial evidence are: ['Inclusion Criter...  Contradiction   \n",
              "1  Primary trial evidence are: ['INTERVENTION 1: ...     Entailment   \n",
              "2  Primary trial evidence are: ['Outcome Measurem...  Contradiction   \n",
              "3  Primary trial evidence are: ['Outcome Measurem...  Contradiction   \n",
              "4  Primary trial evidence are: ['Outcome Measurem...     Entailment   \n",
              "5  Primary trial evidence are: ['Adverse Events 1...  Contradiction   \n",
              "6  Primary trial evidence are: ['Outcome Measurem...     Entailment   \n",
              "7  Primary trial evidence are: ['DISEASE CHARACTE...  Contradiction   \n",
              "8  Primary trial evidence are: ['Outcome Measurem...     Entailment   \n",
              "9  Primary trial evidence are: ['Inclusion Criter...  Contradiction   \n",
              "\n",
              "  predicted_output  \n",
              "0    contradiction  \n",
              "1    contradiction  \n",
              "2       entailment  \n",
              "3    contradiction  \n",
              "4    contradiction  \n",
              "5    contradiction  \n",
              "6    contradiction  \n",
              "7       entailment  \n",
              "8    contradiction  \n",
              "9       entailment  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5b173cab-3745-4899-b510-856877014694\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>predicted_output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Primary trial evidence are: ['Inclusion Criter...</td>\n",
              "      <td>Contradiction</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Primary trial evidence are: ['INTERVENTION 1: ...</td>\n",
              "      <td>Entailment</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Primary trial evidence are: ['Outcome Measurem...</td>\n",
              "      <td>Contradiction</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Primary trial evidence are: ['Outcome Measurem...</td>\n",
              "      <td>Contradiction</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Primary trial evidence are: ['Outcome Measurem...</td>\n",
              "      <td>Entailment</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Primary trial evidence are: ['Adverse Events 1...</td>\n",
              "      <td>Contradiction</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Primary trial evidence are: ['Outcome Measurem...</td>\n",
              "      <td>Entailment</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Primary trial evidence are: ['DISEASE CHARACTE...</td>\n",
              "      <td>Contradiction</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Primary trial evidence are: ['Outcome Measurem...</td>\n",
              "      <td>Entailment</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Primary trial evidence are: ['Inclusion Criter...</td>\n",
              "      <td>Contradiction</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b173cab-3745-4899-b510-856877014694')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5b173cab-3745-4899-b510-856877014694 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5b173cab-3745-4899-b510-856877014694');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dd740c2c-b80d-4621-a56a-3bb6e77915bc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dd740c2c-b80d-4621-a56a-3bb6e77915bc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dd740c2c-b80d-4621-a56a-3bb6e77915bc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_a888f64d-8fc8-4e27-943d-f6953661e46e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a888f64d-8fc8-4e27-943d-f6953661e46e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Primary trial evidence are: ['Outcome Measurement: ', '  Number of Participants With Treatment-Emergent Adverse Events (AEs) or Serious Adverse Events (SAEs)', '  An AE was any untoward medical occurrence in a participant who received study drug without regard to possibility of causal relationship. A SAE was an AE resulting in any of following outcomes or deemed significant for any other reason: death; initial or prolonged inpatient hospitalization; life-threatening experience (immediate risk of dying); persistent or significant disability/incapacity; congenital anomaly. Treatment-emergent are events between first dose of study drug and up to 28 days after last dose that were absent before treatment or that worsened relative to pretreatment state.', '  Time frame: From screening until 28 days post last dose of study drug', 'Results 1: ', '  Arm/Group Title: Sunitinib + Docetaxel + Trastuzumab', '  Arm/Group Description: Sunitinib 37.5 milligram (mg) capsule orally once daily continuously starting from Day 2 up to Day 15 in each cycle, in schedule 2/1 (2 week on treatment, 1 week off treatment) along with docetaxel 75 milligram/square meter (mg/m^2) intravenous infusion over 1 hour on Day 1 of each cycle and trastuzumab either weekly: loading dose of 4 milligram/kilogram (mg/kg) intravenous infusion over 90 minutes on Day 1 followed by weekly maintenance doses of 2 mg/kg intravenous infusion over 30 minutes; or every 3 weeks: loading dose of 8 mg/kg intravenous infusion over 90 minutes on Day 1 followed by maintenance doses of 6 mg/kg intravenous infusion over 90 minutes every 3 weeks. Cycle length was 3 weeks.', '  Overall Number of Participants Analyzed: 25', '  Measure Type: Number', '  Unit of Measure: participants  AEs: 24', 'SAEs: 11'] \\n Question: Does this imply that All but one of the patients treated with Sunitinib + Docetaxel + Trastuzumab in the primary trial suffered a Treatment-Emergent adverse event, and less than half the patients suffered serious Treatment-Emergent Adverse Events.? Answer with only one of the options (Entailment or Contradiction):\",\n          \"Primary trial evidence are: ['INTERVENTION 1: ', '  Gefitinib (ZD1839)', '  ZD1839 at a daily dose of 250 mg or 500 mg depending on final dose in parent trial']\\nSecondary trial evidence are: ['INTERVENTION 1: ', '  Zoledronic Acid', '  Zoledronic acid, vitamin D and calcium supplements.', 'INTERVENTION 2: ', '  Zoledronic Acid + Radiopharmaceuticals', '  Zoledronic acid, vitamin D and calcium supplements, plus Sr-89 or Sm-153.'] \\n Question: Does this imply that The intervention in the primary trial consists of a single drug, whereas in the secondary trial the intervention requires at least 3 different drugs.? Answer with only one of the options (Entailment or Contradiction):\",\n          \"Primary trial evidence are: ['Adverse Events 1:', '  Total: 10/30 (33.33%)', '  Hemoglobin decreased 2/30 (6.67%)', '  Abdominal pain 1/30 (3.33%)', '  Colitis 1/30 (3.33%)', '  Diarrhea 7/30 (23.33%)', '  Nausea 2/30 (6.67%)', '  Rectal hemorrhage 1/30 (3.33%)', '  Fatigue 1/30 (3.33%)', '  Skin infection 1/30 (3.33%)', '  Neutrophil count decreased 1/30 (3.33%)', '  Platelet count decreased 3/30 (10.00%)', '  Dehydration 1/30 (3.33%)']\\nSecondary trial evidence are: ['Adverse Events 1:', '  Total: 3/6 (50.00%)', '  Anaemia 0/6 (0.00%)', '  Febrile neutropenia 0/6 (0.00%)', '  Neutropenia 0/6 (0.00%)', '  Bradycardia 0/6 (0.00%)', '  Diarrhoea 0/6 (0.00%)', '  Pancreatitis 0/6 (0.00%)', '  Vomiting 0/6 (0.00%)', '  Disease progression 0/6 (0.00%)', '  Fatigue 0/6 (0.00%)', '  Pyrexia 0/6 (0.00%)', '  Cholelithiasis 0/6 (0.00%)', '  Hepatic pain 0/6 (0.00%)', '  Bacteraemia 0/6 (0.00%)', 'Adverse Events 2:', '  Total: 3/6 (50.00%)', '  Anaemia 0/6 (0.00%)', '  Febrile neutropenia 0/6 (0.00%)', '  Neutropenia 1/6 (16.67%)', '  Bradycardia 0/6 (0.00%)', '  Diarrhoea 0/6 (0.00%)', '  Pancreatitis 0/6 (0.00%)', '  Vomiting 0/6 (0.00%)', '  Disease progression 0/6 (0.00%)', '  Fatigue 1/6 (16.67%)', '  Pyrexia 1/6 (16.67%)', '  Cholelithiasis 0/6 (0.00%)', '  Hepatic pain 1/6 (16.67%)', '  Bacteraemia 0/6 (0.00%)'] \\n Question: Does this imply that Cholelithiasis was twice as common for the primary trial participants than the secondary trial participants? Answer with only one of the options (Entailment or Contradiction):\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Entailment\",\n          \"Contradiction\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_output\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"entailment\",\n          \"contradiction\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}