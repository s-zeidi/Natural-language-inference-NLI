{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-zeidi/Natural-language-inference-NLI/blob/main/code_03072025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYIp7ctdCIaQ"
      },
      "source": [
        "# 1. Initial settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRHNynd60oPz"
      },
      "outputs": [],
      "source": [
        "# !pip install sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "import numpy\n",
        "from sklearn.metrics import f1_score,precision_score,recall_score\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "import tqdm\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import pandas as pd\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "2kC0QNgcgUrv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uh7CPiU-CK_H"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "#file_path='./data/Complete_dataset/'\n",
        "file_path='/content/drive/MyDrive/sepi_master_thesis/Complete_dataset/'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N81fbR7lTYNJ"
      },
      "source": [
        "# 2. Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zhbEtfHFAU2"
      },
      "outputs": [],
      "source": [
        "# !unzip /content/drive/MyDrive/sepi_master_thesis/NLI/Task-2-SemEval-2024/training_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FphSbtv4Twc7"
      },
      "outputs": [],
      "source": [
        "# Dev set #200\n",
        "dev_path = file_path + 'dev.json'\n",
        "\n",
        "with open(dev_path) as json_file:\n",
        "    dev = json.load(json_file)\n",
        "\n",
        "\n",
        "# Example instance\n",
        "print(dev[list(dev.keys())[1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VJcq1qnirr7"
      },
      "outputs": [],
      "source": [
        "# train set #1700\n",
        "train_path = file_path + 'train.json'\n",
        "with open(train_path) as json_file:\n",
        "    train = json.load(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KaAqX4rm6B-"
      },
      "outputs": [],
      "source": [
        "# test set #500\n",
        "test_path = file_path + 'test.json'\n",
        "with open(test_path) as json_file:\n",
        "    test = json.load(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO4-8YVyoErq"
      },
      "outputs": [],
      "source": [
        "print(len(dev))\n",
        "print(len(test))\n",
        "print(len(train))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "collapsed": true,
        "id": "REK4FGL1GlVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYtfOx_A1kO5"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing  Qwen fintuning format d\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "# === Load CSV ===\n",
        "csv_path = \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/test_table.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# === Filter for correct reasoning only ===\n",
        "#df = df[df[\"Reasoning_status\"].str.strip().str.lower() == \"correct\"]\n",
        "\n",
        "# === Format D Output List ===\n",
        "format_d_list = []\n",
        "\n",
        "# === Process Each Example ===\n",
        "for _, row in df.iterrows():\n",
        "    primary = str(row.get(\"primary_section\", \"\")).strip()\n",
        "    secondary = row.get(\"secondary_section\", \"\")\n",
        "    if pd.isna(secondary) or str(secondary).strip().upper() == \"N/A\":\n",
        "        secondary = \"\"\n",
        "    else:\n",
        "        secondary = str(secondary).strip()\n",
        "\n",
        "    statement = str(row.get(\"statements\", \"\")).strip()\n",
        "    label = str(row.get(\"label\", \"\")).strip()\n",
        "    reasoning = str(row.get(\"Reasoning\", \"\")).strip()\n",
        "\n",
        "    # === Clean reasoning ===\n",
        "    if reasoning.startswith(\"</think>\"):\n",
        "        reasoning = reasoning.replace(\"</think>\", \"\", 1).strip()\n",
        "\n",
        "    # === Input Prompt ===\n",
        "    input_lines = [\n",
        "        \"You are a clinical reasoning assistant.\",\n",
        "        \"Please provide exactly 4 clear and concise reasoning steps that justify whether the statement should be labeled as Entailment or Contradiction, based on the trial information.\",\n",
        "        \"Use only evidence from the trial(s) — avoid assumptions or external knowledge.\",\n",
        "        \"Please follow this format strictly:\",\n",
        "        \"Step-by-step Explanation:\",\n",
        "        \"Step 1: ...\",\n",
        "        \"Step 2: ...\",\n",
        "        \"Step 3: ...\",\n",
        "        \"Step 4: ...\",\n",
        "        \"Final Conclusion: The label (Entailment or Contradiction) is correct because ...\",\n",
        "        \"Label: <Entailment or Contradiction>.\",\n",
        "        f\"\\nPrimary Trial: {primary}\"\n",
        "    ]\n",
        "    if secondary:\n",
        "        input_lines.append(f\"\\nSecondary Trial: {secondary}\")\n",
        "    input_lines.append(f\"\\nStatement: {statement}\")\n",
        "\n",
        "    input_text = \"\\n\".join(input_lines)\n",
        "    #output_text = f\"\\n{reasoning}\\nLabel: {label}\"\n",
        "    output_text = f\"{label}\"\n",
        "\n",
        "    format_d_list.append({\n",
        "        #\"instruction\":\"Based on the clinical trial information and the statement, predict whether the label is Entailment or Contradiction. Provide four clear reasoning steps, followed by a final conclusion and the predicted label in the specified format.\",\n",
        "        \"input\": input_text,\n",
        "        #\"output\": output_text,\n",
        "        \"label\": output_text\n",
        "    })\n",
        "\n",
        "# === Save to JSON ===\n",
        "#output_path = \"/content/drive/MyDrive/sepi_master_thesis/qwen_format_d/dev.json\"  # ⬅️ Change to your output path\n",
        "#with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    #json.dump(format_d_list, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "#print(f\"✅ Saved {len(format_d_list)} examples to '{output_path}'\")\n",
        "\n",
        "# === Save as CSV ===\n",
        "csv_output_path = \"/content/drive/MyDrive/sepi_master_thesis/qwen_format_d/test.csv\"\n",
        "csv_df = pd.DataFrame(format_d_list)\n",
        "csv_df.to_csv(csv_output_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"✅ Also saved {len(format_d_list)} examples to '{csv_output_path}'\")"
      ],
      "metadata": {
        "id": "DXw7sMu9rdvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#format_d_list[2]['input'].split('\\n')\n",
        "#format_d_list[100]['output'].split('\\n')\n",
        "#format_d_list[1]['instruction'].split('\\n')\n",
        "csv_df['input'].iloc[0].split('\\n')"
      ],
      "metadata": {
        "id": "P29oN-4i8nwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing  Qwen fintuning format\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# === File Paths ===\n",
        "#csv_path = \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/dev.csv\"\n",
        "csv_path = \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_03122024/test_table.csv\"\n",
        "output_dir = \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/\"\n",
        "\n",
        "# === Load CSV ===\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# === Output Lists ===\n",
        "format_a_list = []  # Statement in input\n",
        "format_b_list = []  # Statement in instruction\n",
        "\n",
        "# === Process Each Example ===\n",
        "for _, row in df.iterrows():\n",
        "    primary = str(row.get(\"primary_section\", \"\")).strip()\n",
        "    #secondary = str(row.get(\"secondary_section\", \"\")).strip()\n",
        "    secondary = row.get(\"secondary_section\", \"\")\n",
        "    if pd.isna(secondary) or str(secondary).strip().upper() == \"N/A\":\n",
        "        secondary = \"\"\n",
        "    else:\n",
        "        secondary = str(secondary).strip()\n",
        "\n",
        "    statement = str(row.get(\"statements\", \"\")).strip()\n",
        "    #label = str(row.get(\"label\", \"\")).strip().lower()\n",
        "\n",
        "    if not primary or not statement: #or not label:\n",
        "        continue  # skip incomplete rows\n",
        "\n",
        "    # === Format A: Statement in INPUT ===\n",
        "    input_lines_a = [f\"Primary Trial: {primary}\"]\n",
        "    if secondary:\n",
        "        input_lines_a.append(f\"Secondary Trial: {secondary}\")\n",
        "    input_lines_a.append(f\"Statement: {statement}\")\n",
        "    input_text_a = \"\\n\".join(input_lines_a)\n",
        "\n",
        "    entry_a = {\n",
        "        \"instruction\": \"Do the clinical trial descriptions imply the given statement? Answer with only one of the options (Entailment or Contradiction):\",\n",
        "        \"input\": input_text_a,\n",
        "        #\"output\": label\n",
        "    }\n",
        "    format_a_list.append(entry_a)\n",
        "\n",
        "    # === Format B: Statement in INSTRUCTION ===\n",
        "    input_lines_b = [f\"Primary Trial: {primary}\"]\n",
        "    if secondary:\n",
        "        input_lines_b.append(f\"Secondary Trial: {secondary}\")\n",
        "    input_text_b = \"\\n\".join(input_lines_b)\n",
        "\n",
        "    instruction_b = (\n",
        "        f\"Question: Do the clinical trial descriptions imply that {statement}? \"\n",
        "        f\"Answer with only one of the options (Entailment or Contradiction):\"\n",
        "    )\n",
        "\n",
        "    entry_b = {\n",
        "        \"instruction\": instruction_b,\n",
        "        \"input\": input_text_b,\n",
        "        #\"output\": label\n",
        "    }\n",
        "    format_b_list.append(entry_b)\n",
        "\n",
        "# === Save Both JSON Files ===\n",
        "file_a = os.path.join(output_dir, \"format_a_statement_in_input.json\")\n",
        "file_b = os.path.join(output_dir, \"format_b_statement_in_instruction.json\")\n",
        "\n",
        "with open(file_a, \"w\", encoding=\"utf-8\") as f1:\n",
        "    json.dump(format_a_list, f1, indent=2, ensure_ascii=False)\n",
        "\n",
        "with open(file_b, \"w\", encoding=\"utf-8\") as f2:\n",
        "    json.dump(format_b_list, f2, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"✅ Saved {len(format_a_list)} examples to '{file_a}'\")\n",
        "print(f\"✅ Saved {len(format_b_list)} examples to '{file_b}'\")\n"
      ],
      "metadata": {
        "id": "w1MHJq048gfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "-wF16CMDDAEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "format_a_list[1696]['input'].split('\\n')\n"
      ],
      "metadata": {
        "id": "OlC2egg6CN87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_text(premise, hypothesis):\n",
        "    input_text = f\"{premise} \\n Question: Does this imply that {hypothesis}? Answer with only one of the options (Entailment or Contradiction):\"\n",
        "    return input_text"
      ],
      "metadata": {
        "id": "gOqIg35WM8r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67dllYY89-WA"
      },
      "outputs": [],
      "source": [
        "def get_input_text_2shot(premise, hypothesis): # this is for mistral(optimized)\n",
        "    options_prefix = \"OPTIONS:\\n- \"\n",
        "    separator = \"\\n- \"\n",
        "    options_ = options_prefix + f\"{separator}\".join([\"Entailment\", \"Contradiction\"])\n",
        "\n",
        "    # Two-shot examples\n",
        "    example_1=\"\"\"Primary trial evidence are Inclusion Criteria: Age 52-75 years old; Identification as Latina/Hispanic/Chicana female;\n",
        "    Residence in Pilsen, Little Village, East Side or South Chicago; No history of health volunteerism; No history of breast cancer; and Lack of\n",
        "    a mammogram within the last two yearsExclusion Criteria: Not meeting all inclusion criteria; Women will be excluded if they participated in\n",
        "    formative focus groups Question: Does this imply that Patients eligible for the primary trial must live in the USA?\n",
        "    Answer with only one of the options (Entailment or Contradiction): Entailment\"\"\"\n",
        "\n",
        "    example_2=\"\"\"Primary trial evidence are Adverse Events 1: Total: 10/71 (14.08%) ATRIAL FIBRILLATION 1/71 (1.41%) CARDIAC TAMPONADE 1/71 (1.41%)\n",
        "    PERICARDIAL EFFUSION 1/71 (1.41%) SUPRAVENTRICULAR TACHYCARDIA 1/71 (1.41%) DIARRHOEA 1/71 (1.41%) NAUSEA 1/71 (1.41%) VOMITING 1/71 (1.41%)\n",
        "    CHEST PAIN 1/71 (1.41%) PNEUMONIA 1/71 (1.41%) MALIGNANT PLEURAL EFFUSION 1/71 (1.41%) HEPATIC ENCEPHALOPATHY 1/71 (1.41%)\n",
        "    Question: Does this imply that There were only 3 adverse events in the primary trial which occurred more than twice?\n",
        "    Answer with only one of the options (Entailment or Contradiction): Contradiction\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   # Introduction text with examples\n",
        "    example_intro = \"Here are my first examples: \\n \"\n",
        "    example_intro2 = \"Here are my second examples: \\n \"\n",
        "\n",
        "    # Example text (Two-shot)\n",
        "    two_shot_text = (\n",
        "        f\"{example_intro}\"\n",
        "        f\"Example 1: {example_1} \\n \"\n",
        "        f\"{example_intro2}\"\n",
        "        f\"Example 2: {example_2} \\n \")\n",
        "\n",
        "\n",
        "    # For the actual input\n",
        "    input_text = f\"{premise} \\n Question: Does this imply that {hypothesis}? Answer with only one of the options (Entailment or Contradiction):\"\n",
        "\n",
        "    # Combine two-shot examples with the actual input\n",
        "    return two_shot_text + input_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_text2_2shot(premise, hypothesis):\n",
        "    options_prefix = \"OPTIONS:\\n- \"\n",
        "    separator = \"\\n- \"\n",
        "    options_ = options_prefix + f\"{separator}\".join([\"Entailment\", \"Contradiction\"])\n",
        "\n",
        "    example_1 = \"\"\"Example 1:\n",
        "    Clinical Trial Report:\n",
        "    Primary Trial: Inclusion Criteria: Age 52-75 years old; Identification as Latina/Hispanic/Chicana female;\n",
        "    Residence in Pilsen, Little Village, East Side or South Chicago; No history of health volunteerism; No history of breast cancer; and Lack of\n",
        "    a mammogram within the last two years Exclusion Criteria: Not meeting all inclusion criteria; Women will be excluded if they participated in\n",
        "    formative focus groups.\n",
        "\n",
        "    Statement: Patients eligible for the primary trial must live in the USA.\n",
        "    Label: Entailment\"\"\"\n",
        "\n",
        "    example_2 = \"\"\"Example 2:\n",
        "    Clinical Trial Report:\n",
        "    Primary Trial: Adverse Events 1: Total: 10/71 (14.08%) ATRIAL FIBRILLATION 1/71 (1.41%) CARDIAC TAMPONADE 1/71 (1.41%)\n",
        "    PERICARDIAL EFFUSION 1/71 (1.41%) SUPRAVENTRICULAR TACHYCARDIA 1/71 (1.41%) DIARRHOEA 1/71 (1.41%) NAUSEA 1/71 (1.41%) VOMITING 1/71 (1.41%)\n",
        "    CHEST PAIN 1/71 (1.41%) PNEUMONIA 1/71 (1.41%) MALIGNANT PLEURAL EFFUSION 1/71 (1.41%) HEPATIC ENCEPHALOPATHY 1/71 (1.41%).\n",
        "\n",
        "\n",
        "    Statement: There were only 3 adverse events in the primary trial which occurred more than twice.\n",
        "    Label: Contradiction\"\"\"\n",
        "\n",
        "    example_intro = \"Below are two examples:\"\n",
        "    example_intro2 = \"Now, classify the following example using only 'ENTAILMENT' or 'CONTRADICTION'.\"\n",
        "\n",
        "    two_shot_text = (\n",
        "        f\"{example_intro}\\n\\n\"\n",
        "        f\"{example_1} \\n\\n\"\n",
        "        f\"{example_2} \\n\\n\"\n",
        "        f\"{example_intro2}\\n\\n\"\n",
        "    )\n",
        "\n",
        "    input_text = (\n",
        "        f\"Clinical Trial Report:\\n\"\n",
        "        f\"{premise}\\n\"\n",
        "        f\"Statement: {hypothesis}\\n\"\n",
        "        f\"Label:('Entailment' or 'Contradiction')?\"\n",
        "        )\n",
        "\n",
        "    return two_shot_text + input_text\n"
      ],
      "metadata": {
        "id": "9PMZWf2usOV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_text3(premise, hypothesis):\n",
        "    options_prefix = \"OPTIONS:\\n- \"\n",
        "    separator = \"\\n- \"\n",
        "    options_ = options_prefix + f\"{separator}\".join([\"Entailment\", \"Contradiction\"])\n",
        "\n",
        "    example_intro2 = \"Classify the following example using only 'ENTAILMENT' or 'CONTRADICTION'.\"\n",
        "\n",
        "    zero_shot_text = (\n",
        "        f\"{example_intro2}\\n\\n\"\n",
        "    )\n",
        "\n",
        "    input_text = (\n",
        "        f\"Clinical Trial Report:\\n\"\n",
        "        f\"{premise}\\n\"\n",
        "        f\"Statement: {hypothesis}\\n\"\n",
        "        f\"Label:('Entailment' or 'Contradiction')?\"\n",
        "        )\n",
        "\n",
        "    return zero_shot_text + input_text\n"
      ],
      "metadata": {
        "id": "WsWuIkVp_Kvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_cot(premise, hypothesis, label):\n",
        "    input_text = (\n",
        "        f\"Clinical Trial Report:\\n\"\n",
        "        f\"{premise}\\n\"\n",
        "        f\"Statement: {hypothesis}\\n\"\n",
        "        f\"Label: {label}\\n\\n\"\n",
        "    )\n",
        "\n",
        "    explanation_prompt = f\"Explain step-by-step why '{label}' is the appropriate label given the trial(s) and the statement:\"\n",
        "\n",
        "    return input_text + explanation_prompt\n"
      ],
      "metadata": {
        "id": "NGJ0xkyKiefb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_cot4(primary_trial, hypothesis, label, secondary_trial=None):\n",
        "    prompt = (\n",
        "        \"You are a clinical reasoning assistant.\\n\\n\"\n",
        "        f\"Please provide exactly 4 clear and concise reasoning steps that explain why the label '{label}' \"\n",
        "        f\"is appropriate given the trial information and the statement.\\n\\n\"\n",
        "        f\"Use only evidence from the trial(s) — avoid assumptions or external knowledge.\\n\\n\"\n",
        "        f\"Please follow this format strictly:\\n\\n\"\n",
        "        f\"Step-by-step Explanation:\\n\"\n",
        "        f\"Step 1: ...\\n\"\n",
        "        f\"Step 2: ...\\n\"\n",
        "        f\"Step 3: ...\\n\"\n",
        "        f\"Step 4: ...\\n\\n\"\n",
        "        f\"Final Conclusion: The label is correct because ...\"\n",
        "    )\n",
        "\n",
        "    # Add the trial(s) and statement section\n",
        "    prompt += \"\\n\\nClinical Trial Report:\\n\"\n",
        "    prompt += f\"Primary Trial:\\n{primary_trial.strip()}\\n\"\n",
        "    if secondary_trial and len(secondary_trial.strip()) > 0:\n",
        "        prompt += f\"\\nSecondary Trial:\\n{secondary_trial.strip()}\\n\"\n",
        "    prompt += f\"\\nStatement:\\n{hypothesis.strip()}\\n\"\n",
        "\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "qJ5JcLLbmqen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_cot2(premise, hypothesis, label):\n",
        "    input_text = (\n",
        "        f\"Clinical Trial Report:\\n\"\n",
        "        f\"{premise}\\n\"\n",
        "        f\"Statement: {hypothesis}\\n\"\n",
        "        f\"Label: {label}\\n\\n\"\n",
        "    )\n",
        "\n",
        "    explanation_prompt = (\n",
        "        f\"Given the detailed trial(s) information and the statement above, articulate step-by-step (up to 5 steps) \"\n",
        "        f\"why the label '{label}' accurately reflects the relationship between the statement and the trial data. \"\n",
        "        f\"Focus on key evidence and use logical deductions to support each step of your reasoning. Ensure that each \"\n",
        "        f\"step is factually based on the provided information and contributes directly to justifying the assigned label.\"\n",
        "    )\n",
        "\n",
        "    return input_text + explanation_prompt\n"
      ],
      "metadata": {
        "id": "aguCmrl04Yxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_cot3(premise, hypothesis, label):\n",
        "    input_text = (\n",
        "        f\"Clinical Trial Report:\\n\"\n",
        "        f\"{premise}\\n\"\n",
        "        f\"Statement: {hypothesis}\\n\"\n",
        "        f\"Label: {label}\\n\\n\"\n",
        "    )\n",
        "\n",
        "    explanation_prompt = (\n",
        "        f\"Please provide up to 5 clear and concise reasoning steps that explain why the label '{label}' is appropriate given the trial information and the statement. Each step should directly contribute to establishing whether there is a contradiction or entailment between the trials' data and the statement.\"\n",
        "    )\n",
        "\n",
        "    return input_text + explanation_prompt\n",
        "\n"
      ],
      "metadata": {
        "id": "TrdeClxqC-Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ncmxIcnWC4zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "file_path_input = \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_03122024/combined_test(text,statement,lables).csv\"\n",
        "file_path_output=\"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/test-zero-shot.csv\"\n",
        "df=pd.read_csv(file_path_input)\n",
        "def create_cot_data(input_csv, output_csv):\n",
        "\n",
        "    df = pd.read_csv(input_csv)\n",
        "    if not {'text', 'statements', 'label'}.issubset(df.columns):\n",
        "        raise ValueError(\"Dataset must contain 'text', 'statements', and 'label' columns.\")\n",
        "\n",
        "    df['text'] = df.apply(lambda row: get_input_text(row['text'], row['statements']), axis=1)\n",
        "    df = df[['text', 'label']]\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"2-shot test dataset saved as: {output_csv}\")\n",
        "\n",
        "create_cot_data(file_path_input,file_path_output)"
      ],
      "metadata": {
        "id": "EgjNKvr56eG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "file_path_input = \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/train.csv\"\n",
        "file_path_output=\"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/train-finetune.csv\"\n",
        "df=pd.read_csv(file_path_input)\n",
        "def create_2shot_test_data(input_csv, output_csv):\n",
        "\n",
        "    df = pd.read_csv(input_csv)\n",
        "    if not {'text', 'statements', 'label'}.issubset(df.columns):\n",
        "        raise ValueError(\"Dataset must contain 'text', 'statements', and 'label' columns.\")\n",
        "\n",
        "    df['text'] = df.apply(lambda row: get_input_text(row['text'], row['statements']), axis=1)\n",
        "    df = df[['text', 'label']]\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"2-shot test dataset saved as: {output_csv}\")\n",
        "\n",
        "create_2shot_test_data(file_path_input,file_path_output)"
      ],
      "metadata": {
        "id": "QwY3jz86_J_E",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTbgWncGiCV1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_samples(file_path, get_input_cot):\n",
        "    \"\"\"\n",
        "    Process the samples from an Excel file, create input text based on primary and secondary trial evidence,\n",
        "    and append to a list of dictionaries containing 'text' and 'label'.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path: Path to the Excel file\n",
        "    - get_input_text: Function to prepare input text\n",
        "\n",
        "    Returns:\n",
        "    - samples: List of dictionaries with 'text' and 'label'\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the Excel file into a DataFrame\n",
        "    #data_expanded = pd.read_excel(file_path)\n",
        "    data_expanded = pd.read_csv(file_path)\n",
        "    samples = []\n",
        "\n",
        "    for _, sample in data_expanded.iterrows():  # Iterate over rows\n",
        "        primary_evidence = \"\".join(str(sample['primary_section']))\n",
        "        text = f\"Primary trial evidence are {primary_evidence}\"\n",
        "\n",
        "        # Check if secondary evidence is not null\n",
        "        if pd.notna(sample[\"secondary_section\"]):\n",
        "            text = f\"{text} Secondary trial evidence are {sample['secondary_section']}\"\n",
        "\n",
        "        # Generate input text using the provided get_input_text function\n",
        "        #input_text = get_input_text(text, sample['statements'])\n",
        "        input_text = get_input_cot4(text, sample['statements'],sample['label'])\n",
        "        # Append the sample to the list as a dictionary\n",
        "        temp = {\"text\": input_text, \"label\": sample['label']}\n",
        "        samples.append(temp)\n",
        "\n",
        "    return samples\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pu9vjImFb8pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#file_path_input = \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/train.csv\"\n",
        "file_path_input = \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/dev.csv\"\n",
        "\n",
        "file_path_output=\"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/dev_cot_input4.csv\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "samples_1shot = process_samples(file_path_input, get_input_cot4)"
      ],
      "metadata": {
        "id": "k4bBGebz0dix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = pd.DataFrame(samples_1shot)"
      ],
      "metadata": {
        "id": "9cecxpQ9cWBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample.iloc[0]\n",
        "sample.iloc[0]['text'].split('\\n')\n"
      ],
      "metadata": {
        "id": "vq796Kmvc6Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ntw427djAHuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample.to_csv(file_path_output, index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "FQuiOxw9qP6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqHhfPe1iCV1"
      },
      "outputs": [],
      "source": [
        "def clean_pred(preds):\n",
        "    cleaned_pred = []\n",
        "    for pred in preds:\n",
        "        # Extract the string from the list (assuming each sentence is a list with one element)\n",
        "        if isinstance(pred, list):\n",
        "            pred = pred[0]\n",
        "\n",
        "        # Strip unwanted tokens and spaces\n",
        "        pred = pred.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip()\n",
        "\n",
        "        # Check for \"Contradiction\" or \"Entailment\"\n",
        "        if \"Yes\" in pred or \"Entailment\" in pred:\n",
        "            cleaned_pred.append(\"Entailment\")\n",
        "        elif \"No\" in pred or \"Contradiction\" in pred:\n",
        "            cleaned_pred.append(\"Contradiction\")\n",
        "\n",
        "    return cleaned_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YT0TMg4EoErw"
      },
      "outputs": [],
      "source": [
        "data=dev\n",
        "uuid_list = list(data.keys())\n",
        "statements = []\n",
        "gold_dev_primary_evidence = []\n",
        "gold_dev_secondary_evidence = []\n",
        "for i in range(len(uuid_list)):\n",
        "  #Retrieve all statements from the development set\n",
        "  statements.append(data[uuid_list[i]][\"Statement\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=dev"
      ],
      "metadata": {
        "id": "b2SxrKB8IAYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDmz_IhVoErx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "\n",
        "Results = {}\n",
        "df_data = []\n",
        "\n",
        "# Loop through the uuid_list\n",
        "for i in range(len(uuid_list)):\n",
        "    primary_ctr_path = os.path.join(file_path, 'CT json', data[uuid_list[i]][\"Primary_id\"] + \".json\")\n",
        "\n",
        "    # Load the primary trial data\n",
        "    with open(primary_ctr_path) as json_file:\n",
        "        primary_ctr = json.load(json_file)\n",
        "\n",
        "    # Retrieve the full section from the primary trial\n",
        "    primary_section = primary_ctr[data[uuid_list[i]][\"Section_id\"]]\n",
        "\n",
        "    # Convert the primary section entries to a matrix of TF-IDF features\n",
        "    vectorizer = TfidfVectorizer().fit(primary_section)\n",
        "    X_s = vectorizer.transform([statements[i]])\n",
        "    X_p = vectorizer.transform(primary_section)\n",
        "\n",
        "    # Compute the cosine similarity between the primary section entries and the statement\n",
        "    primary_scores = cosine_distances(X_s, X_p)\n",
        "\n",
        "    # Initialize values for secondary section and prediction\n",
        "    secondary_section = None\n",
        "    Prediction = None\n",
        "\n",
        "    # If there is a comparison trial, repeat for the secondary trial\n",
        "    if data[uuid_list[i]][\"Type\"] == \"Comparison\":\n",
        "        secondary_ctr_path = os.path.join(file_path, 'CT json', data[uuid_list[i]][\"Secondary_id\"] + \".json\")\n",
        "\n",
        "        with open(secondary_ctr_path) as json_file:\n",
        "            secondary_ctr = json.load(json_file)\n",
        "\n",
        "        secondary_section = secondary_ctr[data[uuid_list[i]][\"Section_id\"]]\n",
        "\n",
        "        # Apply TF-IDF for secondary section\n",
        "        vectorizer = TfidfVectorizer().fit(secondary_section)\n",
        "        X_s = vectorizer.transform([statements[i]])\n",
        "        X_p = vectorizer.transform(secondary_section)\n",
        "        secondary_scores = cosine_distances(X_s, X_p)\n",
        "\n",
        "        # Combine and average the cosine distances from both trials\n",
        "        combined_scores = []\n",
        "        combined_scores.extend(secondary_scores[0])\n",
        "        combined_scores.extend(primary_scores[0])\n",
        "        score = np.average(combined_scores)\n",
        "\n",
        "        # Determine prediction based on the cosine distance\n",
        "        if score > 0.9:\n",
        "            Prediction = \"Contradiction\"\n",
        "        else:\n",
        "            Prediction = \"Entailment\"\n",
        "    else:\n",
        "        # Use only the primary trial for prediction\n",
        "        score = np.average(primary_scores)\n",
        "\n",
        "        if score > 0.9:\n",
        "            Prediction = \"Contradiction\"\n",
        "        else:\n",
        "            Prediction = \"Entailment\"\n",
        "\n",
        "        secondary_section = \"N/A\"  # If no secondary trial is present\n",
        "\n",
        "    # Store prediction in Results dictionary\n",
        "    Results[str(uuid_list[i])] = {\"Prediction\": Prediction}\n",
        "\n",
        "    # Append data for the dataframe\n",
        "    df_data.append({\n",
        "        \"primary_section\": primary_section,\n",
        "        \"secondary_section\": secondary_section,\n",
        "        \"statements\": statements[i],\n",
        "        \"label\": data[uuid_list[i]][\"Label\"],\n",
        "        \"TF_IDF_prediction\": Prediction\n",
        "    })\n",
        "\n",
        "# Create a DataFrame from the collected data\n",
        "df = pd.DataFrame(df_data)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "# output_file = 'methods_output_test_data.xlsx'  # You can specify a desired path\n",
        "# output_file = 'train_data_new_format.xlsx'  # You can specify a desired path\n",
        "# output_file = 'test_data_new_format.xlsx'  # You can specify a desired path\n",
        "output_file = '/content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/dev.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "# df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"DataFrame saved to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Q2ynegoL-jT"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Use the small Flan-T5 model\n",
        "# tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", cache_dir=\"/mnt/data/huggingface_cache\", device_map=\"auto\")\n",
        "\n",
        "# Use the xl Flan-T5 model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", cache_dir=\"./data/huggingface_cache\" )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6pmbxYwOnk7"
      },
      "outputs": [],
      "source": [
        "# Define or import your get_input_text function\n",
        "def get_input_text(premise, hypothesis):\n",
        "    options_prefix = \"OPTIONS:\\n- \"\n",
        "    separator = \"\\n- \"\n",
        "    options_ = options_prefix + f\"{separator}\".join([\"Entailment\",\"Contradiction\"])\n",
        "    return f\"{premise} \\n Question: Does this imply that {hypothesis} Choose one: [entailment, contradiction].? \"\n",
        "\n",
        "# Call the process_samples function\n",
        "samples = process_samples(\"./test_data_new_format.xlsx\", get_input_text)\n",
        "\n",
        "# Output the samples list\n",
        "#print(samples)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8zL_PqvxVWu"
      },
      "outputs": [],
      "source": [
        "save_path = './data/zero_shot_test_data.json'\n",
        "with open(save_path, 'w') as f:\n",
        "    json.dump(samples, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cw23fogdlK3"
      },
      "source": [
        "# zero_shot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-GPMjp8TJwO",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "labels = []\n",
        "pred = []\n",
        "with torch.inference_mode():\n",
        "    for sample in tqdm.tqdm(samples):\n",
        "        labels.append(sample[\"label\"])\n",
        "        input_ids = tokenizer(sample[\"text\"], return_tensors=\"pt\")\n",
        "        # print(input_ids)\n",
        "\n",
        "        outputs = model.generate(**input_ids)\n",
        "        # pred.append(tokenizer.decode(outputs[0]))\n",
        "        pred.append(tokenizer.batch_decode(outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNleyo7IoEr3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Load the Excel file into a DataFrame\n",
        "# d_path=\"methods_output_test_data.xlsx\"\n",
        "# df = pd.read_excel(d_path)\n",
        "\n",
        "# # Add the new column to the DataFrame\n",
        "# # df['zero_shot_base'] = pred\n",
        "# df['zero_shot_base_clean_pred'] = clean_pred(pred)\n",
        "\n",
        "# # Save the updated DataFrame back to an Excel file\n",
        "# df.to_excel(d_path, index=False)\n",
        "\n",
        "# df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qICru5Ed03K2"
      },
      "outputs": [],
      "source": [
        "# # Compute metrics with different averaging methods ???????????????small ????\n",
        "# metrics_micro = compute_metrics(labels, pred, averaging=\"micro\")\n",
        "# metrics_macro = compute_metrics(labels, pred, averaging=\"macro\")\n",
        "# metrics_weighted = compute_metrics(labels, pred, averaging=\"weighted\")\n",
        "# #save_metrics_to_csv('FLAN-T5-zero-shot)', metrics_micro, metrics_macro, metrics_weighted, result_path)\n",
        "# # Print results\n",
        "# print(\"Micro averaging:\", metrics_micro)\n",
        "# print(\"Macro averaging:\", metrics_macro)\n",
        "# print(\"Weighted averaging:\", metrics_weighted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZv2cmZmoEr5"
      },
      "outputs": [],
      "source": [
        "result_table = evaluate.evaluate_all_methods('./methods_output_test_data.xlsx','./results_test_data.xlsx')\n",
        "result_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLi9TLPH38sl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMBcVGK032-u"
      },
      "source": [
        "#  FLAN-T5_xl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t__w0OSS32jw"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Use the small Flan-T5 model\n",
        "# tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", cache_dir=\"/mnt/data/huggingface_cache\", device_map=\"auto\")\n",
        "\n",
        "# Use the xl Flan-T5 model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", cache_dir=\"./data/huggingface_cache\" )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AGKCjx8UWrky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import torch\n",
        "\n",
        "model = model.to('cuda')\n",
        "\n",
        "labels = []\n",
        "pred = []\n",
        "samples = data  # Assuming this is a pandas DataFrame\n",
        "with torch.inference_mode():\n",
        "    for _, item in tqdm.tqdm(samples.iterrows(), total=len(samples), desc=\"Processing Samples\"):\n",
        "        text = item[\"text\"]  # Access the \"text\" column\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", max_length=2048, truncation=True).to('cuda')\n",
        "        outputs = model.generate(**inputs, max_length=10)\n",
        "        pred.append(tokenizer.batch_decode(outputs))\n"
      ],
      "metadata": {
        "id": "2zzyRZWYUPqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text'].iloc[233]"
      ],
      "metadata": {
        "id": "TtsSkHeHlec9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8Kiz1bO46FX"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = model.to('cuda')\n",
        "\n",
        "input_text=\"\"\"Here are my first examples:\n",
        " Example 1: Primary trial evidence are Inclusion Criteria: Age 52-75 years old; Identification as Latina/Hispanic/Chicana female;\n",
        "    Residence in Pilsen, Little Village, East Side or South Chicago; No history of health volunteerism; No history of breast cancer; and Lack of\n",
        "    a mammogram within the last two yearsExclusion Criteria: Not meeting all inclusion criteria; Women will be excluded if they participated in\n",
        "    formative focus groups Question: Does this imply that Patients eligible for the primary trial must live in the USA?\n",
        "    Answer with only one of the options (Entailment or Contradiction): Entailment\n",
        " Here are my second examples:\n",
        " Example 2: Primary trial evidence are Adverse Events 1: Total: 10/71 (14.08%) ATRIAL FIBRILLATION 1/71 (1.41%) CARDIAC TAMPONADE 1/71 (1.41%)\n",
        "    PERICARDIAL EFFUSION 1/71 (1.41%) SUPRAVENTRICULAR TACHYCARDIA 1/71 (1.41%) DIARRHOEA 1/71 (1.41%) NAUSEA 1/71 (1.41%) VOMITING 1/71 (1.41%)\n",
        "    CHEST PAIN 1/71 (1.41%) PNEUMONIA 1/71 (1.41%) MALIGNANT PLEURAL EFFUSION 1/71 (1.41%) HEPATIC ENCEPHALOPATHY 1/71 (1.41%)\n",
        "    Question: Does this imply that There were only 3 adverse events in the primary trial which occurred more than twice?\n",
        "    Answer with only one of the options (Entailment or Contradiction):\n",
        "    Contradiction\n",
        " Primary trial evidence are: ['Outcome Measurement: ', '  Progression Free Survival (PFS)', '  PFS is defined as the time from randomization to the earlier of disease progression or death due to any cause. Disease progression is defined using Response Evaluation Criteria In Solid Tumors Criteria (RECIST v1.1), as a 20% increase in the sum of the longest diameter of target lesions, or progression in a non-target lesion, or the appearance of new lesions. The primary analysis of PFS was based on PFS events determined retrospectively by the central independent review committee, blinded to treatment assignment and investigator assessments according to RECIST 1.1 criteria.', '  Time frame: Evaluated every 6 - 9 weeks following treatment initiation', 'Results 1: ', '  Arm/Group Title: Capecitabine', '  Arm/Group Description: Capecitabine administered on Days 1 through 14 of each 21 day cycle until disease progression, discontinuation due to toxicity, withdrawal of consent, or end of study.', '  Overall Number of Participants Analyzed: 109', '  Median (95% Confidence Interval)', '  Unit of Measure: months  2.8        (1.6 to 3.2)', 'Results 2: ', '  Arm/Group Title: CDX-011', '  Arm/Group Description: CDX-011 administered on Day 1 of each 21 day cycle until disease progression, discontinuation due to toxicity, withdrawal of consent, or end of study.', '  Overall Number of Participants Analyzed: 218', '  Median (95% Confidence Interval)', '  Unit of Measure: months  2.9        (2.8 to 3.5)']\n",
        " Question: Does this imply that The patient with the longest PFS in the primary trial was in cohort 1, and he survived 3.5 months without disease progression or death ? Answer with only one of the options (Entailment or Contradiction):\"\"\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=2048, truncation=True).to('cuda')\n",
        "outputs = model.generate(**inputs, max_length=199)\n",
        "\n",
        "# Decode and print the result\n",
        "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Answer:\", result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to('cuda')\n",
        "\n",
        "input_text = \"\"\"Primary trial evidence are: ['Inclusion Criteria:', '  Female patients, >/= 18 years of age', '  Histologically confirmed HER2-negative breast cancer', '  Disease progression during or following first-line treatment with Avastin and chemotherapy for locally recurrent or metastatic breast cancer', '  Avastin treatment in first-line setting must have been a minimum of 4 cycles (15 mg/kg) or 6 cycles (10 mg/kg) in combination with chemotherapy', '  ECOG performance status 0-2', '  At least 28 days since prior radiation therapy or surgery and recovery from treatment', 'Exclusion Criteria:', '  Anti-angiogenic therapy or anti-vascular endothelial growth factors other than Avastin for first-line treatment', '  Active malignancy other than superficial basal cell and superficial squamous cell carcinoma of the skin, or in situ carcinoma of the cervix or breast within the last 5 years', '  Inadequate renal function', '  Clinically relevant cardio-vascular disease', '  Known CNS disease except for treated brain metastases', '  Chronic daily treatment with high-dose aspirin (>325 mg/day) or clopidogrel (>75 mg/day)', '  Pregnant or lactating women']\n",
        "Secondary trial evidence are: ['Inclusion Criteria:', '  Female or male  18 years of age.', '  Newly diagnosed ER-positive, HER2-negative breast cancer. ER-positive is defined as  1% immunohistochemical (IHC) staining of any intensity. HER2 test result is negative if a single test (or both tests) performed show:', '  IHC 1+ or 0', '  In situ hybridization negative based on:', '  Single-probe average HER2 copy number < 4.0 signals/cell', '  Dual-probe HER2/CEP17 ratio < 2 with an average HER2 copy number < 4.0 signals/cell.', '  Patients with stage II-III breast cancer are eligible if they are deemed appropriate for neoadjuvant endocrine therapy by the referring or treating medical oncologist. Patients with stage I disease are eligible if they are deemed borderline candidates for breast conservation and the treating surgeon recommends preoperative therapy to increase the chances of breast conservation.', '  Eastern Cooperative Oncology Group performance status and/or other performance status of  1.', '  Female patients who:', '  Are postmenopausal for at least 1 year before the screening visit, OR', '  Are surgically sterile, OR', '  If they are of childbearing potential, agree to practice 1 effective method of contraception and 1 additional effective (barrier) method, at the same time, from the time of signing the ICF through 90 days (or longer, as mandated by local labeling [e.g., United Surgical Partners International, summary of product characteristics, etc.] after the last dose of the study drugs, OR', '  Agree to practice true abstinence, when this is in line with the preferred and usual lifestyle of the patient (periodic abstinence [e.g., calendar, ovulation, symptothermal, postovulation methods] and withdrawal, spermicides only, and lactational amenorrhea are not acceptable methods of contraception. Female and male condom should not be used together).', '  Male patients, even if surgically sterilized (i.e., status post-vasectomy), who:', '  Agree to practice highly effective barrier contraception during the entire study treatment period and through 120 days after the last dose of the study drugs, OR', '  Agree to practice true abstinence, when this is in line with the preferred and usual lifestyle of the patient', '  Agree not to donate sperm during the course of this study or within 120 days after receiving their last dose of the study drugs.', '  Screening clinical laboratory values as specified below:', '  Bone marrow reserve consistent with: absolute neutrophil count  1.5 x 109/L, platelet count  100 x 109/L, and hemoglobin  9 g/dL (without transfusion) within 1 week preceding the administration of the study drugs;', \"  Hepatic status: Serum total bilirubin  1 x upper limit of normal (ULN; in the case of known Gilbert's syndrome, a higher serum total bilirubin [< 1.5 x ULN] is allowed), aspartate aminotransferase and alanine aminotransferase  1.5 x ULN, and alkaline phosphatase  1.5 x ULN;\", '  Renal status: Creatinine clearance 50 mL/min based on Cockcroft-Gault estimate or based on urine collection (12 or 24 hour);', '  Metabolic status: HbA1c < 7.0%, fasting serum glucose  130 mg/dL, and fasting triglycerides  300 mg/dL.', '  Ability to swallow oral medications.', '  Voluntary written consent must be given before performance of any study-related procedure not part of standard medical care, with the understanding that consent may be withdrawn by the patient at any time without prejudice to future medical care.', '  Negative serum pregnancy test within 7 days prior to the administration of the study drugs for female patients of childbearing potential.', '  Patient must be accessible for treatment and follow-up.', '  Patient must be willing to undergo breast biopsies as required by the study protocol.', 'Exclusion Criteria:', '  Any patient with metastatic disease.', \"  Other clinically significant comorbidities, such as uncontrolled pulmonary disease, active central nervous system disease, active infection, or any other condition that could compromise the patient's participation in the study.\", '  Known human immunodeficiency virus infection.', '  Known hepatitis B surface antigen-positive or known or suspected active hepatitis C infection.', \"  Any serious medical or psychiatric illness that could, in the investigator's opinion, potentially interfere with the completion of the protocol-specified treatment.\", '  Diagnosed or treated for another malignancy within 2 years before administration of the first dose of the study drugs or previously diagnosed with another malignancy and have any evidence of residual disease. Patients with non-melanoma skin cancer or carcinoma in situ of any type are not excluded if they have undergone complete resection.\", '  Breastfeeding or pregnant.', '  Manifestations of malabsorption due to prior gastrointestinal surgery, gastrointestinal disease, or an unknown reason that may alter the absorption of TAK-228. Patients with enteric stomata are also excluded.', '  Treatment with any investigational products within 2 weeks before administration of the first dose of the study drugs.', '  Poorly controlled diabetes mellitus (defined as HbA1c > 7%). Patients with a history of transient glucose intolerance due to corticosteroid administration may be enrolled in the study if all other inclusion criteria and none of the other exclusion criteria are met.', '  History of any of the following within the last 6 months before administration of the first dose of the study drugs:', '  Ischemic myocardial event, including angina requiring therapy and artery revascularization procedures', '  Ischemic cerebrovascular event, including transient ischemic attack and artery revascularization procedures', '  Requirement for inotropic support (excluding digoxin) or serious (uncontrolled) cardiac arrhythmia (including atrial flutter/fibrillation, ventricular fibrillation, and ventricular tachycardia)', '  Placement of a pacemaker for control of rhythm', '  New York Heart Association Class III or IV heart failure', '  Pulmonary embolism', '  Significant active cardiovascular or pulmonary disease including:', '  Uncontrolled hypertension (i.e., systolic blood pressure > 180 mm Hg, diastolic blood pressure > 95 mm Hg). Use of antihypertensive agents to control hypertension before week 1, day 1 is allowed.', '  Pulmonary hypertension', '  Uncontrolled asthma or O2 saturation < 90% by arterial blood gas analysis or pulse oximetry on room air', '  Significant valvular disease, severe regurgitation, or stenosis by imaging independent of symptom control with medical intervention or history of valve replacement', '  Medically significant (symptomatic) bradycardia', '  History of arrhythmia requiring an implantable cardiac defibrillator', '  Baseline QTc prolongation (e.g., repeated demonstration of QTc interval > 480 milliseconds or history of congenital long QT syndrome or torsades de pointes)', '  Treatment with strong inhibitors and/or inducers of CYP3A4, CYP2C9, or CYP2C19 within 7 days preceding the first dose of the study drugs.', '  Patients receiving systemic corticosteroids (either IV or oral steroids, excluding inhalers or low-dose hormone replacement therapy) within 1 week before administration of the first dose of the study drugs.', '  Daily or chronic use of a proton pump inhibitor (PPI) and/or having taken a PPI within 7 days before receiving the first dose of the study drugs.', '  Patients unwilling or unable to comply with the study protocol.', '  Patients previously treated with hormonal therapy (tamoxifen, AI) or PI3K, AKT, dual PI3K/mTOR, TORC1/2, or mTORC1 inhibitors.', '  Patients who are currently being treated with cancer therapy (chemotherapy, radiation therapy, immunotherapy, or biologic therapy) other than the trial therapy.', '  Patients with hypersensitivity to mTOR inhibitors or tamoxifen.']\n",
        "Question: does this imply that A 20 year old female patient with a Newly diagnosed ER-positive, HER2-negative breast cancer, could be eligible for the secondary trial and the primary trial\n",
        "choose only one of the options (Entailment or Contradiction):\n",
        "\"\"\"\n",
        "\n",
        "# Truncate the input text to fit within the model's input size (2048 tokens max)\n",
        "max_input_length = 2048\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=max_input_length).to('cuda')\n",
        "\n",
        "# Generate output from the model\n",
        "with torch.inference_mode():\n",
        "    outputs = model.generate(**input_ids)\n",
        "\n",
        "# Decode the output\n",
        "pred = tokenizer.batch_decode(outputs)\n",
        "\n",
        "print(\"Prediction:\", pred[0])  # Accessing the first (and only) prediction\n"
      ],
      "metadata": {
        "id": "HCIflOpx_ew3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to('cuda')  # Move the model to GPU\n"
      ],
      "metadata": {
        "id": "TvtiZprn1lqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure model is on GPU\n",
        "#skipped_rows = [7, 9, 24, 25, 26, 27, 35, 38, 41, 49, 50, 61, 65, 73, 74, 75, 78, 117, 120, 130, 139, 145, 146, 149, 163, 169, 175, 176, 220, 235, 246, 261, 265, 269, 277, 280, 284, 295, 296, 314, 319, 324, 329, 332, 355, 357, 360, 367, 374, 377, 404, 409, 414, 418, 421, 423, 427, 449, 466, 468, 469, 470, 472, 478, 482, 483, 490, 491]\n",
        "skipped_rows = [24, 25, 26, 117, 145, 176, 220, 374, 414, 418]\n",
        "\n",
        "model = model.to('cuda')\n",
        "\n",
        "# Process each skipped row\n",
        "for row_index in tqdm(skipped_rows, desc=\"Processing rows\", unit=\"row\"):\n",
        "    try:\n",
        "        # Get the text for the current row\n",
        "        text = df.loc[row_index, 'text']\n",
        "\n",
        "        # Prepare the input for the model and move to GPU\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", max_length=2048, truncation=True).to('cuda')\n",
        "\n",
        "        # Generate the output (model will now use the same device as inputs)\n",
        "        outputs = model.generate(**inputs, max_length=100)  # Adjust max_length as needed\n",
        "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Update the prediction column with the result\n",
        "        df.at[row_index, 'prediction'] = result\n",
        "\n",
        "    except KeyError:\n",
        "        print(f\"Row {row_index} not found in the dataset.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {row_index}: {e}\")\n"
      ],
      "metadata": {
        "id": "KzLdhIgj39G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FFqQ3A7dozK"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Assume 'data' is your dataframe and contains a column 'text'\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# List of important row numbers (replace with your list)\n",
        "important_rows = [24, 25, 26, 78, 117, 145, 176, 220, 277, 374, 414, 418]# Example row indices\n",
        "\n",
        "# Select the text from the important rows\n",
        "samples = df.iloc[important_rows][\"text\"].tolist()\n",
        "\n",
        "# Initialize list to store predictions\n",
        "pred = []\n",
        "\n",
        "# Use inference mode for efficient processing\n",
        "with torch.inference_mode():\n",
        "    for text in tqdm(samples, desc=\"Processing\"):\n",
        "        input_ids = tokenizer(text, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
        "        outputs = model.generate(**input_ids,max_length=2048)\n",
        "        pred.append(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
        "\n",
        "# The result is stored in 'pred' which contains the model's output for the selected rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5KfOI7L7x72"
      },
      "outputs": [],
      "source": [
        "# input_text =\"\"\"Primary trial evidence are: ['Adverse Events 1:', '  Total: 1/13 (7.69%)', '  Rapid disease progression  [1]1/13 (7.69%)', '  Increased pleural effusion  [2]0/13 (0.00%)', 'Adverse Events 2:', '  Total: 1/4 (25.00%)', '  Rapid disease progression  [1]0/4 (0.00%)', '  Increased pleural effusion  [2]1/4 (25.00%)']\n",
        "#  Question: does this imply that 25% of patients in the primary trial suffer Increased pleural effusion and Rapid disease progression\n",
        "#  Answer with only one of the options (Entailment or Contradiction):\"\"\"\n",
        " #Contradiction\n",
        "\n",
        "# input_text=\"\"\"Primary trial evidence are INTERVENTION 1:   Gefitinib (ZD1839)  ZD1839 at a daily dose of 250 mg or 500 mg depending on final dose in parent trial Secondary trial evidence are INTERVENTION 1:   Zoledronic Acid  Zoledronic acid, vitamin D and calcium supplements.INTERVENTION 2:   Zoledronic Acid + Radiopharmaceuticals  Zoledronic acid, vitamin D and calcium supplements, plus Sr-89 or Sm-153.\n",
        "# Question: Does this imply that\n",
        "# The intervention in the primary trial consists of a single drug, whereas in the secondary trial the intervention requires at least 3 different drugs?\n",
        "# Answer with only one of the options (Entailment or Contradiction):\"\"\"\n",
        "#Entailment\n",
        "\n",
        "# input_text = \"\"\"\n",
        "# Primary trial evidence are INTERVENTION 1:   Laser Therapy Alone therapist administered laser treatment laser: therapist administered laserINTERVENTION 2: Mld Alone therapist administered manual lymphatic drainage manual lymphatic drainage: therapist administered massage therapy Secondary trial evidence are INTERVENTION 1: Part A Abemaciclib: HR+, HER2+ Breast Cancer Abemaciclib 200 mg was administered orally once every 12 hours on days 1-21 of a 21-day cycle when administered as a single agent or in combination with endocrine therapy (ET). Participants with hormone receptor positive HR+, HER2+ breast cancer receiving concurrent trastuzumab, 150 mg abemaciclib was given orally once every 12 hours on days 1-21 of a 21-day cycle. Participants may continue to receive treatment until discontinuation criteria are met.INTERVENTION 2: Part B Abemaciclib: HR+, HER2- Breast Cancer Abemaciclib 200 mg was administered orally once every 12 hours on days 1-21 of a 21-day cycle when administered as a single agent or for in combination with endocrine therapy (ET). Participants may continue to receive treatment until discontinuation criteria are met.\n",
        "# Question: Does this imply that Laser Therapy is in each cohort of the primary trial and the secondary trial, along with neoadjuvant chemotherapy?\n",
        "# Answer with only one of the options (Entailment or Contradiction):\n",
        "# \"\"\"\n",
        "#Contradiction\n",
        "\n",
        "# input_text= \"\"\"Primary trial evidence are: ['Inclusion Criteria:', '  Voluntarily signed and dated written informed consent', '  Age between 18 and 75 years old (both inclusive)', '  Eastern Cooperative Oncology Group (ECOG) performance status (PS) of  1', '  Life expectancy  3 months.', '  Patients with a histologically/cytologically confirmed diagnosis of advanced and/or unresectable disease of any of the following tumors:', '  Breast cancer', '  Epithelial ovarian cancer or gynecological cancer', '  Head and neck squamous cell carcinoma', '  Non-small cell lung cancer', '  Small cell lung cancer', '  Platinum-refractory germ-cell tumors.', '  Adenocarcinoma or carcinoma of unknown primary site', '  Adequate bone marrow, renal, hepatic, and metabolic function', '  Recovery to grade  1 or to baseline from any Adverse Event (AE) derived from previous treatment (excluding alopecia of any grade).', '  Pre-menopausal women must have a negative pregnancy test before study entry and agree to use a medically acceptable method of contraception throughout the treatment period and for at least six weeks after treatment discontinuation', 'Exclusion Criteria:', '  Prior treatment with PM01183 or weekly paclitaxel or nanoalbumin-paclitaxel', '  Patients who have previously discontinued paclitaxel-based regimes due to drug related toxicity.', '  Known hypersensitivity to bevacizumab or any component of its formulation', '  Patients who have previously discontinued bevacizumab-containing regimes due to drug-related toxicity.', '  More than three prior lines of chemotherapy', '  Less than three months since last taxane-containing therapy.', '  Wash-out period:', '  Less than three weeks since the last chemotherapy-containing regimen', '  Less than three weeks since the last radiotherapy dose', '  Less than four weeks since last monoclonal antibody-containing therapy', '  Concomitant diseases/conditions:', '  Unstable angina, myocardial infarction, valvular heart disease, encephalopathy, ischemic attacks, hemorrhagic or ischemic cerebrovascular accident (CVA) or ongoing pulmonary embolism within last year, arrhythmia, hepatopathy, uncontrolled infection, hemoptysis or oxygen requiring dyspnea, known HIV infection, bleeding risk, muscular problems, peripheral neuropathy, Symptomatic or progressive brain metastases or leptomeningeal disease.', '  Men or pre-menopausal women who are not using an effective method of contraception as previously described; actively breast feeding women.', '  Patients who have pelvic irradiation with doses  45 Grays (Gy).', '  History of previous bone marrow and/or stem cell transplantation.', '  Confirmed bone marrow involvement']\n",
        "# Question: does this imply that Patients with a histologically/cytologically confirmed diagnosis of a resectable Non-small cell lung breast cancer or Small cell lung cancer are eligible for the primary trial\n",
        "# Answer with only one of the options (Entailment or Contradiction):\"\"\"\n",
        "# contradiction\n",
        "\n",
        "\n",
        "\n",
        "# input_text= \"\"\"Primary trial evidence are Adverse Events 1:  Total: 5/32 (15.63%)  Febrile neutropenia 1/32 (3.13%)  Supraventricular tachycardia 1/32 (3.13%)  Hypersensitivity 2/32 (6.25%)  Catheter site infection 1/32 (3.13%)  Confusional state 1/32 (3.13%) Secondary trial evidence are Adverse Events 1:  Total: 285/752 (37.90%)  Anaemia 2/752 (0.27%)  Disseminated intravascular coagulation 2/752 (0.27%)  Febrile neutropenia 51/752 (6.78%)  Neutropenia 47/752 (6.25%)  Thrombocytopenia 2/752 (0.27%)  Atrial fibrillation 1/752 (0.13%)  Atrial flutter 0/752 (0.00%)  Cardiac failure congestive 1/752 (0.13%)  Left ventricular dysfunction 0/752 (0.00%)Adverse Events 2:  Total: 117/382 (30.63%)  Anaemia 3/382 (0.79%)  Disseminated intravascular coagulation 0/382 (0.00%)  Febrile neutropenia 11/382 (2.88%)  Neutropenia 20/382 (5.24%)  Thrombocytopenia 0/382 (0.00%)  Atrial fibrillation 1/382 (0.26%)  Atrial flutter 1/382 (0.26%)  Cardiac failure congestive 0/382 (0.00%)  Left ventricular dysfunction 1/382 (0.26%) \"\n",
        "# Question: does this imply that Heart-related adverse events were recorded in both the primary trial and the secondary trial.\n",
        "# Answer with only one of the options (Entailment or Contradiction):\"\"\"\n",
        "# # Entailment\n",
        "\n",
        "\n",
        "\n",
        "# input_text= \"\"\"Primary trial evidence are Inclusion Criteria: Age 52-75 years old; Identification as Latina/Hispanic/Chicana female; Residence in Pilsen, Little Village, East Side or South Chicago; No history of health volunteerism; No history of breast cancer; and Lack of a mammogram within the last two yearsExclusion Criteria: Not meeting all inclusion criteria; Women will be excluded if they participated in formative focus groups.\"\n",
        "# Question: does this imply that\n",
        "# Patients eligible for the primary trial must live in the USA.\n",
        "# Answer with only one of the options (Entailment or Contradiction):\"\"\"\n",
        "#Entailment\n",
        "\n",
        "# input_text= \"\"\"Primary trial evidence are Adverse Events 1: Total: 10/71 (14.08%) ATRIAL FIBRILLATION 1/71 (1.41%) CARDIAC TAMPONADE 1/71 (1.41%) PERICARDIAL EFFUSION 1/71 (1.41%) SUPRAVENTRICULAR TACHYCARDIA 1/71 (1.41%) DIARRHOEA 1/71 (1.41%) NAUSEA 1/71 (1.41%) VOMITING 1/71 (1.41%) CHEST PAIN 1/71 (1.41%) PNEUMONIA 1/71 (1.41%) MALIGNANT PLEURAL EFFUSION 1/71 (1.41%) HEPATIC ENCEPHALOPATHY 1/71 (1.41%) .\n",
        "# Question: does this imply that\n",
        "# There were only 3 adverse events in the primary trial which occurred more than twice?\n",
        "# Answer with only one of the options (Entailment or Contradiction):\"\"\"\n",
        "# \"Contradiction\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzp3QGbDiCV_"
      },
      "source": [
        "# 3. FLAN-T5_2 shots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8CJ-oF5ZSwv"
      },
      "outputs": [],
      "source": [
        "def get_input_text_2shot(premise, hypothesis):\n",
        "    options_prefix = \"OPTIONS:\\n- \"\n",
        "    separator = \"\\n- \"\n",
        "    options_ = options_prefix + f\"{separator}\".join([\"Entailment\", \"Contradiction\"])\n",
        "\n",
        "    # Two-shot examples\n",
        "    example_1_premise=\"Primary trial evidence are Inclusion Criteria: Age 52-75 years old; Identification as Latina/Hispanic/Chicana female; Residence in Pilsen, Little Village, East Side or South Chicago; No history of health volunteerism; No history of breast cancer; and Lack of a mammogram within the last two yearsExclusion Criteria: Not meeting all inclusion criteria; Women will be excluded if they participated in formative focus groups Question: uestion: Does this imply that {hypothesis}? Please answer by reasoning step by step and providing a final answer. OPTIONS: - Entailment - Contradiction\"\n",
        "    example_1_hypothesis=\"Patients eligible for the primary trial must live in the USA.\"\n",
        "    example_1_answer=\"Entailment\"\n",
        "\n",
        "    example_2_premise=\"Primary trial evidence are Adverse Events 1: Total: 10/71 (14.08%) ATRIAL FIBRILLATION 1/71 (1.41%) CARDIAC TAMPONADE 1/71 (1.41%) PERICARDIAL EFFUSION 1/71 (1.41%) SUPRAVENTRICULAR TACHYCARDIA 1/71 (1.41%) DIARRHOEA 1/71 (1.41%) NAUSEA 1/71 (1.41%) VOMITING 1/71 (1.41%) CHEST PAIN 1/71 (1.41%) PNEUMONIA 1/71 (1.41%) MALIGNANT PLEURAL EFFUSION 1/71 (1.41%) HEPATIC ENCEPHALOPATHY 1/71 (1.41%) Question: Does this imply that {hypothesis}? Please answer by reasoning step by step and providing a final answer. OPTIONS: - Entailment - Contradiction\"\n",
        "    example_2_hypothesis=\"There were only 3 adverse events in the primary trial which occurred more than twice.\"\n",
        "    example_2_answer=\"Contradiction\"\n",
        "\n",
        "\n",
        "   # Introduction text with examples\n",
        "    example_intro = \"Here are my 2 examples:\\n\"\n",
        "\n",
        "    # Example text (Two-shot)\n",
        "    two_shot_text = (\n",
        "        f\"{example_intro}\"\n",
        "        f\"Example 1:\\nPremise: {example_1_premise}\\n\"\n",
        "        f\"Hypothesis: {example_1_hypothesis}\\n\"\n",
        "        f\"Answer: {example_1_answer}\\n\\n\"\n",
        "        f\"Example 2:\\nPremise: {example_2_premise}\\n\"\n",
        "        f\"Hypothesis: {example_2_hypothesis}\\n\"\n",
        "        f\"Answer: {example_2_answer}\\n\\n\")\n",
        "\n",
        "\n",
        "    # For the actual input\n",
        "    input_text = f\"{premise} \\nQuestion: Does this imply that {hypothesis}? Just provide the final answer.\\n{options_}\"\n",
        "\n",
        "    # Combine two-shot examples with the actual input\n",
        "    return two_shot_text + input_text\n",
        "# Call the process_samples function\n",
        "samples_2shot = process_samples(\"./test_data_new_format.xlsx\", get_input_text_2shot)\n",
        "\n",
        "# Output the samples list\n",
        "# print(samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8QtluiqG0Xw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wnz5EJcjiCWD"
      },
      "outputs": [],
      "source": [
        "labels = []\n",
        "pred = []\n",
        "with torch.inference_mode():\n",
        "    for samples_2shot in tqdm.tqdm(samples_2shot):\n",
        "        labels.append(samples_2shot[\"label\"])\n",
        "        input_ids = tokenizer(samples_2shot[\"text\"], return_tensors=\"pt\")\n",
        "        # print(input_ids)\n",
        "\n",
        "        outputs = model.generate(**input_ids)\n",
        "        # pred.append(tokenizer.decode(outputs[0]))\n",
        "        pred.append(tokenizer.batch_decode(outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSKIkohEoEr8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the Excel file into a DataFrame\n",
        "d_path=\"methods_output_test_data.xlsx\"\n",
        "df = pd.read_excel(d_path)\n",
        "\n",
        "# Add the new column to the DataFrame\n",
        "df['two_shot_base_s_ex'] = pred\n",
        "df['two_shot_base_clean_pred_s_ex'] = clean_pred(pred)\n",
        "\n",
        "# Save the updated DataFrame back to an Excel file\n",
        "df.to_excel(d_path, index=False)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRRfE8HroEr9"
      },
      "outputs": [],
      "source": [
        "save_path = './data/two_shot_test_data_s_ex.json'\n",
        "with open(save_path, 'w') as f:\n",
        "    json.dump(samples_2shot, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItYICWZAoEr9"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "result_table = evaluate.evaluate_all_methods('./methods_output_test_data.xlsx','./results_test_data.xlsx')\n",
        "result_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nN9_H1fiCWD"
      },
      "source": [
        "# 4.FLAN-T5_cot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qApwlzOdX0nm"
      },
      "outputs": [],
      "source": [
        "def get_input_text_cot(premise, hypothesis):\n",
        "    options_prefix = \"OPTIONS:\\n- \"\n",
        "    separator = \"\\n- \"\n",
        "    options_ = options_prefix + f\"{separator}\".join([\"Entailment\", \"Contradiction\"])\n",
        "\n",
        "    # For the actual input\n",
        "    input_text = f\"{premise} \\nQuestion: Does this imply that {hypothesis}? Please answer by reasoning step by step and providing a final answer.\\n{options_}\"\n",
        "\n",
        "    # Combine two-shot examples with the actual input\n",
        "    return input_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2q2MvvhiCWF"
      },
      "outputs": [],
      "source": [
        "samples_cot = []\n",
        "item=[]\n",
        "for item in data_expanded:\n",
        "    primary_evidence = \"\".join(item['primary_evidence'])\n",
        "    sentence = f\"Primary trial evidence are {primary_evidence}\"\n",
        "    secondary_evidence = item.get(\"secondary_evidence\")\n",
        "    if secondary_evidence:\n",
        "        secondary_evidence = \"\".join(item['secondary_evidence'])\n",
        "        sentence = f\"{sentence} Secondary trial evidence are {secondary_evidence}\"\n",
        "    input_text =get_input_text_cot(sentence, item['statement'])\n",
        "    temp = {\"text\":input_text, \"label\":item['label']}\n",
        "    samples_cot.append(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Y5L2lYxsiCWG",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "samples_cot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwow6XmEiCWG"
      },
      "outputs": [],
      "source": [
        "labels = []\n",
        "pred = []\n",
        "samples= samples_cot\n",
        "with torch.inference_mode():\n",
        "    for samples in tqdm.tqdm(samples):\n",
        "        labels.append(samples[\"label\"])\n",
        "        input_ids = tokenizer(samples[\"text\"], return_tensors=\"pt\")\n",
        "        # print(input_ids)\n",
        "\n",
        "        outputs = model.generate(**input_ids, max_length = 1024)\n",
        "        # pred.append(tokenizer.decode(outputs[0]))\n",
        "        pred.append(tokenizer.batch_decode(outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0xenr6tXiCWH",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GNvCcu7m394"
      },
      "outputs": [],
      "source": [
        "cleaned_pred = []\n",
        "data=pred\n",
        "for sentence in data:\n",
        "    # Extract the string from the list (assuming each sentence is a list with one element)\n",
        "    if isinstance(sentence, list):\n",
        "        sentence = sentence[0]\n",
        "\n",
        "    # Strip unwanted tokens and spaces\n",
        "    sentence = sentence.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip()\n",
        "    # Now check for \"Contradiction\" or \"Entailment\"\n",
        "    if \"Contradiction\" in sentence:\n",
        "        cleaned_pred.append(\"Contradiction\")\n",
        "    elif \"Entailment\" in sentence:\n",
        "        cleaned_pred.append(\"Entailment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_ZmJSNdOnHN3"
      },
      "outputs": [],
      "source": [
        "cleaned_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg04IYInoePk"
      },
      "outputs": [],
      "source": [
        "# Compute metrics with different averaging methods\n",
        "metrics_micro = compute_metrics(labels, cleaned_pred, averaging=\"micro\")\n",
        "metrics_macro = compute_metrics(labels, cleaned_pred, averaging=\"macro\")\n",
        "metrics_weighted = compute_metrics(labels, cleaned_pred, averaging=\"weighted\")\n",
        "#save_metrics_to_csv('FLAN-T5-COT', metrics_micro, metrics_macro, metrics_weighted, result_path)\n",
        "# Print results\n",
        "print(\"Micro averaging:\", metrics_micro)\n",
        "print(\"Macro averaging:\", metrics_macro)\n",
        "print(\"Weighted averaging:\", metrics_weighted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81JqQoh3iCWL"
      },
      "outputs": [],
      "source": [
        "# Compute metrics with different averaging methods\n",
        "pred=cleaned_predictions\n",
        "metrics_micro = compute_metrics(labels, pred, averaging=\"micro\")\n",
        "metrics_macro = compute_metrics(labels, pred, averaging=\"macro\")\n",
        "metrics_weighted = compute_metrics(labels, pred, averaging=\"weighted\")\n",
        "\n",
        "# Print results\n",
        "print(\"Micro averaging:\", metrics_micro)\n",
        "print(\"Macro averaging:\", metrics_macro)\n",
        "print(\"Weighted averaging:\", metrics_weighted)\n",
        "\n",
        "# Micro averaging: {'precision': 0.545, 'recall': 0.545, 'f1_score': 0.545}\n",
        "# Macro averaging: {'precision': 0.5608190295985944, 'recall': 0.545, 'f1_score': 0.5133559720848151}\n",
        "# Weighted averaging: {'precision': 0.5608190295985944, 'recall': 0.545, 'f1_score': 0.5133559720848151}\n",
        "\n",
        "# cot\n",
        "# Micro averaging: {'precision': 0.52, 'recall': 0.52, 'f1_score': 0.52}\n",
        "# Macro averaging: {'precision': 0.5229779411764706, 'recall': 0.52, 'f1_score': 0.5039272426622571}\n",
        "# Weighted averaging: {'precision': 0.5229779411764706, 'recall': 0.52, 'f1_score': 0.5039272426622571}\n",
        "\n",
        "#return f\"{premise} \\nQuestion: Does this imply that {hypothesis}? Please answer by reasoning step by step and providing a final answer.\\n{options_}\"\n",
        "# Micro averaging: {'precision': 0.53, 'recall': 0.53, 'f1_score': 0.53}\n",
        "# Macro averaging: {'precision': 0.18177387914230017, 'recall': 0.17666666666666667, 'f1_score': 0.1778885045007831} ???\n",
        "# Weighted averaging: {'precision': 0.5453216374269005, 'recall': 0.53, 'f1_score': 0.5336655135023493}\n",
        "\n",
        "#########################\n",
        "# #tf_idf:\n",
        "# F1:0.502415\n",
        "# precision_score:0.485981\n",
        "# recall_score:0.520000\n",
        "\n",
        "########   Question: Does this imply that {hypothesis}?\n",
        "# #direct\n",
        "# return f\"{premise} \\nQuestion: Does this imply that {hypothesis}? Just provide the final answer.\\n{options_}\"\n",
        "# Micro averaging: {'precision': 0.52, 'recall': 0.52, 'f1_score': 0.52}\n",
        "# Macro averaging: {'precision': 0.53125, 'recall': 0.52, 'f1_score': 0.4725274725274725}\n",
        "# Weighted averaging: {'precision': 0.53125, 'recall': 0.52, 'f1_score': 0.47252747252747246}\n",
        "\n",
        "# #COT\n",
        "##return f\"{premise} \\nQuestion: Does this imply that {hypothesis}? Please answer by reasoning step by step and providing a final answer.\\n{options_}\"\n",
        "#after adding the posstprossing function:\n",
        "# Micro averaging: {'precision': 0.54, 'recall': 0.54, 'f1_score': 0.54}\n",
        "# Macro averaging: {'precision': 0.5407996736026112, 'recall': 0.54, 'f1_score': 0.5377349010149733}\n",
        "# Weighted averaging: {'precision': 0.5407996736026112, 'recall': 0.54, 'f1_score': 0.5377349010149733}\n",
        "\n",
        "#cot without options:\n",
        "# return f\"{premise} \\nQuestion: Does this imply that {hypothesis}? Please answer by reasoning step by step and providing a final answer.\"\n",
        "# Micro averaging: {'precision': 0.495, 'recall': 0.495, 'f1_score': 0.495}\n",
        "# Macro averaging: {'precision': 0.44871794871794873, 'recall': 0.495, 'f1_score': 0.3478611783696529}\n",
        "# Weighted averaging: {'precision': 0.44871794871794873, 'recall': 0.495, 'f1_score': 0.34786117836965297}\n",
        "\n",
        "\n",
        "\n",
        "######    Read the following text and determine if the sentence is true ...\n",
        "#direct\n",
        "# If you prefer a {options_}\n",
        "# return f\"Read the following text and determine if the sentence is true.\\nText: {premise} \\nSentence: {hypothesis} \\nProvide the final answer: {options_}.\"\n",
        "# Micro averaging: {'precision': 0.515, 'recall': 0.515, 'f1_score': 0.515}\n",
        "# Macro averaging: {'precision': 0.5197394393999211, 'recall': 0.515, 'f1_score': 0.48402883055400414}\n",
        "# Weighted averaging: {'precision': 0.5197394393999211, 'recall': 0.515, 'f1_score': 0.4840288305540041}\n",
        "\n",
        "#cot:\n",
        "# return f\"Read the following text and determine if the sentence is true.\\nText: {premise} \\nSentence: {hypothesis} \\nPlease answer by reasoning step by step and providing a final answer.\\n{options_}\"\n",
        "# Micro averaging: {'precision': 0.52, 'recall': 0.52, 'f1_score': 0.52}\n",
        "# Macro averaging: {'precision': 0.5233754090696587, 'recall': 0.52, 'f1_score': 0.5020230314347961}\n",
        "# Weighted averaging: {'precision': 0.5233754090696587, 'recall': 0.52, 'f1_score': 0.5020230314347961}\n",
        "\n",
        "# result: return f\"{premise} \\nQuestion: Does this imply that {hypothesis}? Please answer by reasoning step by step and providing a final answer.\\n{options_}\"\n",
        "# Micro averaging: {'precision': 0.52, 'recall': 0.52, 'f1_score': 0.52}\n",
        "# Macro averaging: {'precision': 0.5233754090696587, 'recall': 0.52, 'f1_score': 0.5020230314347961}\n",
        "# Weighted averaging: {'precision': 0.5233754090696587, 'recall': 0.52, 'f1_score': 0.5020230314347961}\n",
        "\n",
        "\n",
        "# #######two shots:\n",
        "#  two shots:+ cot\n",
        "#     return (\n",
        "#     f\"Here are two examples of input and answer:\\n\"\n",
        "#     f\"Example 1: Input: {text_1}, Answer: {label_1}\\n\"\n",
        "#     f\"Example 2: Input: {text_2}, Answer: {label_2}\\n\\n\"\n",
        "#     f\"Now consider the following:\\nPremise: {premise}\\n\"\n",
        "#     f\"Question: Does this imply that \\\"{hypothesis}\\\"?\\n\"\n",
        "#     f\"Please provide your reasoning step by step, and conclude with a final answer.\\n{options_}\"\n",
        "\n",
        "\n",
        "# Micro averaging: {'precision': 0.52, 'recall': 0.52, 'f1_score': 0.52}\n",
        "# Macro averaging: {'precision': 0.5233754090696587, 'recall': 0.52, 'f1_score': 0.5020230314347961}\n",
        "# Weighted averaging: {'precision': 0.5233754090696587, 'recall': 0.52, 'f1_score': 0.5020230314347961}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHu_iUuksns9"
      },
      "source": [
        "# 5. FLAN-T5_cot+2shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDsUZkhvtfY3"
      },
      "outputs": [],
      "source": [
        "def get_input_text_cot_2shot(premise, hypothesis):\n",
        "    options_prefix = \"OPTIONS:\\n- \"\n",
        "    separator = \"\\n- \"\n",
        "    options_ = options_prefix + f\"{separator}\".join([\"Entailment\", \"Contradiction\"])\n",
        "\n",
        "    # Two-shot examples\n",
        "    example_1_premise = \"Primary trial evidence are INTERVENTION 1:   Laser Therapy Alone  therapist administered laser treatment  laser: therapist administered laserINTERVENTION 2:   Mld Alone  therapist administered manual lymphatic drainage  manual lymphatic drainage: therapist administered massage therapy Secondary trial evidence are INTERVENTION 1:   Part A Abemaciclib: HR+, HER2+ Breast Cancer  Abemaciclib 200 mg was administered orally once every 12 hours on days 1-21 of a 21-day cycle when administered as a single agent or in combination with endocrine therapy (ET). Participants with hormone receptor positive HR+, HER2+ breast cancer receiving concurrent trastuzumab, 150 mg abemaciclib was given orally once every 12 hours on days 1-21 of a 21-day cycle. Participants may continue to receive treatment until discontinuation criteria are met.INTERVENTION 2:   Part B Abemaciclib: HR+, HER2- Breast Cancer  Abemaciclib 200 mg was administered orally once every 12 hours on days 1-21 of a 21-day cycle when administered as a single agent or for in combination with endocrine therapy (ET).  Participants may continue to receive treatment until discontinuation criteria are met.\"\n",
        "    example_1_hypothesis = \"Laser Therapy is in each cohort of the primary trial and the secondary trial, along with neoadjuvant chemotherapy. \"\n",
        "    example_1_answer = \"Contradiction\"\n",
        "\n",
        "    example_2_premise = \"Primary trial evidence are Adverse Events 1:  Total: 5/32 (15.63%)  Febrile neutropenia 1/32 (3.13%)  Supraventricular tachycardia 1/32 (3.13%)  Hypersensitivity 2/32 (6.25%)  Catheter site infection 1/32 (3.13%)  Confusional state 1/32 (3.13%) Secondary trial evidence are Adverse Events 1:  Total: 285/752 (37.90%)  Anaemia 2/752 (0.27%)  Disseminated intravascular coagulation 2/752 (0.27%)  Febrile neutropenia 51/752 (6.78%)  Neutropenia 47/752 (6.25%)  Thrombocytopenia 2/752 (0.27%)  Atrial fibrillation 1/752 (0.13%)  Atrial flutter 0/752 (0.00%)  Cardiac failure congestive 1/752 (0.13%)  Left ventricular dysfunction 0/752 (0.00%)Adverse Events 2:  Total: 117/382 (30.63%)  Anaemia 3/382 (0.79%)  Disseminated intravascular coagulation 0/382 (0.00%)  Febrile neutropenia 11/382 (2.88%)  Neutropenia 20/382 (5.24%)  Thrombocytopenia 0/382 (0.00%)  Atrial fibrillation 1/382 (0.26%)  Atrial flutter 1/382 (0.26%)  Cardiac failure congestive 0/382 (0.00%)  Left ventricular dysfunction 1/382 (0.26%) \"\n",
        "    example_2_hypothesis = \"Heart-related adverse events were recorded in both the primary trial and the secondary trial.\"\n",
        "    example_2_answer = \"Entailment\"\n",
        "\n",
        "   # Introduction text with examples\n",
        "    example_intro = \"Here are my 2 examples:\\n\"\n",
        "\n",
        "    # Example text (Two-shot)\n",
        "    two_shot_text = (\n",
        "        f\"{example_intro}\"\n",
        "        f\"Example 1:\\nPremise: {example_1_premise}\\n\"\n",
        "        f\"Hypothesis: {example_1_hypothesis}\\n\"\n",
        "        f\"Answer: {example_1_answer}\\n\\n\"\n",
        "        f\"Example 2:\\nPremise: {example_2_premise}\\n\"\n",
        "        f\"Hypothesis: {example_2_hypothesis}\\n\"\n",
        "        f\"Answer: {example_2_answer}\\n\\n\")\n",
        "\n",
        "\n",
        "    # For the actual input\n",
        "    input_text = f\"{premise} \\nQuestion: Does this imply that {hypothesis}? Please answer by reasoning step by step and providing a final answer.\\n{options_}\"\n",
        "\n",
        "    # Combine two-shot examples with the actual input\n",
        "    return two_shot_text + input_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3KvzvUvijVy"
      },
      "outputs": [],
      "source": [
        "samples_cot = []\n",
        "item=[]\n",
        "for item in data_expanded:\n",
        "    primary_evidence = \"\".join(item['primary_evidence'])\n",
        "    sentence = f\"Primary trial evidence are {primary_evidence}\"\n",
        "    secondary_evidence = item.get(\"secondary_evidence\")\n",
        "    if secondary_evidence:\n",
        "        secondary_evidence = \"\".join(item['secondary_evidence'])\n",
        "        sentence = f\"{sentence} Secondary trial evidence are {secondary_evidence}\"\n",
        "    input_text =get_input_text_cot_2shot(sentence, item['statement'])\n",
        "    temp = {\"text\":input_text, \"label\":item['label']}\n",
        "    samples_cot.append(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDHtL8FDjm10"
      },
      "outputs": [],
      "source": [
        "labels = []\n",
        "pred = []\n",
        "samples= samples_cot\n",
        "with torch.inference_mode():\n",
        "    for samples in tqdm.tqdm(samples):\n",
        "        labels.append(samples[\"label\"])\n",
        "        input_ids = tokenizer(samples[\"text\"], return_tensors=\"pt\")\n",
        "        # print(input_ids)\n",
        "\n",
        "        outputs = model.generate(**input_ids, max_length = 1024)\n",
        "        # pred.append(tokenizer.decode(outputs[0]))\n",
        "        pred.append(tokenizer.batch_decode(outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vwY5GJ5ZkGcf"
      },
      "outputs": [],
      "source": [
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkchPRjfkHyJ"
      },
      "outputs": [],
      "source": [
        "cleaned_pred = []\n",
        "data=pred\n",
        "for sentence in data:\n",
        "    # Extract the string from the list (assuming each sentence is a list with one element)\n",
        "    if isinstance(sentence, list):\n",
        "        sentence = sentence[0]\n",
        "\n",
        "    # Strip unwanted tokens and spaces\n",
        "    sentence = sentence.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip()\n",
        "    # Now check for \"Contradiction\" or \"Entailment\"\n",
        "    if \"Contradiction\" in sentence:\n",
        "        cleaned_pred.append(\"Contradiction\")\n",
        "    elif \"Entailment\" in sentence:\n",
        "        cleaned_pred.append(\"Entailment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UPJ2m8BIu-kg"
      },
      "outputs": [],
      "source": [
        "cleaned_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqlSUM8akRzM"
      },
      "outputs": [],
      "source": [
        "metrics_micro = compute_metrics(labels, cleaned_pred, averaging=\"micro\")\n",
        "metrics_macro = compute_metrics(labels, cleaned_pred, averaging=\"macro\")\n",
        "metrics_weighted = compute_metrics(labels, cleaned_pred, averaging=\"weighted\")\n",
        "save_metrics_to_csv('FLAN-T5-COT+2shot', metrics_micro, metrics_macro, metrics_weighted, result_path)\n",
        "# Print results\n",
        "print(\"Micro averaging:\", metrics_micro)\n",
        "print(\"Macro averaging:\", metrics_macro)\n",
        "print(\"Weighted averaging:\", metrics_weighted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAlVtND_tCzQ"
      },
      "source": [
        "# 6. FLAN-T5 FINE-TUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uc31z_AU1KxM"
      },
      "outputs": [],
      "source": [
        "file_path='/content/drive/MyDrive/sepi_master_thesis/data/'\n",
        "\n",
        "train_path = file_path + 'combined_train2.json'\n",
        "with open(train_path) as json_file:\n",
        "    train_data = json.load(json_file)# training data (list of dicts with \"text\" and \"label\")\n",
        "\n",
        "dev_path = file_path + 'combined_dev2.json'\n",
        "with open(dev_path) as json_file:\n",
        "    dev_data = json.load(json_file)  #  dev/validation data\n",
        "\n",
        "test_path = file_path + 'combined_test2.json'\n",
        "with open(test_path) as json_file:\n",
        "    test_data = json.load(json_file)  #  test data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kQbREMgvCSft"
      },
      "outputs": [],
      "source": [
        "test_data[50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIk_wREotbzX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tqdm\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "epochs = 4  # Number of epochs\n",
        "# train_data\n",
        "# dev_data\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    for sample in tqdm.tqdm(train_data, desc=\"Training\"):\n",
        "        input_ids = tokenizer(sample[\"text\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=512).input_ids.to(device)\n",
        "        labels = tokenizer(sample[\"label\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=512).input_ids.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluation phase (optional)\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for sample in dev_data:\n",
        "            input_ids = tokenizer(sample[\"text\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=512).input_ids.to(device)\n",
        "            labels = tokenizer(sample[\"label\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=512).input_ids.to(device)\n",
        "\n",
        "            # Forward pass (no backprop)\n",
        "            outputs = model(input_ids=input_ids, labels=labels)\n",
        "            total_loss += outputs.loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} - Evaluation Loss: {total_loss / len(dev_data):.4f}\")\n",
        "\n",
        "# Save the fine-tuned model to Google Drive\n",
        "save_path = '/content/drive/MyDrive/sepi_master_thesis/large-x2/'\n",
        "\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEUN5BC65OkH"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/drive/MyDrive/sepi_master_thesis/result_25.11.2024/models/large\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "labels = []\n",
        "pred = []\n",
        "samples= test_data\n",
        "with torch.inference_mode():\n",
        "    for samples in tqdm.tqdm(samples):\n",
        "        labels.append(samples[\"label\"])\n",
        "        input_ids = tokenizer(samples[\"text\"], return_tensors=\"pt\")\n",
        "        # print(input_ids)\n",
        "\n",
        "        outputs = model.generate(**input_ids, max_length = 1024)\n",
        "        # pred.append(tokenizer.decode(outputs[0]))\n",
        "        pred.append(tokenizer.batch_decode(outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sU5hc-Z19oQ8"
      },
      "outputs": [],
      "source": [
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsjtsZ4-E4WM"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the Excel file\n",
        "file_path = '/content/drive/MyDrive/sepi_master_thesis/result_25.11.2024/methods_output_test_data2.xlsx' # Replace with your Excel file path\n",
        "df = pd.read_excel(file_path)\n",
        "# Define the value for the new column\n",
        "new_column_value = cleaned_pred  # This will populate the new column\n",
        "\n",
        "# Add the new column named \"fine-tuned\" to the DataFrame\n",
        "df[\"fine-tuned_largex2_clean_pred\"] = new_column_value\n",
        "\n",
        "# Save the updated DataFrame back to an Excel file\n",
        "output_path = '/content/drive/MyDrive/sepi_master_thesis/result_25.11.2024/methods_output_test_data2.xlsx'  # Path for the updated Excel file\n",
        "df.to_excel(output_path, index=False)\n",
        "\n",
        "print(f\"New column added and saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEmliId7_NMr"
      },
      "outputs": [],
      "source": [
        "cleaned_pred = []\n",
        "data=pred\n",
        "for sentence in data:\n",
        "    # Extract the string from the list (assuming each sentence is a list with one element)\n",
        "    if isinstance(sentence, list):\n",
        "        sentence = sentence[0]\n",
        "\n",
        "    # Strip unwanted tokens and spaces\n",
        "    sentence = sentence.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip()\n",
        "    # Now check for \"Contradiction\" or \"Entailment\"\n",
        "    if \"Contradiction\" in sentence:\n",
        "        cleaned_pred.append(\"Contradiction\")\n",
        "    elif \"Entailment\" in sentence:\n",
        "        cleaned_pred.append(\"Entailment\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fN5rkCst_XiV"
      },
      "outputs": [],
      "source": [
        "cleaned_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOOT2Us1_iyy"
      },
      "outputs": [],
      "source": [
        "# Compute metrics with different averaging methods\n",
        "metrics_micro = compute_metrics(labels, cleaned_pred, averaging=\"micro\")\n",
        "metrics_macro = compute_metrics(labels, cleaned_pred, averaging=\"macro\")\n",
        "metrics_weighted = compute_metrics(labels, cleaned_pred, averaging=\"weighted\")\n",
        "save_metrics_to_csv('FLAN-T5_fine-tune', metrics_micro, metrics_macro, metrics_weighted, result_path)\n",
        "# Print results\n",
        "print(\"Micro averaging:\", metrics_micro)\n",
        "print(\"Macro averaging:\", metrics_macro)\n",
        "print(\"Weighted averaging:\", metrics_weighted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nard7gHeiCWQ"
      },
      "source": [
        "# 3.  2_shot+ with COT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHqje_GbiCWQ"
      },
      "outputs": [],
      "source": [
        "samples = []\n",
        "for sample in data_expanded:\n",
        "    primary_evidence = \"\".join(sample['primary_evidence'])\n",
        "    sentence = f\"Primary trial evidence are {primary_evidence}\"\n",
        "    secondary_evidence = sample.get(\"secondary_evidence\")\n",
        "    if secondary_evidence:\n",
        "        secondary_evidence = \"\".join(sample['secondary_evidence'])\n",
        "        sentence = f\"{sentence} Secondary trial evidence are {secondary_evidence}\"\n",
        "    input_text = get_input_text(sentence, sample['statement'])\n",
        "    temp = {\"text\":input_text, \"label\":sample['label']}\n",
        "    samples.append(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BejQ_XOoiCWR"
      },
      "outputs": [],
      "source": [
        "#runing model:\n",
        "labels = []\n",
        "pred = []\n",
        "# samples= samples[0:8]\n",
        "with torch.inference_mode():\n",
        "    for samples in tqdm.tqdm(samples):\n",
        "        labels.append(samples[\"label\"])\n",
        "        input_ids = tokenizer(samples[\"text\"], return_tensors=\"pt\")\n",
        "        # print(input_ids)\n",
        "\n",
        "        outputs = model.generate(**input_ids, max_length = 1024)\n",
        "        # pred.append(tokenizer.decode(outputs[0]))\n",
        "        pred.append(tokenizer.batch_decode(outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmS72AHwiCWR"
      },
      "outputs": [],
      "source": [
        "# Compute metrics with different averaging methods\n",
        "pred=cleaned_predictions\n",
        "metrics_micro = compute_metrics(labels, pred, averaging=\"micro\")\n",
        "metrics_macro = compute_metrics(labels, pred, averaging=\"macro\")\n",
        "metrics_weighted = compute_metrics(labels, pred, averaging=\"weighted\")\n",
        "\n",
        "# Print results\n",
        "print(\"Micro averaging:\", metrics_micro)\n",
        "print(\"Macro averaging:\", metrics_macro)\n",
        "print(\"Weighted averaging:\", metrics_weighted)\n",
        "\n",
        "\n",
        "#########################\n",
        "# #tf_idf:\n",
        "# F1:0.502415\n",
        "# precision_score:0.485981\n",
        "# recall_score:0.520000\n",
        "\n",
        "########   Question: Does this imply that {hypothesis}?\n",
        "# #direct\n",
        "# return f\"{premise} \\nQuestion: Does this imply that {hypothesis}? Just provide the final answer.\\n{options_}\"\n",
        "# Micro averaging: {'precision': 0.52, 'recall': 0.52, 'f1_score': 0.52}\n",
        "# Macro averaging: {'precision': 0.53125, 'recall': 0.52, 'f1_score': 0.4725274725274725}\n",
        "# Weighted averaging: {'precision': 0.53125, 'recall': 0.52, 'f1_score': 0.47252747252747246}\n",
        "\n",
        "##COT\n",
        "##return f\"{premise} \\nQuestion: Does this imply that {hypothesis}? Please answer by reasoning step by step and providing a final answer.\\n{options_}\"\n",
        "#after adding the posstprossing function:\n",
        "# Micro averaging: {'precision': 0.54, 'recall': 0.54, 'f1_score': 0.54}\n",
        "# Macro averaging: {'precision': 0.5407996736026112, 'recall': 0.54, 'f1_score': 0.5377349010149733}\n",
        "# Weighted averaging: {'precision': 0.5407996736026112, 'recall': 0.54, 'f1_score': 0.5377349010149733}\n",
        "\n",
        "\n",
        "\n",
        "# #######two shots:\n",
        "#  two shots:+ cot\n",
        "#     return (\n",
        "#     f\"Here are two examples of input and answer:\\n\"\n",
        "#     f\"Example 1: Input: {text_1}, Answer: {label_1}\\n\"\n",
        "#     f\"Example 2: Input: {text_2}, Answer: {label_2}\\n\\n\"\n",
        "#     f\"Now consider the following:\\nPremise: {premise}\\n\"\n",
        "#     f\"Question: Does this imply that \\\"{hypothesis}\\\"?\\n\"\n",
        "#     f\"Please provide your reasoning step by step, and conclude with a final answer.\\n{options_}\"\n",
        "\n",
        "\n",
        "# Micro averaging: {'precision': 0.52, 'recall': 0.52, 'f1_score': 0.52}\n",
        "# Macro averaging: {'precision': 0.5233754090696587, 'recall': 0.52, 'f1_score': 0.5020230314347961}\n",
        "# Weighted averaging: {'precision': 0.5233754090696587, 'recall': 0.52, 'f1_score': 0.5020230314347961}\n",
        "\n",
        "\n",
        "# fine tunning\n",
        "\n",
        "# Micro averaging: {'precision': 0.548, 'recall': 0.548, 'f1_score': 0.548}\n",
        "# Macro averaging: {'precision': 0.549016404156591, 'recall': 0.548, 'f1_score': 0.5456446217189912}\n",
        "# Weighted averaging: {'precision': 0.549016404156591, 'recall': 0.548, 'f1_score': 0.5456446217189913}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPELfQmTbq1l"
      },
      "source": [
        "# sepi's results\n",
        "## TFIDF\n",
        "F1:0.492424\n",
        "\n",
        "precision_score:0.467626\n",
        "\n",
        "recall_score:0.520000\n",
        "\n",
        "## ZERO SHOT\n",
        "\n",
        "Question :{premise} \\n Question: Does this imply that {hypothesis}? {options_}\n",
        "\n",
        "Micro averaging: {'precision': 0.49 ,    'recall':0.49,     'f1_score': 0.49}\n",
        "\n",
        "Macro averaging: {'precision': 0.3247814735,    'recall': 0.3266666667,\n",
        "   'f1_score': 0.3005619396}\n",
        "\n",
        "Weighted averaging: {'precision': 0.4871722102,    'recall': 0.49,\n",
        "   'f1_score': 0.4508429094}\n",
        "\n",
        "## TWO SHO\n",
        "\n",
        "Question: \"Here are my 2 examples:\"Example 1:Premise-Hypothesis-Answer\n",
        "           Example 2:Premise-Hypothesis-Answer/{premise} \\nQuestion: Does this imply that {hypothesis}? Just provide the final answer.\\n{options_}  \"\n",
        "  \n",
        "\n",
        "Micro averaging: {'precision': 0.518, 'recall': 0.518, 'f1_score': 0.518}\n",
        "\n",
        "Macro averaging: {'precision': 0.520612323421096, 'recall': 0.518, 'f1_score': 0.5022286113210976}\n",
        "\n",
        "Weighted averaging: {'precision': 0.5206123234210961, 'recall': 0.518, 'f1_score': 0.5022286113210976}\n",
        "\n",
        "\n",
        "## COT\n",
        "\n",
        "Question:\n",
        "\n",
        "Micro averaging: {'precision': 0.512, 'recall': 0.512, 'f1_score': 0.512}\n",
        "\n",
        "Macro averaging: {'precision': 0.5120069159836065, 'recall': 0.512, 'f1_score': 0.5119297178793747}\n",
        "\n",
        "Weighted averaging: {'precision': 0.5120069159836065, 'recall': 0.512, 'f1_score': 0.5119297178793746}\n",
        "\n",
        "##cot+2shot\n",
        "Micro averaging: {'precision': 0.504, 'recall': 0.504, 'f1_score': 0.504}\n",
        "\n",
        "Macro averaging: {'precision': 0.5066602728047741, 'recall': 0.504, 'f1_score': 0.44897705256047915}\n",
        "\n",
        "Weighted averaging: {'precision': 0.5066602728047741, 'recall': 0.504, 'f1_score': 0.4489770525604792}\n",
        "\n",
        "\n",
        "\n",
        "##finetuned\n",
        "\n",
        "Micro averaging: {'precision': 0.548, 'recall': 0.548, 'f1_score': 0.548}\n",
        "\n",
        "Macro averaging: {'precision': 0.549016404156591, 'recall': 0.548, 'f1_score': 0.5456446217189912}\n",
        "\n",
        "Weighted averaging: {'precision': 0.549016404156591, 'recall': 0.548, 'f1_score': 0.5456446217189913}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9wEoCRW2vwV"
      },
      "source": [
        "# Mistral-7B\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mq_uu5HnZq0e"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "br-s0oq7HKj2"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lGia0vRX47tU"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "jmp1esxTGZ9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=df"
      ],
      "metadata": {
        "id": "z4rrnfouTI8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "WdGnPfD-TMWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Q63_AuawlSwp"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "model = model.to('cuda')\n",
        "\n",
        "responses = []\n",
        "\n",
        "for input_text in tqdm(data['text'], desc=\"Processing inputs\", total=len(data['text'])):\n",
        "\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=5,  # Generate a short response\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    responses.append(response)\n",
        "\n",
        "data['model_response_mistral_twoshots'] = responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1b4V6bSgFxJ"
      },
      "outputs": [],
      "source": [
        "responses[499]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tit8Lwzhg6cy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "responses_df = pd.DataFrame(responses, columns=[\"model_response\"])\n",
        "\n",
        "responses_df.to_csv('/content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/responses-two-shot-mistral.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "responses_df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2EgUkVqEY2ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59v5Urh5hxYR"
      },
      "outputs": [],
      "source": [
        "labels = []\n",
        "\n",
        "#responses = df['mistral-7b-zero-shot']\n",
        "#responses= q1['prediction']\n",
        "for idx, response in enumerate(responses):  # Ensure idx is properly defined\n",
        "    tokens = response.split()\n",
        "\n",
        "    # Check if the last token is valid\n",
        "    if tokens and tokens[-1] in [\"Entailment\", \"Contradiction\"]:\n",
        "        labels.append(tokens[-1])  # Use the last token if valid\n",
        "    else:\n",
        "        labels.append('nan')\n",
        "\n",
        "labels_df = pd.DataFrame(labels, columns=[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "umw0qMy4ZFNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"//content/drive/MyDrive/pycharm_results/test_zero_shot_qwen.csv\"\n",
        "df= pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "C_yfmWNLaH9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eSLJfmNiqQd"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def calculate_metrics(true_labels, predicted_labels):\n",
        "    \"\"\"\n",
        "    Calculate precision, recall, and F1 scores (weighted, micro, and macro).\n",
        "\n",
        "    Args:\n",
        "        true_labels (list or pd.Series): True labels.\n",
        "        predicted_labels (list or pd.Series): Predicted labels.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing precision, recall, and F1 scores\n",
        "              for weighted, micro, and macro averaging.\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "\n",
        "    # Weighted metrics\n",
        "    metrics['precision_weighted'] = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "    metrics['recall_weighted'] = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "    metrics['f1_weighted'] = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "    # Micro metrics\n",
        "    metrics['precision_micro'] = precision_score(true_labels, predicted_labels, average='micro')\n",
        "    metrics['recall_micro'] = recall_score(true_labels, predicted_labels, average='micro')\n",
        "    metrics['f1_micro'] = f1_score(true_labels, predicted_labels, average='micro')\n",
        "\n",
        "    # Macro metrics\n",
        "    metrics['precision_macro'] = precision_score(true_labels, predicted_labels, average='macro')\n",
        "    metrics['recall_macro'] = recall_score(true_labels, predicted_labels, average='macro')\n",
        "    metrics['f1_macro'] = f1_score(true_labels, predicted_labels, average='macro')\n",
        "\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "GiFHThh_aYgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['predicted_output'] = df['predicted_output'].str.capitalize()"
      ],
      "metadata": {
        "id": "NrRGZiNpbhNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_rows = df[df['mistral_twoshots_lable'].astype(str) == 'nan']\n",
        "print(nan_rows)\n"
      ],
      "metadata": {
        "id": "l0LukCK6bujd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nan_rows.index.tolist())\n"
      ],
      "metadata": {
        "id": "jRfGIPn1ciX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "responses[499]"
      ],
      "metadata": {
        "id": "jkpDauixcn0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_rows"
      ],
      "metadata": {
        "id": "kJRYQUeebza6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsriBBj1itMI"
      },
      "outputs": [],
      "source": [
        "metrics = calculate_metrics(df['label'], df['predicted_output'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MNw3ZJF2ktO"
      },
      "outputs": [],
      "source": [
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTmnkD3InR-q"
      },
      "outputs": [],
      "source": [
        "metrics #qwen zero shot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'] = df['label'].astype(str)\n",
        "df['mistral_twoshots_lable'] = df['mistral_twoshots_lable'].astype(str)"
      ],
      "metadata": {
        "id": "rLY4oq5cbIW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sl-riKr9fu-o"
      },
      "outputs": [],
      "source": [
        "metrics = calculate_metrics(df['label'], df['mistral_twoshots_lable'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics"
      ],
      "metadata": {
        "id": "j3zWUHfIhkib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5e4G0sqkHqu"
      },
      "outputs": [],
      "source": [
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTDCfQ1_JD5M"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "import ast\n",
        "import re\n",
        "import openpyxl\n",
        "from openpyxl.styles import Color, PatternFill\n",
        "import string\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "!pip install xlsxwriter\n",
        "\n",
        "\n",
        "def compute_metrics(label, pred, averaging=\"micro\"):\n",
        "    \"\"\"\n",
        "    Compute precision, recall, and F1 score using the specified averaging method.\n",
        "\n",
        "    Parameters:\n",
        "    - labels: True labels\n",
        "    - pred: Predicted labels\n",
        "    - averaging: Averaging method (\"micro\", \"macro\", \"weighted\")\n",
        "\n",
        "    Returns:\n",
        "    - A dictionary with precision, recall, and F1 score\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the precision, recall, and F1 score\n",
        "\n",
        "    precision = precision_score(label, pred, average=averaging)\n",
        "    recall = recall_score(label, pred, average=averaging)\n",
        "    f1 = f1_score(label, pred, average=averaging)\n",
        "\n",
        "    # Return the results as a dictionary\n",
        "    return precision, recall,f1\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_all_methods(input_path,  output_path ):\n",
        "    # Read the Excel file into a DataFrame\n",
        "    df = pd.read_csv(input_path)\n",
        "    label=df[\"label\"]\n",
        "\n",
        "    # excluded_columns = df.columns[4:]\n",
        "    #excluded_columns = [\"TF_IDF_prediction\"]\n",
        "\n",
        "    # Then append the other columns that end with \"clean_pred\"\n",
        "    excluded_columns = [col for col in df.columns if col.endswith(\"clean_pred\")]\n",
        "\n",
        "    # Initialize an empty DataFrame to store the results\n",
        "    # results_df = pd.DataFrame(columns=['Column', 'Precision', 'Recall', 'F1-score'])\n",
        "    results_df = pd.DataFrame(columns=['Column'])\n",
        "    print(excluded_columns)\n",
        "    for col in excluded_columns:\n",
        "        # Compute metrics with different averaging methods\n",
        "\n",
        "        mi_precision, mi_recall,mi_f1 = compute_metrics(label, df[col], averaging=\"micro\")\n",
        "        ma_precision, ma_recall,ma_f1 = compute_metrics(label, df[col], averaging=\"macro\")\n",
        "\n",
        "\n",
        "        # Append the results to the DataFrame\n",
        "        results_df = pd.concat([results_df, pd.DataFrame({\n",
        "            'Column': [col],\n",
        "            'micro_Precision': [mi_precision],\n",
        "            'micro_Recall': [mi_recall],\n",
        "            'micro_F1-score': [mi_f1],\n",
        "            'macro_Precision': [ma_precision],\n",
        "            'macro_Recall': [ma_recall],\n",
        "            'macro_F1-score': [ma_f1],\n",
        "\n",
        "\n",
        "        })], ignore_index=True)\n",
        "\n",
        "\n",
        "    report_to_excel(results_df,output_path)\n",
        "\n",
        "    print(f\"results_partial_match is saved in {output_path}!...\" )\n",
        "\n",
        "    return results_df\n",
        "\n",
        "\n",
        "def report_to_excel(df, excel_file):\n",
        "    \"\"\"\n",
        "    Use conditional formatting on report and write to excel.\n",
        "    \"\"\"\n",
        "    writer = pd.ExcelWriter(excel_file, engine='xlsxwriter')\n",
        "    workbook = writer.book\n",
        "    df.to_excel(writer, sheet_name='Confusion Matrix')\n",
        "    for worksheet in workbook.worksheets():\n",
        "        max_row, max_col = df.shape\n",
        "        for col in range(2,8):\n",
        "            worksheet.conditional_format(1, col, max_row, col ,\n",
        "                                         {'type': '3_color_scale', 'min_value': 0, 'max_value': 1})\n",
        "    # writer.save()\n",
        "    writer.close()\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     parser = argparse.ArgumentParser()\n",
        "#     parser.add_argument('-dp', '--input_path', default='./methods_output_test_data.xlsx')\n",
        "#     parser.add_argument('-op', '--output_path', default='./results_test_data.xlsx')\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "\n",
        "    # Example usage\n",
        "\n",
        "# df = evaluate_all_methods(f\"{args.input_path}\",f\"{args.output_path}\" )\n",
        "# print(df)\n",
        "methods_output_file=\"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_03122024/total.csv\"\n",
        "results_file=\"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_03122024/results_test_data2.xlsx\"\n",
        "rf = evaluate_all_methods(methods_output_file,results_file )\n",
        "\n",
        "rf\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPaZnq-_GCmK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# File path\n",
        "file_path = \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_03122024/responses-zero-shot-qwen.csv\"\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "q1= pd.read_csv(file_path)\n",
        "\n",
        "# Rename the column\n",
        "# Replace 'old_column_name' with the current name and 'new_column_name' with the desired name\n",
        "# df.rename(columns={'model_response': 'mistral-7b-zero-shot'}, inplace=True)\n",
        "#df\n",
        "# # Save the updated DataFrame back to the same file\n",
        "#s.to_csv(file_path, index=False)\n",
        "\n",
        "# print(\"Column renamed and file saved successfully!\")\n",
        "\n",
        "#total.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ro0wtD7SGgCq"
      },
      "outputs": [],
      "source": [
        "q1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFmaSYt_Gjaq"
      },
      "outputs": [],
      "source": [
        "data['qwen2-7b-zero-shot']=q1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3iFrpVxGPdr"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWD_zX3g9RQ4"
      },
      "source": [
        "# Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQh_7uAd9Qnt"
      },
      "outputs": [],
      "source": [
        "  #Primary trial evidence are INTERVENTION 1:   Gefitinib (ZD1839)  ZD1839 at a daily dose of 250 mg or 500 mg depending on final dose in parent trial Secondary trial evidence are INTERVENTION 1:   Zoledronic Acid  Zoledronic acid, vitamin D and calcium supplements.INTERVENTION 2:   Zoledronic Acid + Radiopharmaceuticals  Zoledronic acid, vitamin D and calcium supplements, plus Sr-89 or Sm-153.\n",
        "   #\\n Question: Does this imply that\n",
        "   #The intervention in the primary trial consists of a single drug, whereas in the secondary trial the intervention requires at least 3 different drugs.\n",
        "   #Entailment\n",
        "\n",
        "    # example_1_premise = \"Primary trial evidence are INTERVENTION 1:   Laser Therapy Alone  therapist administered laser treatment  laser: therapist administered laserINTERVENTION 2:   Mld Alone  therapist administered manual lymphatic drainage  manual lymphatic drainage: therapist administered massage therapy Secondary trial evidence are INTERVENTION 1:   Part A Abemaciclib: HR+, HER2+ Breast Cancer  Abemaciclib 200 mg was administered orally once every 12 hours on days 1-21 of a 21-day cycle when administered as a single agent or in combination with endocrine therapy (ET). Participants with hormone receptor positive HR+, HER2+ breast cancer receiving concurrent trastuzumab, 150 mg abemaciclib was given orally once every 12 hours on days 1-21 of a 21-day cycle. Participants may continue to receive treatment until discontinuation criteria are met.INTERVENTION 2:   Part B Abemaciclib: HR+, HER2- Breast Cancer  Abemaciclib 200 mg was administered orally once every 12 hours on days 1-21 of a 21-day cycle when administered as a single agent or for in combination with endocrine therapy (ET).  Participants may continue to receive treatment until discontinuation criteria are met.\"\n",
        "    # example_1_hypothesis = \"Laser Therapy is in each cohort of the primary trial and the secondary trial, along with neoadjuvant chemotherapy. \"\n",
        "    # example_1_answer = \"Contradiction\"\n",
        "\n",
        "    # example_2_premise = \"Primary trial evidence are Adverse Events 1:  Total: 5/32 (15.63%)  Febrile neutropenia 1/32 (3.13%)  Supraventricular tachycardia 1/32 (3.13%)  Hypersensitivity 2/32 (6.25%)  Catheter site infection 1/32 (3.13%)  Confusional state 1/32 (3.13%) Secondary trial evidence are Adverse Events 1:  Total: 285/752 (37.90%)  Anaemia 2/752 (0.27%)  Disseminated intravascular coagulation 2/752 (0.27%)  Febrile neutropenia 51/752 (6.78%)  Neutropenia 47/752 (6.25%)  Thrombocytopenia 2/752 (0.27%)  Atrial fibrillation 1/752 (0.13%)  Atrial flutter 0/752 (0.00%)  Cardiac failure congestive 1/752 (0.13%)  Left ventricular dysfunction 0/752 (0.00%)Adverse Events 2:  Total: 117/382 (30.63%)  Anaemia 3/382 (0.79%)  Disseminated intravascular coagulation 0/382 (0.00%)  Febrile neutropenia 11/382 (2.88%)  Neutropenia 20/382 (5.24%)  Thrombocytopenia 0/382 (0.00%)  Atrial fibrillation 1/382 (0.26%)  Atrial flutter 1/382 (0.26%)  Cardiac failure congestive 0/382 (0.00%)  Left ventricular dysfunction 1/382 (0.26%) \"\n",
        "    # example_2_hypothesis = \"Heart-related adverse events were recorded in both the primary trial and the secondary trial.\"\n",
        "    # example_2_answer = \"Entailment\"\n",
        "\n",
        "\n",
        "\n",
        "    # example_3_premise=\"Primary trial evidence are Inclusion Criteria: Age 52-75 years old; Identification as Latina/Hispanic/Chicana female; Residence in Pilsen, Little Village, East Side or South Chicago; No history of health volunteerism; No history of breast cancer; and Lack of a mammogram within the last two yearsExclusion Criteria: Not meeting all inclusion criteria; Women will be excluded if they participated in formative focus groups.\"\n",
        "    # example_3_hypothesis=\"Patients eligible for the primary trial must live in the USA.\"\n",
        "    # example_3_answer=\"Entailment\"\n",
        "\n",
        "    # example_4_premise=\"Primary trial evidence are Adverse Events 1: Total: 10/71 (14.08%) ATRIAL FIBRILLATION 1/71 (1.41%) CARDIAC TAMPONADE 1/71 (1.41%) PERICARDIAL EFFUSION 1/71 (1.41%) SUPRAVENTRICULAR TACHYCARDIA 1/71 (1.41%) DIARRHOEA 1/71 (1.41%) NAUSEA 1/71 (1.41%) VOMITING 1/71 (1.41%) CHEST PAIN 1/71 (1.41%) PNEUMONIA 1/71 (1.41%) MALIGNANT PLEURAL EFFUSION 1/71 (1.41%) HEPATIC ENCEPHALOPATHY 1/71 (1.41%) .\"\n",
        "    # example_4_hypothesis=\"There were only 3 adverse events in the primary trial which occurred more than twice.\"\n",
        "    # example_4_answer=\"Contradiction\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_ictaIYCntZ"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Initialize tokenizer and model for LLaMA 2\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"  # This is the LLaMA 2 7B model\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HN3spV6lCspG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# input_text= \"\"\"Primary trial evidence are: ['Inclusion Criteria:', '  Voluntarily signed and dated written informed consent', '  Age between 18 and 75 years old (both inclusive)', '  Eastern Cooperative Oncology Group (ECOG) performance status (PS) of  1', '  Life expectancy  3 months.', '  Patients with a histologically/cytologically confirmed diagnosis of advanced and/or unresectable disease of any of the following tumors:', '  Breast cancer', '  Epithelial ovarian cancer or gynecological cancer', '  Head and neck squamous cell carcinoma', '  Non-small cell lung cancer', '  Small cell lung cancer', '  Platinum-refractory germ-cell tumors.', '  Adenocarcinoma or carcinoma of unknown primary site', '  Adequate bone marrow, renal, hepatic, and metabolic function', '  Recovery to grade  1 or to baseline from any Adverse Event (AE) derived from previous treatment (excluding alopecia of any grade).', '  Pre-menopausal women must have a negative pregnancy test before study entry and agree to use a medically acceptable method of contraception throughout the treatment period and for at least six weeks after treatment discontinuation', 'Exclusion Criteria:', '  Prior treatment with PM01183 or weekly paclitaxel or nanoalbumin-paclitaxel', '  Patients who have previously discontinued paclitaxel-based regimes due to drug related toxicity.', '  Known hypersensitivity to bevacizumab or any component of its formulation', '  Patients who have previously discontinued bevacizumab-containing regimes due to drug-related toxicity.', '  More than three prior lines of chemotherapy', '  Less than three months since last taxane-containing therapy.', '  Wash-out period:', '  Less than three weeks since the last chemotherapy-containing regimen', '  Less than three weeks since the last radiotherapy dose', '  Less than four weeks since last monoclonal antibody-containing therapy', '  Concomitant diseases/conditions:', '  Unstable angina, myocardial infarction, valvular heart disease, encephalopathy, ischemic attacks, hemorrhagic or ischemic cerebrovascular accident (CVA) or ongoing pulmonary embolism within last year, arrhythmia, hepatopathy, uncontrolled infection, hemoptysis or oxygen requiring dyspnea, known HIV infection, bleeding risk, muscular problems, peripheral neuropathy, Symptomatic or progressive brain metastases or leptomeningeal disease.', '  Men or pre-menopausal women who are not using an effective method of contraception as previously described; actively breast feeding women.', '  Patients who have pelvic irradiation with doses  45 Grays (Gy).', '  History of previous bone marrow and/or stem cell transplantation.', '  Confirmed bone marrow involvement']\n",
        "# Question: does this imply that Patients with a histologically/cytologically confirmed diagnosis of a resectable Non-small cell lung breast cancer or Small cell lung cancer are eligible for the primary trial\n",
        "# Answer with only one of the options (Entailment or Contradiction):\"\"\"\n",
        "# contradiction\n",
        "\n",
        "# input_text = \"\"\"\n",
        "# Primary trial evidence are INTERVENTION 1:   Laser Therapy Alone therapist administered laser treatment laser: therapist administered laserINTERVENTION 2: Mld Alone therapist administered manual lymphatic drainage manual lymphatic drainage: therapist administered massage therapy Secondary trial evidence are INTERVENTION 1: Part A Abemaciclib: HR+, HER2+ Breast Cancer Abemaciclib 200 mg was administered orally once every 12 hours on days 1-21 of a 21-day cycle when administered as a single agent or in combination with endocrine therapy (ET). Participants with hormone receptor positive HR+, HER2+ breast cancer receiving concurrent trastuzumab, 150 mg abemaciclib was given orally once every 12 hours on days 1-21 of a 21-day cycle. Participants may continue to receive treatment until discontinuation criteria are met.INTERVENTION 2: Part B Abemaciclib: HR+, HER2- Breast Cancer Abemaciclib 200 mg was administered orally once every 12 hours on days 1-21 of a 21-day cycle when administered as a single agent or for in combination with endocrine therapy (ET). Participants may continue to receive treatment until discontinuation criteria are met.\n",
        "# Question: Does this imply that Laser Therapy is in each cohort of the primary trial and the secondary trial, along with neoadjuvant chemotherapy? Answer with only one of the options (Entailment or Contradiction):\n",
        "# \"\"\"\n",
        "#Contradiction\n",
        "\n",
        "input_text=\"\"\"Primary trial evidence are INTERVENTION 1:   Gefitinib (ZD1839)  ZD1839 at a daily dose of 250 mg or 500 mg depending on final dose in parent trial Secondary trial evidence are INTERVENTION 1:   Zoledronic Acid  Zoledronic acid, vitamin D and calcium supplements.INTERVENTION 2:   Zoledronic Acid + Radiopharmaceuticals  Zoledronic acid, vitamin D and calcium supplements, plus Sr-89 or Sm-153.\n",
        "\\n Question: Does this imply that\n",
        "The intervention in the primary trial consists of a single drug, whereas in the secondary trial the intervention requires at least 3 different drugs?\n",
        "Answer with only one of the options (Entailment or Contradiction):\"\"\"\n",
        "# Entailment\n",
        "\n",
        "# input_text=\"\"\"Primary trial evidence are Inclusion Criteria: Age 52-75 years old; Identification as Latina/Hispanic/Chicana female; Residence in Pilsen, Little Village, East Side or South Chicago; No history of health volunteerism; No history of breast cancer; and Lack of a mammogram within the last two yearsExclusion Criteria: Not meeting all inclusion criteria; Women will be excluded if they participated in formative focus groups.\"\n",
        "# \\n Question: Does this imply that\n",
        "# Patients eligible for the primary trial must live in the USA?\n",
        "# Answer with only one of the options (Entailment or Contradiction):\"\"\"\n",
        "# Entailment\n",
        "\n",
        "\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Forward pass through the model\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Extract logits and compute probabilities\n",
        "logits = outputs.logits\n",
        "probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "# Get the predicted label\n",
        "predicted_label = torch.argmax(probs, dim=-1).item()\n",
        "\n",
        "# Map the label to the actual class\n",
        "# Assuming the model uses labels 0 = Contradiction, 1 = Entailment\n",
        "labels = [\"Contradiction\", \"Entailment\"]\n",
        "print(labels[predicted_label])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcz6BCR3gRiA"
      },
      "outputs": [],
      "source": [
        "#combined_test_zero_shot.csv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# File path\n",
        "file_path = \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_03122024/combined_test_zero_shot.csv\"\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Rename the column\n",
        "# Replace 'old_column_name' with the current name and 'new_column_name' with the desired name\n",
        "# df.rename(columns={'model_response': 'mistral-7b-zero-shot'}, inplace=True)\n",
        "#df\n",
        "# # Save the updated DataFrame back to the same file\n",
        "#df.to_csv(file_path, index=False)\n",
        "\n",
        "# print(\"Column renamed and file saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOKa4RVflzE2"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "---6janBd7ZG"
      },
      "source": [
        "# Qwen2-7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gCqbqzId8jv"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load Qwen-2-7B model and tokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
        "# Load model and tokenize\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "base_model_path = \"/content/drive/MyDrive/sepi_master_thesis/models/qwen2-original\"\n",
        "model.save_pretrained(base_model_path)\n",
        "tokenizer.save_pretrained(base_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEDFUZ7izKV_"
      },
      "outputs": [],
      "source": [
        "input_text =\"\"\"Primary trial evidence are: ['Adverse Events 1:', '  Total: 1/13 (7.69%)', '  Rapid disease progression  [1]1/13 (7.69%)', '  Increased pleural effusion  [2]0/13 (0.00%)', 'Adverse Events 2:', '  Total: 1/4 (25.00%)', '  Rapid disease progression  [1]0/4 (0.00%)', '  Increased pleural effusion  [2]1/4 (25.00%)']\n",
        " Question: does this imply that 25% of patients in the primary trial suffer Increased pleural effusion and Rapid disease progression\n",
        " Answer with only one of the options (Entailment or Contradiction):\"\"\"\n",
        " #Contradiction\n",
        "\n",
        "# input_text=\"\"\"Primary trial evidence are INTERVENTION 1:   Gefitinib (ZD1839)  ZD1839 at a daily dose of 250 mg or 500 mg depending on final dose in parent trial Secondary trial evidence are INTERVENTION 1:   Zoledronic Acid  Zoledronic acid, vitamin D and calcium supplements.INTERVENTION 2:   Zoledronic Acid + Radiopharmaceuticals  Zoledronic acid, vitamin D and calcium supplements, plus Sr-89 or Sm-153.\n",
        "# Question: Does this imply that\n",
        "# The intervention in the primary trial consists of a single drug, whereas in the secondary trial the intervention requires at least 3 different drugs?\n",
        "# Answer with only one of the options (Entailment or Contradiction):\"\"\"\n",
        "#Entailment\n",
        "\n",
        "# input_text = \"\"\"\n",
        "# Primary trial evidence are INTERVENTION 1:   Laser Therapy Alone therapist administered laser treatment laser: therapist administered laserINTERVENTION 2: Mld Alone therapist administered manual lymphatic drainage manual lymphatic drainage: therapist administered massage therapy Secondary trial evidence are INTERVENTION 1: Part A Abemaciclib: HR+, HER2+ Breast Cancer Abemaciclib 200 mg was administered orally once every 12 hours on days 1-21 of a 21-day cycle when administered as a single agent or in combination with endocrine therapy (ET). Participants with hormone receptor positive HR+, HER2+ breast cancer receiving concurrent trastuzumab, 150 mg abemaciclib was given orally once every 12 hours on days 1-21 of a 21-day cycle. Participants may continue to receive treatment until discontinuation criteria are met.INTERVENTION 2: Part B Abemaciclib: HR+, HER2- Breast Cancer Abemaciclib 200 mg was administered orally once every 12 hours on days 1-21 of a 21-day cycle when administered as a single agent or for in combination with endocrine therapy (ET). Participants may continue to receive treatment until discontinuation criteria are met.\n",
        "# Question: Does this imply that Laser Therapy is in each cohort of the primary trial and the secondary trial, along with neoadjuvant chemotherapy?\n",
        "# Answer with only one of the options (Entailment or Contradiction):\n",
        "# \"\"\"\n",
        "#Contradiction\n",
        "\n",
        "# input_text= \"\"\"Primary trial evidence are: ['Inclusion Criteria:', '  Voluntarily signed and dated written informed consent', '  Age between 18 and 75 years old (both inclusive)', '  Eastern Cooperative Oncology Group (ECOG) performance status (PS) of  1', '  Life expectancy  3 months.', '  Patients with a histologically/cytologically confirmed diagnosis of advanced and/or unresectable disease of any of the following tumors:', '  Breast cancer', '  Epithelial ovarian cancer or gynecological cancer', '  Head and neck squamous cell carcinoma', '  Non-small cell lung cancer', '  Small cell lung cancer', '  Platinum-refractory germ-cell tumors.', '  Adenocarcinoma or carcinoma of unknown primary site', '  Adequate bone marrow, renal, hepatic, and metabolic function', '  Recovery to grade  1 or to baseline from any Adverse Event (AE) derived from previous treatment (excluding alopecia of any grade).', '  Pre-menopausal women must have a negative pregnancy test before study entry and agree to use a medically acceptable method of contraception throughout the treatment period and for at least six weeks after treatment discontinuation', 'Exclusion Criteria:', '  Prior treatment with PM01183 or weekly paclitaxel or nanoalbumin-paclitaxel', '  Patients who have previously discontinued paclitaxel-based regimes due to drug related toxicity.', '  Known hypersensitivity to bevacizumab or any component of its formulation', '  Patients who have previously discontinued bevacizumab-containing regimes due to drug-related toxicity.', '  More than three prior lines of chemotherapy', '  Less than three months since last taxane-containing therapy.', '  Wash-out period:', '  Less than three weeks since the last chemotherapy-containing regimen', '  Less than three weeks since the last radiotherapy dose', '  Less than four weeks since last monoclonal antibody-containing therapy', '  Concomitant diseases/conditions:', '  Unstable angina, myocardial infarction, valvular heart disease, encephalopathy, ischemic attacks, hemorrhagic or ischemic cerebrovascular accident (CVA) or ongoing pulmonary embolism within last year, arrhythmia, hepatopathy, uncontrolled infection, hemoptysis or oxygen requiring dyspnea, known HIV infection, bleeding risk, muscular problems, peripheral neuropathy, Symptomatic or progressive brain metastases or leptomeningeal disease.', '  Men or pre-menopausal women who are not using an effective method of contraception as previously described; actively breast feeding women.', '  Patients who have pelvic irradiation with doses  45 Grays (Gy).', '  History of previous bone marrow and/or stem cell transplantation.', '  Confirmed bone marrow involvement']\n",
        "# Question: does this imply that Patients with a histologically/cytologically confirmed diagnosis of a resectable Non-small cell lung breast cancer or Small cell lung cancer are eligible for the primary trial\n",
        "# Answer with only one of the options (Entailment or Contradiction):\"\"\"\n",
        "# contradiction\n",
        "\n",
        "\n",
        "\n",
        "# input_text= \"\"\"Primary trial evidence are Adverse Events 1:  Total: 5/32 (15.63%)  Febrile neutropenia 1/32 (3.13%)  Supraventricular tachycardia 1/32 (3.13%)  Hypersensitivity 2/32 (6.25%)  Catheter site infection 1/32 (3.13%)  Confusional state 1/32 (3.13%) Secondary trial evidence are Adverse Events 1:  Total: 285/752 (37.90%)  Anaemia 2/752 (0.27%)  Disseminated intravascular coagulation 2/752 (0.27%)  Febrile neutropenia 51/752 (6.78%)  Neutropenia 47/752 (6.25%)  Thrombocytopenia 2/752 (0.27%)  Atrial fibrillation 1/752 (0.13%)  Atrial flutter 0/752 (0.00%)  Cardiac failure congestive 1/752 (0.13%)  Left ventricular dysfunction 0/752 (0.00%)Adverse Events 2:  Total: 117/382 (30.63%)  Anaemia 3/382 (0.79%)  Disseminated intravascular coagulation 0/382 (0.00%)  Febrile neutropenia 11/382 (2.88%)  Neutropenia 20/382 (5.24%)  Thrombocytopenia 0/382 (0.00%)  Atrial fibrillation 1/382 (0.26%)  Atrial flutter 1/382 (0.26%)  Cardiac failure congestive 0/382 (0.00%)  Left ventricular dysfunction 1/382 (0.26%) \"\n",
        "# Question: does this imply that Heart-related adverse events were recorded in both the primary trial and the secondary trial.\n",
        "# Answer with only one of the options (Entailment or Contradiction):\"\"\"\n",
        "# # Entailment\n",
        "\n",
        "\n",
        "\n",
        "# input_text= \"\"\"Primary trial evidence are Inclusion Criteria: Age 52-75 years old; Identification as Latina/Hispanic/Chicana female; Residence in Pilsen, Little Village, East Side or South Chicago; No history of health volunteerism; No history of breast cancer; and Lack of a mammogram within the last two yearsExclusion Criteria: Not meeting all inclusion criteria; Women will be excluded if they participated in formative focus groups.\"\n",
        "# Question: does this imply that\n",
        "# Patients eligible for the primary trial must live in the USA.\n",
        "# Answer with only one of the options (Entailment or Contradiction):\"\"\"\n",
        "#Entailment\n",
        "\n",
        "# input_text= \"\"\"Primary trial evidence are Adverse Events 1: Total: 10/71 (14.08%) ATRIAL FIBRILLATION 1/71 (1.41%) CARDIAC TAMPONADE 1/71 (1.41%) PERICARDIAL EFFUSION 1/71 (1.41%) SUPRAVENTRICULAR TACHYCARDIA 1/71 (1.41%) DIARRHOEA 1/71 (1.41%) NAUSEA 1/71 (1.41%) VOMITING 1/71 (1.41%) CHEST PAIN 1/71 (1.41%) PNEUMONIA 1/71 (1.41%) MALIGNANT PLEURAL EFFUSION 1/71 (1.41%) HEPATIC ENCEPHALOPATHY 1/71 (1.41%) .\n",
        "# Question: does this imply that\n",
        "# There were only 3 adverse events in the primary trial which occurred more than twice?\n",
        "# Answer with only one of the options (Entailment or Contradiction):\"\"\"\n",
        "# \"Contradiction\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "outputs = model.generate(inputs['input_ids'], max_length=1024, num_return_sequences=1)\n",
        "\n",
        "# Decode the response\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the response\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tCh_DJ7ciTSX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "model = model.to('cuda')\n",
        "\n",
        "first_five_inputs = df['text'].head(5)\n",
        "\n",
        "# Initialize progress bar with tqdm\n",
        "progress_bar = tqdm(enumerate(first_five_inputs, start=1), total=len(first_five_inputs), desc=\"Processing inputs\")\n",
        "\n",
        "# Loop through the first five inputs and process each one\n",
        "results = []\n",
        "for i, input_text in progress_bar:\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True).to('cuda')\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=2048, num_return_sequences=1)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    results.append(response)\n",
        "    progress_bar.set_description(f\"Processing input {i}\")\n",
        "\n",
        "# Print the results\n",
        "for i, response in enumerate(results, start=1):\n",
        "    print(f\"Result for input {i}: {response}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "df2=df.head(5)\n",
        "df2"
      ],
      "metadata": {
        "id": "5WYPJtzUiXga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxKfgUD7kmYt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm  # For progress bar\n",
        "\n",
        "# Assuming `df` is your DataFrame and 'inputs' is the column containing the text.\n",
        "text_column = df2['text']\n",
        "\n",
        "# Initialize progress bar with tqdm\n",
        "progress_bar = tqdm(enumerate(text_column, start=1), total=len(text_column), desc=\"Processing inputs\")\n",
        "\n",
        "# Initialize an empty list to store the results\n",
        "results = []\n",
        "\n",
        "for i, input_text in progress_bar:\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=4096, padding=True)\n",
        "\n",
        "    # Generate the output with max_new_tokens=1\n",
        "    outputs = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        max_new_tokens=4,  # Limit output to one word/token\n",
        "        do_sample=True    # Optional: Set to True for random sampling, or False for deterministic results\n",
        "    )\n",
        "\n",
        "    # Decode the response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Save the result\n",
        "    results.append(response.strip())  # Strip extra spaces for cleanliness\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqWkyIzyODwm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ... your existing code ...\n",
        "\n",
        "# Create a DataFrame from the results list\n",
        "results_df = pd.DataFrame({'prediction': results})  # Assuming 'prediction' is a suitable column name\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "results_df.to_csv(file_path, index=False)\n",
        "\n",
        "print(\"CSV file saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rc1_s_WpeMaG"
      },
      "outputs": [],
      "source": [
        "flan.iloc[7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enAqNONJB-7M"
      },
      "outputs": [],
      "source": [
        "file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fqv_0gUg_LB0"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# File path\n",
        "file_path = \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_03122024/combined_test_zero_shot.csv\"\n",
        "#file_path = \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_03122024/responses-zero-shot-qwen.csv\"\n",
        "#file_path = \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_03122024/total.csv\"\n",
        "#file_path = \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_03122024/responses-two-shot-qwen.csv\"\n",
        "#file_path = \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_03122024/flan_t5_xl_2_zero_shot.csv\"\n",
        "#file_path = \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_03122024/flan_t5_xl_2.2_zero_shot.csv\"\n",
        "#file_path = \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_03122024/flan_t5_response_zero_shot.csv\"\n",
        "#file_path = \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_03122024/combined_test_two_shot_mistral.csv\"\n",
        "#file_path = \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_03122024/flan_t5_xl__two_shot.csv\"\n",
        "#file_path = \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_03122024/combined_test(text,statement,lables).csv\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Load the CSV file into a DataFrame\n",
        "data= pd.read_csv(file_path)\n",
        "#data2= pd.read_csv(file_path)\n",
        "#flan= pd.read_csv(file_path)\n",
        "\n",
        "#all= pd.read_csv(file_path)\n",
        "#results=pd.read_csv(file_path)\n",
        "# Rename the column\n",
        "# Replace 'old_column_name' with the current name and 'new_column_name' with the desired name\n",
        "# df.rename(columns={'model_response': 'mistral-7b-zero-shot'}, inplace=True)\n",
        "#df\n",
        "# # Save the updated DataFrame back to the same file\n",
        "#df_pred = pd.DataFrame({'prediction': pred})\n",
        "\n",
        "#data.to_csv(file_path, index=False)\n",
        "#s.to_csv(file_path, index=False)\n",
        "#results.to_csv(file_path, index=False)\n",
        "# print(\"Column renamed and file saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "hAWSn4OVnmaz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_aGPzgogcNC"
      },
      "outputs": [],
      "source": [
        "# Replace 'prediction' with the name of the column containing the predictions\n",
        "\n",
        "\n",
        "column_name = 'prediction'\n",
        "\n",
        "# List to store skipped row numbers\n",
        "skipped_rows = []\n",
        "\n",
        "# Iterate through each row and process the column\n",
        "for index, row in df.iterrows():\n",
        "    value = row[column_name]\n",
        "    if value == ['<pad> Yes</s>'] or value == ['<pad> yes</s>']:\n",
        "        df.at[index, column_name] = 'Entailment'\n",
        "    elif value == ['<pad> no</s>'] or value == ['<pad> No</s>']:\n",
        "        df.at[index, column_name] = 'Contradiction'\n",
        "    elif value == ['<pad> Contradiction</s>']:\n",
        "        df.at[index, column_name] = 'Contradiction'\n",
        "    elif value == ['<pad> Entailment</s>']:\n",
        "        df.at[index, column_name] = 'Entailment'\n",
        "    elif value not in [['<pad> Entailment</s>'],['<pad> Contradiction</s>']]:\n",
        "        skipped_rows.append(index)\n",
        "\n",
        "        #df.at[index, column_name] = None  # Mark for removal later\n",
        "\n",
        "# Remove rows with None values (skipped rows)\n",
        "#df = df.dropna(subset=[column_name])\n",
        "\n",
        "# Save the processed data to a new CSV file\n",
        "\n",
        "\n",
        "# Output the skipped rows\n",
        "print(\"Skipped Rows:\", skipped_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Qwen2 7b finetuning\n"
      ],
      "metadata": {
        "id": "y-naKgvP-MmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all dependencies\n",
        "!pip install -q transformers accelerate peft datasets bitsandbytes einops\n",
        "!pip install -q fsspec==2025.3.2\n",
        "# Imports\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import load_dataset\n",
        "import torch"
      ],
      "metadata": {
        "id": "amDooKPz-Lue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "Tr7uxuGpJoBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download and save  Base Model & Tokenizer (Qwen2-7B-Instruct)\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16  # Use float16 if bfloat16 not supported\n",
        ")\n",
        "\n",
        "model_id = \"Qwen/Qwen2-7B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "base_model_path = \"/content/drive/MyDrive/sepi_master_thesis/models/qwen2-original\"\n",
        "model.save_pretrained(base_model_path)\n",
        "tokenizer.save_pretrained(base_model_path)\n"
      ],
      "metadata": {
        "id": "IkxNp7rNp3te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the saved model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16  # use float16 for A100\n",
        ")\n",
        "\n",
        "# 👇 Load from your saved Google Drive folder\n",
        "local_model_path = \"/content/drive/MyDrive/sepi_master_thesis/models/DeepSeek-R1-Distill-Qwen-7B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(local_model_path, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    local_model_path,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config\n",
        ")"
      ],
      "metadata": {
        "id": "e1DbfEB3VUDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply QLoRA (PEFT)\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "Pz-_S7zL0S4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset\n",
        "data_files = {\n",
        "    \"train\": \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/train-finetune.csv\",\n",
        "    \"validation\": \"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/dev-finetune.csv\"\n",
        "}\n",
        "\n",
        "dataset = load_dataset(\"csv\", data_files=data_files)"
      ],
      "metadata": {
        "id": "Fmc2Lu_x1K4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize Dataset\n",
        "def tokenize(example):\n",
        "    result = tokenizer(\n",
        "        example[\"text\"],  # Make sure \"text\" is the correct column\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=1024\n",
        "    )\n",
        "\n",
        "    # Convert label string to single token ID\n",
        "    label_text = example[\"label\"].strip().lower()\n",
        "    label_token_id = tokenizer(label_text, add_special_tokens=False)[\"input_ids\"][0]\n",
        "\n",
        "    result[\"labels\"] = label_token_id\n",
        "    return result\n",
        "\n",
        "# ⛏️ Remove original string fields like \"label\" that crash training\n",
        "tokenized = dataset.map(tokenize, remove_columns=dataset[\"train\"].column_names)\n"
      ],
      "metadata": {
        "id": "JGRSWTstBH-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(example):\n",
        "    full_input = example[\"text\"].strip() + \"\\n\" + example[\"label\"].strip()\n",
        "    return tokenizer(full_input, padding=\"max_length\", truncation=True, max_length=1024)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize, batched=True)"
      ],
      "metadata": {
        "id": "krUl-9Mftuzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(example):\n",
        "    prompt = example[\"text\"].strip()\n",
        "    label = example[\"label\"].strip()\n",
        "\n",
        "    # Full input: prompt + label\n",
        "    full_input = prompt + \"\\n\" + label\n",
        "\n",
        "    # Tokenize full sequence\n",
        "    tokenized = tokenizer(\n",
        "        full_input,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=1024\n",
        "    )\n",
        "\n",
        "    # Tokenize the label only\n",
        "    label_ids = tokenizer(label, add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "    # Compute position of label tokens\n",
        "    label_start = len(tokenized[\"input_ids\"]) - len(label_ids)\n",
        "\n",
        "    # Create labels: mask prompt tokens with -100\n",
        "    labels = [-100] * label_start + label_ids\n",
        "\n",
        "    # Ensure labels is 1024 tokens long\n",
        "    labels = labels[:1024] + [-100] * (1024 - len(labels))\n",
        "\n",
        "    tokenized[\"labels\"] = labels\n",
        "    return tokenized\n",
        "tokenized = dataset.map(tokenize, remove_columns=dataset[\"train\"].column_names)\n"
      ],
      "metadata": {
        "id": "8xt79PL2cIte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define Training Arguments\n",
        "output_path = \"/content/drive/MyDrive/sepi_master_thesis/models/DeepSeek-R1-Distill-Qwen-7B-finetuned\"\n",
        "\n",
        "\n",
        "ttraining_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/sepi_master_thesis/models/DeepSeek-R1-Distill-Qwen-7B-finetuned\",\n",
        "    per_device_train_batch_size=4,  # Increased batch size\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=5,  # Increased number of epochs\n",
        "    learning_rate=5e-5,  # Adjusted learning rate\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n"
      ],
      "metadata": {
        "id": "1RLdhXs2BSVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!pip install wandb\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "NhZm5TR1On0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Start Fine-Tuning\n",
        "import gc\n",
        "import torch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"validation\"],\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "xgFfh3XmBVFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the Fine-Tuned Model\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n"
      ],
      "metadata": {
        "id": "O17zI18sBtUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "# Load model and tokenizer\n",
        "local_model_path = \"/content/drive/MyDrive/sepi_master_thesis/models/DeepSeek-R1-Distill-Qwen-7B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(local_model_path)\n",
        "model.to('cuda')  # Ensure the model is on GPU\n",
        "\n",
        "# Adjust batch size based on your GPU memory\n",
        "batch_size = 4  # This is a starting point; adjust based on your GPU\n",
        "\n",
        "# Define Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/sepi_master_thesis/models/DeepSeek-R1-Distill-Qwen-7B-finetuned\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=5e-5,\n",
        "    fp16=True,  # Enable mixed precision to save memory and potentially speed up training\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "def tokenize(example):\n",
        "    full_input = example[\"text\"].strip() + \"\\n\" + example[\"label\"].strip()\n",
        "    return tokenizer(full_input, padding=\"max_length\", truncation=True, max_length=1024)\n",
        "\n",
        "# Apply tokenization to the dataset\n",
        "tokenized_datasets = dataset.map(tokenize, batched=True)\n",
        "# Implement memory cleanup and efficiency\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()  # Clear any cached memory to avoid OOM\n",
        "\n",
        "# Load and prepare dataset as previously defined\n",
        "\n",
        "# Proceed with training\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "m5BraENiubYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the Fine-Tuned Model\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n"
      ],
      "metadata": {
        "id": "AjKa_27bup9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Qwen2 7b finetuning 2\n"
      ],
      "metadata": {
        "id": "SPiVDhQSoptV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate peft datasets bitsandbytes einops\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K2HeD3z4o0A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Load and format your dataset\n",
        "\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/train-finetune.csv\")\n",
        "dev_df = pd.read_csv(\"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/dev-finetune.csv\")\n",
        "\n",
        "def format_fn(example):\n",
        "    return {\n",
        "        \"text\": f\"{example['text'].strip()}\\nAnswer: {example['label'].strip()}\"\n",
        "    }\n",
        "\n",
        "train_data = Dataset.from_pandas(train_df).map(format_fn).remove_columns([\"label\"])\n",
        "dev_data = Dataset.from_pandas(dev_df).map(format_fn).remove_columns([\"label\"])\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    load_in_8bit=True  # Save memory\n",
        ")\n",
        "\n",
        "# ✅ STEP 4: Tokenize Data\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
        "\n",
        "tokenized_train = train_data.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "tokenized_dev = dev_data.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# ✅ STEP 5: Apply LoRA\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n"
      ],
      "metadata": {
        "id": "9Ok3QPW09Am9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "P1a5rs0Po9kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ STEP 6: Define Training Arguments\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/sepi_master_thesis/models/Qwen-7B-finetuned\",\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=5,\n",
        "    logging_steps=20,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    bf16=True,  # Set to fp16=True if Colab GPU doesn't support bf16\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# ✅ STEP 7: Define Data Collator and Trainer\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_dev,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# ✅ STEP 8: Train and Save Model\n",
        "trainer.train()\n",
        "trainer.save_model(\"/content/drive/MyDrive/sepi_master_thesis/models/Qwen-7B-finetuned\")\n"
      ],
      "metadata": {
        "id": "_fYDw8gn6e6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## testing"
      ],
      "metadata": {
        "id": "45e3ywm7ng0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# ✅ Load your test data\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/test-zero-shot.csv\")  # Make sure it has a 'text' column\n",
        "\n",
        "# ✅ Load the fine-tuned model\n",
        "model_path = \"/content/drive/MyDrive/sepi_master_thesis/models/Qwen-7B-finetuned\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# ✅ Generate predictions\n",
        "predictions = []\n",
        "\n",
        "for i, row in test_df.iterrows():\n",
        "    prompt = row[\"text\"].strip()\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
        "\n",
        "    # Generate prediction\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=8,\n",
        "            do_sample=False,        # Greedy decoding for consistent results\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode generated text and extract label\n",
        "    decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the answer (after \"Answer: \")\n",
        "    if \"Answer:\" in decoded:\n",
        "        predicted_label = decoded.split(\"Answer:\")[-1].strip().split(\"\\n\")[0]\n",
        "    else:\n",
        "        predicted_label = decoded.strip()\n",
        "\n",
        "    predictions.append(predicted_label)\n",
        "\n",
        "# ✅ Save predictions\n",
        "test_df[\"predicted_label\"] = predictions\n",
        "#test_df.to_csv(\"/content/test_predictions.csv\", index=False)\n",
        "\n",
        "print(\"✅ Predictions saved to /content/test_predictions.csv\")\n",
        "print (test_df)"
      ],
      "metadata": {
        "id": "0Qdyb4FlngPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.to_csv(\"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/test-zero-shot-onQwen2-finetunned.csv\", index=False)"
      ],
      "metadata": {
        "id": "0G1x6c2Bv3q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lzQyx6RWwB2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DeepSeek R1\n"
      ],
      "metadata": {
        "id": "XEhqCrYzhDr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torch transformers datasets accelerate bitsandbytes sentencepiece peft\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DG5DYV57BIDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate datasets torch"
      ],
      "metadata": {
        "id": "ICeoU_-fgWLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "model_name = \"deepseek-ai/deepseek-r1-distill-qwen-7b\"\n",
        "save_directory = \"/content/drive/MyDrive/sepi_master_thesis/models/DeepSeek-R1-Distill-Qwen-7B\"\n",
        "\n",
        "# Load tokenizer and model (downloads automatically)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Save model & tokenizer to Google Drive\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "model.save_pretrained(save_directory)\n",
        "\n",
        "print(\"✅ Model successfully downloaded and saved in Google Drive!\")"
      ],
      "metadata": {
        "id": "gIl3jP3NBoVb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "train=pd.read_csv(\"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/train-finetune.csv\")\n",
        "dev=pd.read_csv(\"/content/drive/MyDrive/sepi_master_thesis/new_generated_data_01022025/dev-finetune.csv\")\n",
        "\n",
        "# Convert Pandas DataFrame to Hugging Face Dataset\n",
        "train_dataset = Dataset.from_pandas(train)\n",
        "dev_dataset = Dataset.from_pandas(dev)"
      ],
      "metadata": {
        "id": "SuL8L5uTYn1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/sepi_master_thesis/models/DeepSeek-R1-Distill-Qwen-7B\"\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "#💡 If you get an A100 or H100 GPU: torch_dtype=torch.bfloat16\n",
        "#💡 If you get a T4/V100: torch_dtype=torch.float16\n",
        "\n",
        "print(\"✅ Model successfully loaded from Google Drive!\")"
      ],
      "metadata": {
        "id": "CkeU0jkEEKzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./deepseek-finetune\",\n",
        "    evaluation_strategy=\"epoch\",  # Evaluate after every epoch\n",
        "    save_strategy=\"epoch\",  # Save model after each epoch\n",
        "    per_device_train_batch_size=1,  # Adjust if needed (larger batch → more memory)\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,  # Simulates larger batch size\n",
        "    learning_rate=2e-5,  # Typical for LLM fine-tuning\n",
        "    num_train_epochs=2,  # Adjust if needed\n",
        "    weight_decay=0.01,  # Regularization\n",
        "    bf16=True,  # Use bfloat16 for lower memory usage\n",
        "    save_total_limit=2,  # Keep last 2 checkpoints\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\"  # No online logging\n",
        ")\n",
        "\n",
        "print(\"Training Parameters Set ✅\")"
      ],
      "metadata": {
        "id": "2z1qs9k6gqHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,  # Only truncate if text exceeds 1024 tokens\n",
        "        padding=\"longest\",  # Dynamic padding to match the longest sentence\n",
        "        max_length=1024  # Increased limit to 1024 tokens\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Start Training\n",
        "trainer.train()\n",
        "\n",
        "print(\"Fine-Tuning Complete ✅\")"
      ],
      "metadata": {
        "id": "HdGfdbPkjQOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/sepi_master_thesis/finetuned-deepseek\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/sepi_master_thesis/finetuned-deepseek\")\n",
        "\n",
        "print(\"Model Saved Successfully ✅\")\n"
      ],
      "metadata": {
        "id": "0cQeIJ1YjkXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Path to your fine-tuned model\n",
        "fine_tuned_model_path = \"/content/drive/MyDrive/sepi_master_thesis/models/DeepSeek-R1-Distill-Qwen-7B-finetuned\"\n",
        "\n",
        "# Load tokenizer and model from the fine-tuned directory\n",
        "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_path)\n"
      ],
      "metadata": {
        "id": "oeFXNTkeh4FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1=data"
      ],
      "metadata": {
        "collapsed": true,
        "id": "x08muKTijCpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=data1[5:10]"
      ],
      "metadata": {
        "id": "mojT0F67kHEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "ffgrFuQpkSLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# Path to your fine-tuned model\n",
        "#fine_tuned_model_path = \"/content/drive/MyDrive/sepi_master_thesis/models/DeepSeek-R1-Distill-Qwen-7B-finetuned\"\n",
        "fine_tuned_model_path = \"/content/drive/MyDrive/sepi_master_thesis/models/qwen2-nli-finetuned\"\n",
        "\n",
        "\n",
        "# Load tokenizer and model from the fine-tuned directory\n",
        "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_path)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# Assuming 'data' is your DataFrame and it has a column named 'text'\n",
        "responses = []\n",
        "\n",
        "for input_text in tqdm(data['text'], desc=\"Processing inputs\", total=len(data['text'])):\n",
        "    # Encode the input text and move tensors to the same device as model\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate output using the model\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=50,  # Generate a short response\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "\n",
        "    # Decode and clean up the generated text\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    responses.append(response)\n",
        "\n",
        "# Add responses to the DataFrame\n",
        "data['model_response'] = responses\n"
      ],
      "metadata": {
        "id": "eL0CVnw6ipNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# Assuming 'data' is your DataFrame and it has a column named 'text'\n",
        "responses = []\n",
        "\n",
        "for input_text in tqdm(data['text'], desc=\"Processing inputs\", total=len(data['text'])):\n",
        "    # Encode the input text and move tensors to the same device as model\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate output using the model\n",
        "    outputs = model.generate(\n",
        "      **inputs,\n",
        "      max_new_tokens=5,  # Increased length for full reasoning\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=False,  # Enables sampling for more detailed responses\n",
        "        temperature=0.6,  # Ensures balanced diversity\n",
        "        top_p=0.85,  # Helps focus on high-probability tokens\n",
        "        repetition_penalty=1.1  # Keeps the generation conservative\n",
        "    )\n",
        "\n",
        "    # Decode and clean up the generated text\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    responses.append(response)\n",
        "\n",
        "# Add responses to the DataFrame\n",
        "data['model_response'] = responses"
      ],
      "metadata": {
        "id": "qtWmERfemi-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "collapsed": true,
        "id": "c719CxWwmFqs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}